{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSprop optimizer\n",
    "\n",
    "RMSprop (Root Mean Square Propagation) is an optimization algorithm commonly used in training artificial neural networks (ANNs). It is particularly effective in scenarios where other optimization algorithms like vanilla stochastic gradient descent (SGD) may struggle due to problems such as vanishing or exploding gradients.\n",
    "\n",
    "## Details of RMSprop Algorithm\n",
    "\n",
    "RMSprop is an adaptive learning rate optimization algorithm proposed by Geoffrey Hinton in his course on Neural Networks for Machine Learning. The algorithm is designed to adaptively adjust the learning rates for different parameters during training.\n",
    "\n",
    "Summary of how RMSprop works:\n",
    "\n",
    "1. Compute Squared Gradients: RMSprop maintains a moving average of the squared gradients for each parameter. This is similar to AdaGrad but with a decaying average.\n",
    "\n",
    "2. Update Parameters: The update rule adjusts the learning rate for each parameter based on the average of the squared gradients.\n",
    "\n",
    "3. Adaptive Learning Rates: RMSprop divides the learning rate by the square root of the exponentially decaying average of squared gradients for each parameter. This helps to normalize the learning rates and overcome the problems of vanishing or exploding gradients.\n",
    "\n",
    "\n",
    "## Pros of RMSprop optimizer\n",
    "\n",
    "1. Adaptive Learning Rates: RMSprop adapts the learning rates for each parameter individually based on the magnitude of their gradients. This helps converge faster and more efficiently, especially in deep neural networks.\n",
    "\n",
    "2. Stability: It helps to stabilize the learning process by mitigating the issues of vanishing and exploding gradients.\n",
    "\n",
    "3. Simple Implementation: RMSprop is relatively easy to implement and widely used in practice.\n",
    "\n",
    "## Cons of RMSprop optimizer\n",
    "\n",
    "1. Hyperparameter Sensitivity: RMSprop, like other adaptive methods, has hyperparameters that need to be tuned, such as the learning rate and the decay rate. Improper tuning can lead to suboptimal performance.\n",
    "\n",
    "2. Memory Usage: RMSprop maintains a moving average of squared gradients for each parameter, which can require additional memory, especially for large models with many parameters.\n",
    "\n",
    "\n",
    "## References\n",
    "- https://keras.io/api/optimizers/rmsprop/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fashionmnist_model import FMM\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the data\n",
    "X_train, y_train, X_test, y_test = FMM.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data\n",
    "X_train, X_test = FMM.reshape_data(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.RMSprop()\n",
    "model = FMM.create_model()\n",
    "print(f\"Training with {optimizer.__class__.__name__} optimizer...\")\n",
    "history, train_accuracy, val_accuracy = FMM.compile_and_train(\n",
    "    model, X_train, y_train, X_test, y_test, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = FMM.evaluate(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training accuracy : {train_accuracy}\")\n",
    "print(f\"Validation accuracy : {val_accuracy}\")\n",
    "print(f\"Loss : {loss}\")\n",
    "print(f\"Accuracy : {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FMM.plot_history(history, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trend shows that the training loss is increasing over time, while the validation loss is decreasing. This suggests that the model is overfitting the training data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
