{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Adam optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0SoqfPksDKZg"
   },
   "source": [
    "\n",
    "\n",
    "The Adam optimizer is a stochastic gradient descent algorithm used in training artificial neural networks (ANNs). It combines elements of two other popular optimization techniques, AdaGrad and RMSProp:\n",
    "\n",
    "It is based on adaptive estimation of first-order and second-order moments. First, it looks at how steep the slope is when we are trying to find the best way to decrease the error (that's the first-order moment). Then, it also considers how quickly the slope changes in different directions (that's the second-order moment).\n",
    "\n",
    "Adam adjusts its learning process by keeping track of these moments as it goes along. This helps it figure out how much to change the learning rates for different parts of the neural network. By doing this, Adam can learn more effectively and get better results, especially when the data is noisy or the learning process is complicated.\n",
    "\n",
    "## Details of the Adam Optimizer\n",
    "\n",
    "1. Adaptive Learning Rate: Adam adjusts the learning rate for each parameter individually, based on the first and second moments of the gradients.\n",
    "\n",
    "2. Momentum: It utilizes momentum to accelerate the optimization process by accumulating exponentially decaying average of past gradients.\n",
    "\n",
    "3. Bias Correction: Adam applies bias correction to the estimates of the first and second moments to alleviate their initialization bias towards zero, especially in the initial training stages.\n",
    "\n",
    "4. Parameter Updates: The parameters are updated based on the calculated moving averages of the gradients.\n",
    "\n",
    "## Pros of Adam Optimizer\n",
    "\n",
    "1. Efficient: Adam is known for its efficiency in training deep neural networks, often converging faster than other optimization algorithms.\n",
    "\n",
    "2. Adaptive Learning Rates: Its adaptive learning rate mechanism allows for dynamic adjustments to the learning rates for each parameter, which can be beneficial in different stages of training.\n",
    "\n",
    "3. Suitability for Sparse Data: Adam performs well with sparse gradients and noisy data, making it suitable for a wide range of applications.\n",
    "\n",
    "## Cons of Adam Optimizer\n",
    "\n",
    "1. Memory Usage: Adam maintains additional state variables for each parameter, which can increase memory consumption, especially for large models.\n",
    "\n",
    "2. Hyperparameter Sensitivity: Adam requires careful tuning of hyperparameters such as learning rate, beta1, beta2, and epsilon, to achieve optimal performance. Improper tuning may lead to suboptimal results.\n",
    "\n",
    "3. Robustness to Noise: In some cases, Adam may exhibit sensitivity to noise in the objective function, causing fluctuations in convergence behavior.\n",
    "\n",
    "## References\n",
    "- https://keras.io/api/optimizers/adam/\n",
    "- https://www.geeksforgeeks.org/adam-optimizer-in-tensorflow/\n",
    "- https://www.geeksforgeeks.org/intuition-of-adam-optimizer/\n",
    "- https://spotintelligence.com/2023/03/01/adam-optimizer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0BL-NITwDKZi"
   },
   "source": [
    "# Implementation using TensorFlow on fashion minst dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q7jewJ3bDKZi"
   },
   "outputs": [],
   "source": [
    "from fashionmnist_model import FMM\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9g8A7WBWDKZj"
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "X_train, y_train, X_test, y_test = FMM.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = FMM.reshape_data(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RgUHMkiSDKZj",
    "outputId": "88b1f24e-23bf-42b4-f9b4-38e5512d009d"
   },
   "outputs": [],
   "source": [
    "# Create and compile the model with the optimizer\n",
    "model = FMM.create_model_v2()\n",
    "print(f\"Training with {optimizer.__class__.__name__} optimizer...\")\n",
    "history = FMM.compile_and_train(\n",
    "    model, X_train, y_train, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FMM.evaluate(model, X_test, y_test, history)\n",
    "FMM.plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJq5_I4wDKZl"
   },
   "source": [
    "### Application to Fashion MNIST Dataset:\n",
    "\n",
    "- Loss: The \"loss\" measures how well the neural network is performing. It represents the difference between the predicted output and the actual output. Lower loss values indicate better performance.\n",
    "\n",
    "- Accuracy: The \"accuracy\" measures how often the neural network makes correct predictions. Higher accuracy values indicate better performance.\n",
    "\n",
    "Analysis of the results:\n",
    "\n",
    "1. Epochs: The training process was repeated 30 times (epochs) to improve the neural network's performance.\n",
    "\n",
    "2. Training and Validation: The neural network was trained on a training set and evaluated on a validation set after each epoch. This helped to monitor how well the model generalizes to new data.\n",
    "\n",
    "3. Improvement: Over the epochs, both training and validation loss decrease, and accuracy increases. This indicates that the model is learning from the data and improving its performance.\n",
    "\n",
    "4. Performance: The neural network achieved an accuracy of around 87% on the validation set by the end of training. This means that it correctly classified the images in the Fashion MNIST dataset about 87% of the time.\n",
    "\n",
    "5. Stability: The loss and accuracy values on the validation set appear to stabilize towards the end of training, suggesting that the model's performance isn't changing significantly after a certain number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise KeyboardInterrupt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NaC2gDY1Deuj",
    "outputId": "a64ff6f3-5cd7-4bfc-efe7-dbd80870a5d2"
   },
   "outputs": [],
   "source": [
    "! pip install -U \"ray[data,train,tune,serve]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jwo_a1uyDKZm"
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import tune, train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "id": "GsucQLSADKZm",
    "outputId": "e2db9b2b-5634-4d4e-bc46-973d4d1254ba"
   },
   "outputs": [],
   "source": [
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-NTHKLMvDKZm"
   },
   "outputs": [],
   "source": [
    "# Define a function to train the model\n",
    "def train_model(config):\n",
    "    from fashionmnist_model import FMM\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    X_train, y_train, X_test, y_test = FMM.load_data()\n",
    "    X_train, X_test = FMM.reshape_data(X_train, X_test)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.legacy.Adam(\n",
    "        learning_rate=config[\"learning_rate\"],\n",
    "        beta_1=config[\"beta_1\"],\n",
    "        beta_2=config[\"beta_2\"],\n",
    "        epsilon=config[\"epsilon\"],\n",
    "        weight_decay=config[\"weight_decay\"],\n",
    "    )\n",
    "\n",
    "    model = FMM.create_model_v2()\n",
    "    history = FMM.compile_and_train(\n",
    "        model, X_train, y_train, optimizer\n",
    "    )\n",
    "    \n",
    "    loss, accuracy, _, _ = FMM.evaluate(model, X_test, y_test, history)\n",
    "\n",
    "    train.report({\"accuracy\": accuracy, \"loss\": loss, **config})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    \"learning_rate\": tune.grid_search([0.001, 0.0005, 0.0001]),\n",
    "    \"beta_1\": tune.grid_search([0.9, 0.95, 0.99]),\n",
    "    \"beta_2\": tune.grid_search([0.999, 0.9999]),\n",
    "    \"epsilon\": tune.grid_search([1e-8, 1e-7, 1e-6]),\n",
    "    \"weight_decay\": tune.grid_search([1e-6, 1e-5, 1e-4]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ro-ukGpHH_EV",
    "outputId": "4cf2dcde-484e-459d-8dbd-175805b65d96"
   },
   "outputs": [],
   "source": [
    "# Run hyperparameter tuning\n",
    "analysis = tune.run(\n",
    "    train_model,\n",
    "    config=search_space,\n",
    "    metric=\"accuracy\",\n",
    "    mode=\"max\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FMM.plot_analysis_results(analysis, x_axis=\"learning_rate\", y_axis=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ksM1feu4D1aH"
   },
   "outputs": [],
   "source": [
    "# Print the best hyperparameters and results\n",
    "best_config = analysis.best_config\n",
    "print(\"Best hyperparameters:\", best_config)\n",
    "print(\"Best accuracy:\", analysis.best_result[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(**best_config)\n",
    "model = FMM.create_model_v2()\n",
    "print(f\"Training with {optimizer.__class__.__name__} optimizer...\")\n",
    "history = FMM.compile_and_train(\n",
    "    model, X_train, y_train, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FMM.evaluate(model, X_test, y_test, history)\n",
    "FMM.plot_history(history)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
