{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0a9ceb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'fashionmnist_model' from '/Users/noah/Humber/Advanced-DL/Ad-DL-Midterm-Project/fashionmnist_model.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import fashionmnist_model as fmm_module  # Ensures clarity by matching the file name\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam, Adagrad\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reloads the module to ensure any changes are applied\n",
    "importlib.reload(fmm_module)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c588bca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = fmm_module.FMM.load_data()\n",
    "X_train, X_test = fmm_module.FMM.reshape_data(X_train, X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eed54881",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers = {\n",
    "    'RMSprop': RMSprop,\n",
    "    'Adam': Adam,\n",
    "    'Adagrad': Adagrad\n",
    "}\n",
    "learning_rates = [0.001, 0.0005, 0.0001]\n",
    "batch_sizes = [32, 64, 128]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b29b636c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with RMSprop, LR: 0.001, Batch Size: 32\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-22 19:31:13.128693: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.7729 - accuracy: 0.7453 - val_loss: 0.5111 - val_accuracy: 0.8083\n",
      "Epoch 2/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.5706 - accuracy: 0.8080 - val_loss: 0.4185 - val_accuracy: 0.8518\n",
      "Epoch 3/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.5300 - accuracy: 0.8237 - val_loss: 0.4056 - val_accuracy: 0.8496\n",
      "Epoch 4/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.5032 - accuracy: 0.8313 - val_loss: 0.3967 - val_accuracy: 0.8618\n",
      "Epoch 5/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4898 - accuracy: 0.8335 - val_loss: 0.3980 - val_accuracy: 0.8548\n",
      "Epoch 6/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4750 - accuracy: 0.8420 - val_loss: 0.3993 - val_accuracy: 0.8573\n",
      "Epoch 7/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4714 - accuracy: 0.8441 - val_loss: 0.3893 - val_accuracy: 0.8608\n",
      "Epoch 8/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4621 - accuracy: 0.8458 - val_loss: 0.3756 - val_accuracy: 0.8682\n",
      "Epoch 9/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4531 - accuracy: 0.8484 - val_loss: 0.3759 - val_accuracy: 0.8695\n",
      "Epoch 10/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4481 - accuracy: 0.8507 - val_loss: 0.3858 - val_accuracy: 0.8626\n",
      "Epoch 11/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4415 - accuracy: 0.8515 - val_loss: 0.3694 - val_accuracy: 0.8706\n",
      "Epoch 12/30\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 0.4412 - accuracy: 0.8530 - val_loss: 0.3690 - val_accuracy: 0.8708\n",
      "Epoch 13/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4309 - accuracy: 0.8556 - val_loss: 0.3673 - val_accuracy: 0.8724\n",
      "Epoch 14/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4281 - accuracy: 0.8539 - val_loss: 0.3593 - val_accuracy: 0.8742\n",
      "Epoch 15/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4264 - accuracy: 0.8565 - val_loss: 0.3785 - val_accuracy: 0.8709\n",
      "Epoch 16/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4238 - accuracy: 0.8580 - val_loss: 0.3622 - val_accuracy: 0.8727\n",
      "Epoch 17/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4232 - accuracy: 0.8595 - val_loss: 0.3782 - val_accuracy: 0.8643\n",
      "Epoch 18/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4166 - accuracy: 0.8618 - val_loss: 0.3524 - val_accuracy: 0.8779\n",
      "Epoch 19/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4121 - accuracy: 0.8625 - val_loss: 0.3559 - val_accuracy: 0.8736\n",
      "Epoch 20/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4093 - accuracy: 0.8626 - val_loss: 0.3536 - val_accuracy: 0.8750\n",
      "Epoch 21/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4114 - accuracy: 0.8633 - val_loss: 0.3623 - val_accuracy: 0.8771\n",
      "Epoch 22/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4072 - accuracy: 0.8630 - val_loss: 0.3500 - val_accuracy: 0.8787\n",
      "Epoch 23/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4006 - accuracy: 0.8665 - val_loss: 0.3459 - val_accuracy: 0.8802\n",
      "Epoch 24/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4038 - accuracy: 0.8659 - val_loss: 0.3749 - val_accuracy: 0.8663\n",
      "Epoch 25/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3967 - accuracy: 0.8682 - val_loss: 0.3644 - val_accuracy: 0.8680\n",
      "Epoch 26/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3985 - accuracy: 0.8664 - val_loss: 0.3489 - val_accuracy: 0.8789\n",
      "Epoch 27/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4013 - accuracy: 0.8647 - val_loss: 0.3539 - val_accuracy: 0.8803\n",
      "Epoch 28/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3945 - accuracy: 0.8691 - val_loss: 0.3609 - val_accuracy: 0.8758\n",
      "Epoch 29/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3911 - accuracy: 0.8699 - val_loss: 0.3494 - val_accuracy: 0.8817\n",
      "Epoch 30/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.3896 - accuracy: 0.8701 - val_loss: 0.3552 - val_accuracy: 0.8803\n",
      "313/313 - 0s - loss: 0.3860 - accuracy: 0.8715 - 297ms/epoch - 948us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.870104193687439\n",
      "Validation accuracy : 0.8803333044052124\n",
      "Loss : 0.3859785199165344\n",
      "Accuracy : 0.8715000152587891\n",
      "\n",
      "Train Accuracy: 0.870104193687439, Validation Accuracy: 0.8803333044052124\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with RMSprop, LR: 0.001, Batch Size: 64\n",
      "Epoch 1/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.7672 - accuracy: 0.7396 - val_loss: 0.4734 - val_accuracy: 0.8336\n",
      "Epoch 2/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.5362 - accuracy: 0.8182 - val_loss: 0.4092 - val_accuracy: 0.8537\n",
      "Epoch 3/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.4868 - accuracy: 0.8353 - val_loss: 0.4145 - val_accuracy: 0.8501\n",
      "Epoch 4/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.4661 - accuracy: 0.8434 - val_loss: 0.4399 - val_accuracy: 0.8476\n",
      "Epoch 5/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.4500 - accuracy: 0.8469 - val_loss: 0.3969 - val_accuracy: 0.8562\n",
      "Epoch 6/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.4312 - accuracy: 0.8546 - val_loss: 0.3821 - val_accuracy: 0.8623\n",
      "Epoch 7/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.4224 - accuracy: 0.8558 - val_loss: 0.3712 - val_accuracy: 0.8680\n",
      "Epoch 8/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.4142 - accuracy: 0.8608 - val_loss: 0.3624 - val_accuracy: 0.8701\n",
      "Epoch 9/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.4095 - accuracy: 0.8622 - val_loss: 0.3527 - val_accuracy: 0.8726\n",
      "Epoch 10/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3974 - accuracy: 0.8641 - val_loss: 0.3495 - val_accuracy: 0.8753\n",
      "Epoch 11/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3902 - accuracy: 0.8677 - val_loss: 0.3701 - val_accuracy: 0.8693\n",
      "Epoch 12/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3860 - accuracy: 0.8672 - val_loss: 0.3523 - val_accuracy: 0.8757\n",
      "Epoch 13/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3813 - accuracy: 0.8713 - val_loss: 0.3534 - val_accuracy: 0.8765\n",
      "Epoch 14/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.3745 - accuracy: 0.8739 - val_loss: 0.3584 - val_accuracy: 0.8749\n",
      "Epoch 15/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.3771 - accuracy: 0.8734 - val_loss: 0.3497 - val_accuracy: 0.8746\n",
      "Epoch 16/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.3681 - accuracy: 0.8746 - val_loss: 0.3368 - val_accuracy: 0.8809\n",
      "Epoch 17/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3675 - accuracy: 0.8763 - val_loss: 0.3583 - val_accuracy: 0.8741\n",
      "Epoch 18/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3664 - accuracy: 0.8747 - val_loss: 0.3586 - val_accuracy: 0.8760\n",
      "Epoch 19/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.3633 - accuracy: 0.8768 - val_loss: 0.3649 - val_accuracy: 0.8700\n",
      "Epoch 20/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3617 - accuracy: 0.8777 - val_loss: 0.3411 - val_accuracy: 0.8805\n",
      "Epoch 21/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3572 - accuracy: 0.8779 - val_loss: 0.3407 - val_accuracy: 0.8781\n",
      "Epoch 22/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3555 - accuracy: 0.8796 - val_loss: 0.3363 - val_accuracy: 0.8827\n",
      "Epoch 23/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3563 - accuracy: 0.8796 - val_loss: 0.3331 - val_accuracy: 0.8812\n",
      "Epoch 24/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3581 - accuracy: 0.8795 - val_loss: 0.3388 - val_accuracy: 0.8823\n",
      "Epoch 25/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3516 - accuracy: 0.8812 - val_loss: 0.3624 - val_accuracy: 0.8775\n",
      "Epoch 26/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.3507 - accuracy: 0.8803 - val_loss: 0.3408 - val_accuracy: 0.8807\n",
      "Epoch 27/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.3461 - accuracy: 0.8828 - val_loss: 0.3480 - val_accuracy: 0.8781\n",
      "Epoch 28/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.3494 - accuracy: 0.8830 - val_loss: 0.3485 - val_accuracy: 0.8763\n",
      "Epoch 29/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3389 - accuracy: 0.8844 - val_loss: 0.3385 - val_accuracy: 0.8798\n",
      "Epoch 30/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3396 - accuracy: 0.8839 - val_loss: 0.3326 - val_accuracy: 0.8823\n",
      "313/313 - 0s - loss: 0.3598 - accuracy: 0.8759 - 194ms/epoch - 619us/step\n",
      "\n",
      "Training accuracy : 0.8839166760444641\n",
      "Validation accuracy : 0.8823333382606506\n",
      "Loss : 0.35977646708488464\n",
      "Accuracy : 0.8758999705314636\n",
      "\n",
      "Train Accuracy: 0.8839166760444641, Validation Accuracy: 0.8823333382606506\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with RMSprop, LR: 0.001, Batch Size: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "375/375 [==============================] - 2s 3ms/step - loss: 0.7863 - accuracy: 0.7400 - val_loss: 0.4399 - val_accuracy: 0.8370\n",
      "Epoch 2/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.5279 - accuracy: 0.8218 - val_loss: 0.4460 - val_accuracy: 0.8407\n",
      "Epoch 3/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4769 - accuracy: 0.8383 - val_loss: 0.3937 - val_accuracy: 0.8565\n",
      "Epoch 4/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4503 - accuracy: 0.8453 - val_loss: 0.3798 - val_accuracy: 0.8601\n",
      "Epoch 5/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4324 - accuracy: 0.8514 - val_loss: 0.3868 - val_accuracy: 0.8597\n",
      "Epoch 6/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4172 - accuracy: 0.8546 - val_loss: 0.3942 - val_accuracy: 0.8599\n",
      "Epoch 7/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4044 - accuracy: 0.8602 - val_loss: 0.3845 - val_accuracy: 0.8647\n",
      "Epoch 8/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3962 - accuracy: 0.8639 - val_loss: 0.3619 - val_accuracy: 0.8720\n",
      "Epoch 9/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3899 - accuracy: 0.8657 - val_loss: 0.3536 - val_accuracy: 0.8746\n",
      "Epoch 10/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3827 - accuracy: 0.8679 - val_loss: 0.3738 - val_accuracy: 0.8664\n",
      "Epoch 11/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3774 - accuracy: 0.8701 - val_loss: 0.3766 - val_accuracy: 0.8633\n",
      "Epoch 12/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3660 - accuracy: 0.8730 - val_loss: 0.3569 - val_accuracy: 0.8733\n",
      "Epoch 13/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3672 - accuracy: 0.8720 - val_loss: 0.3672 - val_accuracy: 0.8711\n",
      "Epoch 14/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3610 - accuracy: 0.8749 - val_loss: 0.3537 - val_accuracy: 0.8735\n",
      "Epoch 15/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3551 - accuracy: 0.8763 - val_loss: 0.3411 - val_accuracy: 0.8789\n",
      "Epoch 16/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3538 - accuracy: 0.8759 - val_loss: 0.3427 - val_accuracy: 0.8804\n",
      "Epoch 17/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3470 - accuracy: 0.8800 - val_loss: 0.3473 - val_accuracy: 0.8790\n",
      "Epoch 18/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3427 - accuracy: 0.8814 - val_loss: 0.3425 - val_accuracy: 0.8817\n",
      "Epoch 19/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3418 - accuracy: 0.8818 - val_loss: 0.3573 - val_accuracy: 0.8761\n",
      "Epoch 20/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3368 - accuracy: 0.8829 - val_loss: 0.3393 - val_accuracy: 0.8831\n",
      "Epoch 21/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3345 - accuracy: 0.8842 - val_loss: 0.3673 - val_accuracy: 0.8639\n",
      "Epoch 22/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3352 - accuracy: 0.8830 - val_loss: 0.3513 - val_accuracy: 0.8771\n",
      "Epoch 23/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3328 - accuracy: 0.8843 - val_loss: 0.3504 - val_accuracy: 0.8792\n",
      "Epoch 24/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3272 - accuracy: 0.8865 - val_loss: 0.3378 - val_accuracy: 0.8791\n",
      "Epoch 25/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3299 - accuracy: 0.8853 - val_loss: 0.3375 - val_accuracy: 0.8814\n",
      "Epoch 26/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3243 - accuracy: 0.8869 - val_loss: 0.3855 - val_accuracy: 0.8622\n",
      "Epoch 27/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3268 - accuracy: 0.8876 - val_loss: 0.3559 - val_accuracy: 0.8747\n",
      "Epoch 28/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3181 - accuracy: 0.8892 - val_loss: 0.3500 - val_accuracy: 0.8814\n",
      "Epoch 29/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3177 - accuracy: 0.8893 - val_loss: 0.3740 - val_accuracy: 0.8710\n",
      "Epoch 30/30\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3142 - accuracy: 0.8917 - val_loss: 0.3368 - val_accuracy: 0.8842\n",
      "313/313 - 0s - loss: 0.3631 - accuracy: 0.8785 - 203ms/epoch - 648us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.8917499780654907\n",
      "Validation accuracy : 0.8842499852180481\n",
      "Loss : 0.36314547061920166\n",
      "Accuracy : 0.8784999847412109\n",
      "\n",
      "Train Accuracy: 0.8917499780654907, Validation Accuracy: 0.8842499852180481\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with RMSprop, LR: 0.0005, Batch Size: 32\n",
      "Epoch 1/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.8445 - accuracy: 0.7142 - val_loss: 0.4733 - val_accuracy: 0.8297\n",
      "Epoch 2/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.5892 - accuracy: 0.8023 - val_loss: 0.4392 - val_accuracy: 0.8456\n",
      "Epoch 3/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.5365 - accuracy: 0.8220 - val_loss: 0.4044 - val_accuracy: 0.8565\n",
      "Epoch 4/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.5054 - accuracy: 0.8318 - val_loss: 0.3833 - val_accuracy: 0.8611\n",
      "Epoch 5/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4869 - accuracy: 0.8371 - val_loss: 0.3790 - val_accuracy: 0.8626\n",
      "Epoch 6/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4694 - accuracy: 0.8428 - val_loss: 0.3831 - val_accuracy: 0.8663\n",
      "Epoch 7/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4583 - accuracy: 0.8486 - val_loss: 0.3686 - val_accuracy: 0.8698\n",
      "Epoch 8/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4516 - accuracy: 0.8485 - val_loss: 0.3927 - val_accuracy: 0.8633\n",
      "Epoch 9/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4449 - accuracy: 0.8520 - val_loss: 0.3666 - val_accuracy: 0.8709\n",
      "Epoch 10/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4385 - accuracy: 0.8535 - val_loss: 0.3665 - val_accuracy: 0.8732\n",
      "Epoch 11/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4279 - accuracy: 0.8571 - val_loss: 0.3429 - val_accuracy: 0.8793\n",
      "Epoch 12/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4250 - accuracy: 0.8575 - val_loss: 0.3532 - val_accuracy: 0.8792\n",
      "Epoch 13/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4212 - accuracy: 0.8622 - val_loss: 0.3693 - val_accuracy: 0.8710\n",
      "Epoch 14/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4166 - accuracy: 0.8606 - val_loss: 0.3778 - val_accuracy: 0.8704\n",
      "Epoch 15/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4154 - accuracy: 0.8635 - val_loss: 0.3391 - val_accuracy: 0.8800\n",
      "Epoch 16/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4101 - accuracy: 0.8637 - val_loss: 0.3499 - val_accuracy: 0.8813\n",
      "Epoch 17/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4092 - accuracy: 0.8652 - val_loss: 0.3670 - val_accuracy: 0.8716\n",
      "Epoch 18/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4016 - accuracy: 0.8687 - val_loss: 0.3440 - val_accuracy: 0.8791\n",
      "Epoch 19/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3995 - accuracy: 0.8668 - val_loss: 0.3511 - val_accuracy: 0.8788\n",
      "Epoch 20/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.3944 - accuracy: 0.8697 - val_loss: 0.3561 - val_accuracy: 0.8748\n",
      "Epoch 21/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3922 - accuracy: 0.8694 - val_loss: 0.3619 - val_accuracy: 0.8733\n",
      "Epoch 22/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3931 - accuracy: 0.8698 - val_loss: 0.3523 - val_accuracy: 0.8768\n",
      "Epoch 23/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3927 - accuracy: 0.8694 - val_loss: 0.3382 - val_accuracy: 0.8861\n",
      "Epoch 24/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3866 - accuracy: 0.8732 - val_loss: 0.3365 - val_accuracy: 0.8823\n",
      "Epoch 25/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3862 - accuracy: 0.8727 - val_loss: 0.3369 - val_accuracy: 0.8843\n",
      "Epoch 26/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3865 - accuracy: 0.8720 - val_loss: 0.3564 - val_accuracy: 0.8761\n",
      "Epoch 27/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3772 - accuracy: 0.8752 - val_loss: 0.3480 - val_accuracy: 0.8823\n",
      "Epoch 28/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3811 - accuracy: 0.8744 - val_loss: 0.3475 - val_accuracy: 0.8812\n",
      "Epoch 29/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3762 - accuracy: 0.8765 - val_loss: 0.3738 - val_accuracy: 0.8753\n",
      "Epoch 30/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3822 - accuracy: 0.8755 - val_loss: 0.3547 - val_accuracy: 0.8809\n",
      "313/313 - 0s - loss: 0.3833 - accuracy: 0.8734 - 307ms/epoch - 982us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.8754583597183228\n",
      "Validation accuracy : 0.8809166550636292\n",
      "Loss : 0.38332247734069824\n",
      "Accuracy : 0.8733999729156494\n",
      "\n",
      "Train Accuracy: 0.8754583597183228, Validation Accuracy: 0.8809166550636292\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with RMSprop, LR: 0.0005, Batch Size: 64\n",
      "Epoch 1/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.8408 - accuracy: 0.7174 - val_loss: 0.4744 - val_accuracy: 0.8338\n",
      "Epoch 2/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.5632 - accuracy: 0.8087 - val_loss: 0.4210 - val_accuracy: 0.8521\n",
      "Epoch 3/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.5089 - accuracy: 0.8299 - val_loss: 0.4210 - val_accuracy: 0.8505\n",
      "Epoch 4/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.4750 - accuracy: 0.8389 - val_loss: 0.4094 - val_accuracy: 0.8488\n",
      "Epoch 5/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.4524 - accuracy: 0.8481 - val_loss: 0.4003 - val_accuracy: 0.8559\n",
      "Epoch 6/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.4404 - accuracy: 0.8500 - val_loss: 0.4103 - val_accuracy: 0.8533\n",
      "Epoch 7/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.4279 - accuracy: 0.8561 - val_loss: 0.3676 - val_accuracy: 0.8717\n",
      "Epoch 8/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.4172 - accuracy: 0.8605 - val_loss: 0.3661 - val_accuracy: 0.8713\n",
      "Epoch 9/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.4030 - accuracy: 0.8652 - val_loss: 0.3602 - val_accuracy: 0.8745\n",
      "Epoch 10/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.4015 - accuracy: 0.8640 - val_loss: 0.3565 - val_accuracy: 0.8714\n",
      "Epoch 11/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3910 - accuracy: 0.8678 - val_loss: 0.3486 - val_accuracy: 0.8789\n",
      "Epoch 12/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3844 - accuracy: 0.8694 - val_loss: 0.3672 - val_accuracy: 0.8770\n",
      "Epoch 13/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3808 - accuracy: 0.8722 - val_loss: 0.3393 - val_accuracy: 0.8821\n",
      "Epoch 14/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3746 - accuracy: 0.8724 - val_loss: 0.3555 - val_accuracy: 0.8750\n",
      "Epoch 15/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3721 - accuracy: 0.8729 - val_loss: 0.3558 - val_accuracy: 0.8767\n",
      "Epoch 16/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3653 - accuracy: 0.8755 - val_loss: 0.3562 - val_accuracy: 0.8757\n",
      "Epoch 17/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3667 - accuracy: 0.8760 - val_loss: 0.3531 - val_accuracy: 0.8772\n",
      "Epoch 18/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3594 - accuracy: 0.8781 - val_loss: 0.3436 - val_accuracy: 0.8825\n",
      "Epoch 19/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3571 - accuracy: 0.8782 - val_loss: 0.3551 - val_accuracy: 0.8798\n",
      "Epoch 20/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3520 - accuracy: 0.8800 - val_loss: 0.3370 - val_accuracy: 0.8831\n",
      "Epoch 21/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3519 - accuracy: 0.8810 - val_loss: 0.3294 - val_accuracy: 0.8880\n",
      "Epoch 22/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.3513 - accuracy: 0.8804 - val_loss: 0.3362 - val_accuracy: 0.8814\n",
      "Epoch 23/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.3475 - accuracy: 0.8818 - val_loss: 0.3424 - val_accuracy: 0.8810\n",
      "Epoch 24/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.3403 - accuracy: 0.8838 - val_loss: 0.3340 - val_accuracy: 0.8857\n",
      "Epoch 25/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.3398 - accuracy: 0.8860 - val_loss: 0.3311 - val_accuracy: 0.8875\n",
      "Epoch 26/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.3386 - accuracy: 0.8849 - val_loss: 0.3341 - val_accuracy: 0.8825\n",
      "Epoch 27/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.3331 - accuracy: 0.8867 - val_loss: 0.3298 - val_accuracy: 0.8836\n",
      "Epoch 28/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.3341 - accuracy: 0.8868 - val_loss: 0.3379 - val_accuracy: 0.8829\n",
      "Epoch 29/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.3305 - accuracy: 0.8872 - val_loss: 0.3345 - val_accuracy: 0.8852\n",
      "Epoch 30/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.3307 - accuracy: 0.8884 - val_loss: 0.3460 - val_accuracy: 0.8825\n",
      "313/313 - 0s - loss: 0.3713 - accuracy: 0.8756 - 195ms/epoch - 622us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.8883958458900452\n",
      "Validation accuracy : 0.8824999928474426\n",
      "Loss : 0.3712642788887024\n",
      "Accuracy : 0.8755999803543091\n",
      "\n",
      "Train Accuracy: 0.8883958458900452, Validation Accuracy: 0.8824999928474426\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with RMSprop, LR: 0.0005, Batch Size: 128\n",
      "Epoch 1/30\n",
      "375/375 [==============================] - 2s 3ms/step - loss: 0.8748 - accuracy: 0.7085 - val_loss: 0.4768 - val_accuracy: 0.8309\n",
      "Epoch 2/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.5746 - accuracy: 0.8065 - val_loss: 0.4332 - val_accuracy: 0.8409\n",
      "Epoch 3/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.5045 - accuracy: 0.8277 - val_loss: 0.4062 - val_accuracy: 0.8543\n",
      "Epoch 4/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4743 - accuracy: 0.8405 - val_loss: 0.3835 - val_accuracy: 0.8627\n",
      "Epoch 5/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4497 - accuracy: 0.8459 - val_loss: 0.3713 - val_accuracy: 0.8683\n",
      "Epoch 6/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4286 - accuracy: 0.8539 - val_loss: 0.3696 - val_accuracy: 0.8680\n",
      "Epoch 7/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4134 - accuracy: 0.8577 - val_loss: 0.3569 - val_accuracy: 0.8748\n",
      "Epoch 8/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4051 - accuracy: 0.8624 - val_loss: 0.3831 - val_accuracy: 0.8628\n",
      "Epoch 9/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3925 - accuracy: 0.8681 - val_loss: 0.3700 - val_accuracy: 0.8691\n",
      "Epoch 10/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3877 - accuracy: 0.8683 - val_loss: 0.3559 - val_accuracy: 0.8731\n",
      "Epoch 11/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3792 - accuracy: 0.8697 - val_loss: 0.3560 - val_accuracy: 0.8785\n",
      "Epoch 12/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3712 - accuracy: 0.8742 - val_loss: 0.3695 - val_accuracy: 0.8706\n",
      "Epoch 13/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3631 - accuracy: 0.8759 - val_loss: 0.3430 - val_accuracy: 0.8785\n",
      "Epoch 14/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3616 - accuracy: 0.8775 - val_loss: 0.3410 - val_accuracy: 0.8807\n",
      "Epoch 15/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3516 - accuracy: 0.8802 - val_loss: 0.3715 - val_accuracy: 0.8696\n",
      "Epoch 16/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3531 - accuracy: 0.8795 - val_loss: 0.3573 - val_accuracy: 0.8773\n",
      "Epoch 17/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3452 - accuracy: 0.8816 - val_loss: 0.3428 - val_accuracy: 0.8810\n",
      "Epoch 18/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3403 - accuracy: 0.8830 - val_loss: 0.3378 - val_accuracy: 0.8825\n",
      "Epoch 19/30\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.3384 - accuracy: 0.8835 - val_loss: 0.3407 - val_accuracy: 0.8805\n",
      "Epoch 20/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3313 - accuracy: 0.8850 - val_loss: 0.3439 - val_accuracy: 0.8792\n",
      "Epoch 21/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3304 - accuracy: 0.8862 - val_loss: 0.3411 - val_accuracy: 0.8808\n",
      "Epoch 22/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3274 - accuracy: 0.8875 - val_loss: 0.3321 - val_accuracy: 0.8842\n",
      "Epoch 23/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3235 - accuracy: 0.8897 - val_loss: 0.3329 - val_accuracy: 0.8844\n",
      "Epoch 24/30\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3208 - accuracy: 0.8897 - val_loss: 0.3354 - val_accuracy: 0.8826\n",
      "Epoch 25/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3205 - accuracy: 0.8900 - val_loss: 0.3469 - val_accuracy: 0.8806\n",
      "Epoch 26/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3199 - accuracy: 0.8896 - val_loss: 0.3385 - val_accuracy: 0.8823\n",
      "Epoch 27/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3124 - accuracy: 0.8936 - val_loss: 0.3387 - val_accuracy: 0.8820\n",
      "Epoch 28/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3095 - accuracy: 0.8930 - val_loss: 0.3342 - val_accuracy: 0.8842\n",
      "Epoch 29/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3056 - accuracy: 0.8945 - val_loss: 0.3306 - val_accuracy: 0.8866\n",
      "Epoch 30/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3049 - accuracy: 0.8951 - val_loss: 0.3285 - val_accuracy: 0.8866\n",
      "313/313 - 0s - loss: 0.3523 - accuracy: 0.8811 - 187ms/epoch - 599us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.895145833492279\n",
      "Validation accuracy : 0.8865833282470703\n",
      "Loss : 0.3522811532020569\n",
      "Accuracy : 0.8810999989509583\n",
      "\n",
      "Train Accuracy: 0.895145833492279, Validation Accuracy: 0.8865833282470703\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with RMSprop, LR: 0.0001, Batch Size: 32\n",
      "Epoch 1/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 1.2765 - accuracy: 0.5753 - val_loss: 0.6124 - val_accuracy: 0.8009\n",
      "Epoch 2/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.7862 - accuracy: 0.7413 - val_loss: 0.4926 - val_accuracy: 0.8313\n",
      "Epoch 3/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.6745 - accuracy: 0.7783 - val_loss: 0.4399 - val_accuracy: 0.8457\n",
      "Epoch 4/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.6136 - accuracy: 0.7991 - val_loss: 0.4169 - val_accuracy: 0.8542\n",
      "Epoch 5/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.5725 - accuracy: 0.8136 - val_loss: 0.4032 - val_accuracy: 0.8562\n",
      "Epoch 6/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.5447 - accuracy: 0.8204 - val_loss: 0.3861 - val_accuracy: 0.8637\n",
      "Epoch 7/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.5284 - accuracy: 0.8271 - val_loss: 0.3825 - val_accuracy: 0.8614\n",
      "Epoch 8/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.5074 - accuracy: 0.8335 - val_loss: 0.3835 - val_accuracy: 0.8620\n",
      "Epoch 9/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4941 - accuracy: 0.8398 - val_loss: 0.3665 - val_accuracy: 0.8683\n",
      "Epoch 10/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4802 - accuracy: 0.8421 - val_loss: 0.3652 - val_accuracy: 0.8685\n",
      "Epoch 11/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4702 - accuracy: 0.8460 - val_loss: 0.3585 - val_accuracy: 0.8713\n",
      "Epoch 12/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4639 - accuracy: 0.8473 - val_loss: 0.3568 - val_accuracy: 0.8748\n",
      "Epoch 13/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4548 - accuracy: 0.8495 - val_loss: 0.3587 - val_accuracy: 0.8700\n",
      "Epoch 14/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4453 - accuracy: 0.8536 - val_loss: 0.3499 - val_accuracy: 0.8764\n",
      "Epoch 15/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4436 - accuracy: 0.8556 - val_loss: 0.3552 - val_accuracy: 0.8731\n",
      "Epoch 16/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4352 - accuracy: 0.8582 - val_loss: 0.3495 - val_accuracy: 0.8765\n",
      "Epoch 17/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4359 - accuracy: 0.8579 - val_loss: 0.3598 - val_accuracy: 0.8723\n",
      "Epoch 18/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4251 - accuracy: 0.8597 - val_loss: 0.3511 - val_accuracy: 0.8746\n",
      "Epoch 19/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4229 - accuracy: 0.8598 - val_loss: 0.3430 - val_accuracy: 0.8799\n",
      "Epoch 20/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4126 - accuracy: 0.8649 - val_loss: 0.3461 - val_accuracy: 0.8799\n",
      "Epoch 21/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4110 - accuracy: 0.8644 - val_loss: 0.3369 - val_accuracy: 0.8813\n",
      "Epoch 22/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4064 - accuracy: 0.8663 - val_loss: 0.3392 - val_accuracy: 0.8790\n",
      "Epoch 23/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4053 - accuracy: 0.8677 - val_loss: 0.3347 - val_accuracy: 0.8817\n",
      "Epoch 24/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.3961 - accuracy: 0.8680 - val_loss: 0.3435 - val_accuracy: 0.8802\n",
      "Epoch 25/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.3966 - accuracy: 0.8708 - val_loss: 0.3370 - val_accuracy: 0.8819\n",
      "Epoch 26/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.3919 - accuracy: 0.8694 - val_loss: 0.3320 - val_accuracy: 0.8859\n",
      "Epoch 27/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3896 - accuracy: 0.8720 - val_loss: 0.3363 - val_accuracy: 0.8828\n",
      "Epoch 28/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.3850 - accuracy: 0.8726 - val_loss: 0.3392 - val_accuracy: 0.8827\n",
      "Epoch 29/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.3805 - accuracy: 0.8740 - val_loss: 0.3289 - val_accuracy: 0.8834\n",
      "Epoch 30/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.3798 - accuracy: 0.8751 - val_loss: 0.3364 - val_accuracy: 0.8825\n",
      "313/313 - 0s - loss: 0.3640 - accuracy: 0.8756 - 296ms/epoch - 947us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.8751041889190674\n",
      "Validation accuracy : 0.8824999928474426\n",
      "Loss : 0.36396268010139465\n",
      "Accuracy : 0.8755999803543091\n",
      "\n",
      "Train Accuracy: 0.8751041889190674, Validation Accuracy: 0.8824999928474426\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with RMSprop, LR: 0.0001, Batch Size: 64\n",
      "Epoch 1/30\n",
      "750/750 [==============================] - 3s 2ms/step - loss: 1.3441 - accuracy: 0.5469 - val_loss: 0.6781 - val_accuracy: 0.7780\n",
      "Epoch 2/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.8308 - accuracy: 0.7270 - val_loss: 0.5322 - val_accuracy: 0.8213\n",
      "Epoch 3/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.6948 - accuracy: 0.7735 - val_loss: 0.4770 - val_accuracy: 0.8345\n",
      "Epoch 4/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.6294 - accuracy: 0.7946 - val_loss: 0.4443 - val_accuracy: 0.8454\n",
      "Epoch 5/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.5848 - accuracy: 0.8084 - val_loss: 0.4266 - val_accuracy: 0.8509\n",
      "Epoch 6/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.5585 - accuracy: 0.8188 - val_loss: 0.4154 - val_accuracy: 0.8525\n",
      "Epoch 7/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.5285 - accuracy: 0.8263 - val_loss: 0.3988 - val_accuracy: 0.8593\n",
      "Epoch 8/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.5122 - accuracy: 0.8295 - val_loss: 0.3934 - val_accuracy: 0.8614\n",
      "Epoch 9/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.4988 - accuracy: 0.8363 - val_loss: 0.3860 - val_accuracy: 0.8627\n",
      "Epoch 10/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.4838 - accuracy: 0.8396 - val_loss: 0.3789 - val_accuracy: 0.8665\n",
      "Epoch 11/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.4677 - accuracy: 0.8443 - val_loss: 0.3783 - val_accuracy: 0.8653\n",
      "Epoch 12/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.4612 - accuracy: 0.8477 - val_loss: 0.3701 - val_accuracy: 0.8663\n",
      "Epoch 13/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.4533 - accuracy: 0.8501 - val_loss: 0.3689 - val_accuracy: 0.8693\n",
      "Epoch 14/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.4415 - accuracy: 0.8527 - val_loss: 0.3633 - val_accuracy: 0.8709\n",
      "Epoch 15/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.4390 - accuracy: 0.8540 - val_loss: 0.3585 - val_accuracy: 0.8730\n",
      "Epoch 16/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.4309 - accuracy: 0.8554 - val_loss: 0.3516 - val_accuracy: 0.8746\n",
      "Epoch 17/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.4216 - accuracy: 0.8602 - val_loss: 0.3596 - val_accuracy: 0.8731\n",
      "Epoch 18/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.4149 - accuracy: 0.8616 - val_loss: 0.3484 - val_accuracy: 0.8760\n",
      "Epoch 19/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.4121 - accuracy: 0.8625 - val_loss: 0.3458 - val_accuracy: 0.8774\n",
      "Epoch 20/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.4091 - accuracy: 0.8631 - val_loss: 0.3437 - val_accuracy: 0.8760\n",
      "Epoch 21/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.4022 - accuracy: 0.8657 - val_loss: 0.3476 - val_accuracy: 0.8776\n",
      "Epoch 22/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.3990 - accuracy: 0.8664 - val_loss: 0.3389 - val_accuracy: 0.8798\n",
      "Epoch 23/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.3951 - accuracy: 0.8693 - val_loss: 0.3435 - val_accuracy: 0.8798\n",
      "Epoch 24/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.3918 - accuracy: 0.8710 - val_loss: 0.3430 - val_accuracy: 0.8800\n",
      "Epoch 25/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.3847 - accuracy: 0.8711 - val_loss: 0.3416 - val_accuracy: 0.8813\n",
      "Epoch 26/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.3781 - accuracy: 0.8733 - val_loss: 0.3343 - val_accuracy: 0.8840\n",
      "Epoch 27/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.3771 - accuracy: 0.8728 - val_loss: 0.3390 - val_accuracy: 0.8808\n",
      "Epoch 28/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.3768 - accuracy: 0.8745 - val_loss: 0.3334 - val_accuracy: 0.8826\n",
      "Epoch 29/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.3736 - accuracy: 0.8756 - val_loss: 0.3338 - val_accuracy: 0.8827\n",
      "Epoch 30/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.3665 - accuracy: 0.8779 - val_loss: 0.3314 - val_accuracy: 0.8841\n",
      "313/313 - 0s - loss: 0.3623 - accuracy: 0.8755 - 186ms/epoch - 593us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.877916693687439\n",
      "Validation accuracy : 0.8840833306312561\n",
      "Loss : 0.3622831702232361\n",
      "Accuracy : 0.8755000233650208\n",
      "\n",
      "Train Accuracy: 0.877916693687439, Validation Accuracy: 0.8840833306312561\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with RMSprop, LR: 0.0001, Batch Size: 128\n",
      "Epoch 1/30\n",
      "375/375 [==============================] - 2s 3ms/step - loss: 1.5426 - accuracy: 0.4909 - val_loss: 0.8411 - val_accuracy: 0.7596\n",
      "Epoch 2/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.9357 - accuracy: 0.6985 - val_loss: 0.5858 - val_accuracy: 0.8091\n",
      "Epoch 3/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.7658 - accuracy: 0.7541 - val_loss: 0.5075 - val_accuracy: 0.8288\n",
      "Epoch 4/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.6758 - accuracy: 0.7814 - val_loss: 0.4698 - val_accuracy: 0.8381\n",
      "Epoch 5/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.6165 - accuracy: 0.7984 - val_loss: 0.4419 - val_accuracy: 0.8449\n",
      "Epoch 6/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.5843 - accuracy: 0.8095 - val_loss: 0.4269 - val_accuracy: 0.8501\n",
      "Epoch 7/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.5546 - accuracy: 0.8185 - val_loss: 0.4102 - val_accuracy: 0.8561\n",
      "Epoch 8/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.5301 - accuracy: 0.8227 - val_loss: 0.4055 - val_accuracy: 0.8557\n",
      "Epoch 9/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.5086 - accuracy: 0.8314 - val_loss: 0.3955 - val_accuracy: 0.8583\n",
      "Epoch 10/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4966 - accuracy: 0.8339 - val_loss: 0.3862 - val_accuracy: 0.8631\n",
      "Epoch 11/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4864 - accuracy: 0.8371 - val_loss: 0.3822 - val_accuracy: 0.8650\n",
      "Epoch 12/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4743 - accuracy: 0.8416 - val_loss: 0.3799 - val_accuracy: 0.8672\n",
      "Epoch 13/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4629 - accuracy: 0.8472 - val_loss: 0.3758 - val_accuracy: 0.8678\n",
      "Epoch 14/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4541 - accuracy: 0.8476 - val_loss: 0.3715 - val_accuracy: 0.8693\n",
      "Epoch 15/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4470 - accuracy: 0.8510 - val_loss: 0.3691 - val_accuracy: 0.8698\n",
      "Epoch 16/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4308 - accuracy: 0.8566 - val_loss: 0.3673 - val_accuracy: 0.8707\n",
      "Epoch 17/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4332 - accuracy: 0.8536 - val_loss: 0.3634 - val_accuracy: 0.8730\n",
      "Epoch 18/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4222 - accuracy: 0.8580 - val_loss: 0.3584 - val_accuracy: 0.8741\n",
      "Epoch 19/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4201 - accuracy: 0.8591 - val_loss: 0.3584 - val_accuracy: 0.8733\n",
      "Epoch 20/30\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.4146 - accuracy: 0.8616 - val_loss: 0.3621 - val_accuracy: 0.8744\n",
      "Epoch 21/30\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.4084 - accuracy: 0.8616 - val_loss: 0.3609 - val_accuracy: 0.8723\n",
      "Epoch 22/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4027 - accuracy: 0.8649 - val_loss: 0.3503 - val_accuracy: 0.8781\n",
      "Epoch 23/30\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3963 - accuracy: 0.8673 - val_loss: 0.3507 - val_accuracy: 0.8782\n",
      "Epoch 24/30\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.3945 - accuracy: 0.8669 - val_loss: 0.3490 - val_accuracy: 0.8796\n",
      "Epoch 25/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3897 - accuracy: 0.8683 - val_loss: 0.3496 - val_accuracy: 0.8782\n",
      "Epoch 26/30\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3832 - accuracy: 0.8700 - val_loss: 0.3447 - val_accuracy: 0.8826\n",
      "Epoch 27/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3813 - accuracy: 0.8722 - val_loss: 0.3440 - val_accuracy: 0.8820\n",
      "Epoch 28/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3772 - accuracy: 0.8727 - val_loss: 0.3446 - val_accuracy: 0.8803\n",
      "Epoch 29/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3726 - accuracy: 0.8735 - val_loss: 0.3417 - val_accuracy: 0.8821\n",
      "Epoch 30/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3740 - accuracy: 0.8733 - val_loss: 0.3430 - val_accuracy: 0.8816\n",
      "313/313 - 0s - loss: 0.3650 - accuracy: 0.8764 - 190ms/epoch - 607us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.8733333349227905\n",
      "Validation accuracy : 0.8815833330154419\n",
      "Loss : 0.364958256483078\n",
      "Accuracy : 0.8763999938964844\n",
      "\n",
      "Train Accuracy: 0.8733333349227905, Validation Accuracy: 0.8815833330154419\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with Adam, LR: 0.001, Batch Size: 32\n",
      "Epoch 1/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.7988 - accuracy: 0.7276 - val_loss: 0.4986 - val_accuracy: 0.8175\n",
      "Epoch 2/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5896 - accuracy: 0.7969 - val_loss: 0.4744 - val_accuracy: 0.8259\n",
      "Epoch 3/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.5515 - accuracy: 0.8124 - val_loss: 0.4100 - val_accuracy: 0.8510\n",
      "Epoch 4/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.5293 - accuracy: 0.8180 - val_loss: 0.4071 - val_accuracy: 0.8518\n",
      "Epoch 5/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.5162 - accuracy: 0.8257 - val_loss: 0.3966 - val_accuracy: 0.8593\n",
      "Epoch 6/30\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.4985 - accuracy: 0.8306 - val_loss: 0.4167 - val_accuracy: 0.8533\n",
      "Epoch 7/30\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.4925 - accuracy: 0.8310 - val_loss: 0.3970 - val_accuracy: 0.8571\n",
      "Epoch 8/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4801 - accuracy: 0.8364 - val_loss: 0.3947 - val_accuracy: 0.8589\n",
      "Epoch 9/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4753 - accuracy: 0.8386 - val_loss: 0.3888 - val_accuracy: 0.8602\n",
      "Epoch 10/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4733 - accuracy: 0.8394 - val_loss: 0.3920 - val_accuracy: 0.8593\n",
      "Epoch 11/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4647 - accuracy: 0.8423 - val_loss: 0.3695 - val_accuracy: 0.8669\n",
      "Epoch 12/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4560 - accuracy: 0.8431 - val_loss: 0.3720 - val_accuracy: 0.8681\n",
      "Epoch 13/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4541 - accuracy: 0.8433 - val_loss: 0.3739 - val_accuracy: 0.8675\n",
      "Epoch 14/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4478 - accuracy: 0.8468 - val_loss: 0.3694 - val_accuracy: 0.8683\n",
      "Epoch 15/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4413 - accuracy: 0.8481 - val_loss: 0.3700 - val_accuracy: 0.8667\n",
      "Epoch 16/30\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 0.4406 - accuracy: 0.8483 - val_loss: 0.3667 - val_accuracy: 0.8670\n",
      "Epoch 17/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4410 - accuracy: 0.8482 - val_loss: 0.3615 - val_accuracy: 0.8717\n",
      "Epoch 18/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4281 - accuracy: 0.8538 - val_loss: 0.3580 - val_accuracy: 0.8719\n",
      "Epoch 19/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4302 - accuracy: 0.8526 - val_loss: 0.3613 - val_accuracy: 0.8711\n",
      "Epoch 20/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4296 - accuracy: 0.8532 - val_loss: 0.3742 - val_accuracy: 0.8673\n",
      "Epoch 21/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4230 - accuracy: 0.8541 - val_loss: 0.3623 - val_accuracy: 0.8689\n",
      "Epoch 22/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4202 - accuracy: 0.8534 - val_loss: 0.3741 - val_accuracy: 0.8676\n",
      "Epoch 23/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4226 - accuracy: 0.8559 - val_loss: 0.3629 - val_accuracy: 0.8715\n",
      "Epoch 24/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4206 - accuracy: 0.8573 - val_loss: 0.3603 - val_accuracy: 0.8730\n",
      "Epoch 25/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4115 - accuracy: 0.8580 - val_loss: 0.3597 - val_accuracy: 0.8748\n",
      "Epoch 26/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4162 - accuracy: 0.8580 - val_loss: 0.3639 - val_accuracy: 0.8673\n",
      "Epoch 27/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4131 - accuracy: 0.8580 - val_loss: 0.3689 - val_accuracy: 0.8687\n",
      "Epoch 28/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4118 - accuracy: 0.8577 - val_loss: 0.3630 - val_accuracy: 0.8697\n",
      "Epoch 29/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4047 - accuracy: 0.8604 - val_loss: 0.3604 - val_accuracy: 0.8737\n",
      "Epoch 30/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4052 - accuracy: 0.8599 - val_loss: 0.3579 - val_accuracy: 0.8691\n",
      "313/313 - 0s - loss: 0.3843 - accuracy: 0.8644 - 299ms/epoch - 954us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.8598750233650208\n",
      "Validation accuracy : 0.8690833449363708\n",
      "Loss : 0.38432350754737854\n",
      "Accuracy : 0.8644000291824341\n",
      "\n",
      "Train Accuracy: 0.8598750233650208, Validation Accuracy: 0.8690833449363708\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with Adam, LR: 0.001, Batch Size: 64\n",
      "Epoch 1/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.8046 - accuracy: 0.7284 - val_loss: 0.4848 - val_accuracy: 0.8183\n",
      "Epoch 2/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.5602 - accuracy: 0.8101 - val_loss: 0.4300 - val_accuracy: 0.8410\n",
      "Epoch 3/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.5072 - accuracy: 0.8258 - val_loss: 0.3882 - val_accuracy: 0.8601\n",
      "Epoch 4/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.4847 - accuracy: 0.8313 - val_loss: 0.3922 - val_accuracy: 0.8572\n",
      "Epoch 5/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.4593 - accuracy: 0.8413 - val_loss: 0.4081 - val_accuracy: 0.8506\n",
      "Epoch 6/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.4544 - accuracy: 0.8430 - val_loss: 0.3926 - val_accuracy: 0.8513\n",
      "Epoch 7/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.4463 - accuracy: 0.8477 - val_loss: 0.3891 - val_accuracy: 0.8623\n",
      "Epoch 8/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.4329 - accuracy: 0.8502 - val_loss: 0.3715 - val_accuracy: 0.8677\n",
      "Epoch 9/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.4298 - accuracy: 0.8505 - val_loss: 0.3664 - val_accuracy: 0.8673\n",
      "Epoch 10/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.4236 - accuracy: 0.8533 - val_loss: 0.3636 - val_accuracy: 0.8692\n",
      "Epoch 11/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.4310 - accuracy: 0.8497 - val_loss: 0.3685 - val_accuracy: 0.8659\n",
      "Epoch 12/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.4226 - accuracy: 0.8515 - val_loss: 0.3531 - val_accuracy: 0.8711\n",
      "Epoch 13/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.4050 - accuracy: 0.8592 - val_loss: 0.3923 - val_accuracy: 0.8572\n",
      "Epoch 14/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.3982 - accuracy: 0.8609 - val_loss: 0.3700 - val_accuracy: 0.8674\n",
      "Epoch 15/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.3989 - accuracy: 0.8606 - val_loss: 0.3691 - val_accuracy: 0.8652\n",
      "Epoch 16/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3915 - accuracy: 0.8641 - val_loss: 0.3434 - val_accuracy: 0.8787\n",
      "Epoch 17/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.3849 - accuracy: 0.8664 - val_loss: 0.3569 - val_accuracy: 0.8726\n",
      "Epoch 18/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3853 - accuracy: 0.8656 - val_loss: 0.3390 - val_accuracy: 0.8802\n",
      "Epoch 19/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.3746 - accuracy: 0.8674 - val_loss: 0.3555 - val_accuracy: 0.8738\n",
      "Epoch 20/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.3768 - accuracy: 0.8684 - val_loss: 0.3545 - val_accuracy: 0.8751\n",
      "Epoch 21/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.3732 - accuracy: 0.8691 - val_loss: 0.3482 - val_accuracy: 0.8752\n",
      "Epoch 22/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3682 - accuracy: 0.8720 - val_loss: 0.3427 - val_accuracy: 0.8787\n",
      "Epoch 23/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3643 - accuracy: 0.8723 - val_loss: 0.3332 - val_accuracy: 0.8798\n",
      "Epoch 24/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.3573 - accuracy: 0.8748 - val_loss: 0.3381 - val_accuracy: 0.8814\n",
      "Epoch 25/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.3558 - accuracy: 0.8753 - val_loss: 0.3416 - val_accuracy: 0.8796\n",
      "Epoch 26/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.3608 - accuracy: 0.8732 - val_loss: 0.3497 - val_accuracy: 0.8749\n",
      "Epoch 27/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.3535 - accuracy: 0.8751 - val_loss: 0.3484 - val_accuracy: 0.8794\n",
      "Epoch 28/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3576 - accuracy: 0.8723 - val_loss: 0.4062 - val_accuracy: 0.8637\n",
      "Epoch 29/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.3492 - accuracy: 0.8767 - val_loss: 0.3484 - val_accuracy: 0.8774\n",
      "Epoch 30/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.3465 - accuracy: 0.8785 - val_loss: 0.3314 - val_accuracy: 0.8842\n",
      "313/313 - 0s - loss: 0.3613 - accuracy: 0.8755 - 195ms/epoch - 624us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.8784791827201843\n",
      "Validation accuracy : 0.8841666579246521\n",
      "Loss : 0.3612822890281677\n",
      "Accuracy : 0.8755000233650208\n",
      "\n",
      "Train Accuracy: 0.8784791827201843, Validation Accuracy: 0.8841666579246521\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with Adam, LR: 0.001, Batch Size: 128\n",
      "Epoch 1/30\n",
      "375/375 [==============================] - 2s 3ms/step - loss: 0.8421 - accuracy: 0.7194 - val_loss: 0.4792 - val_accuracy: 0.8234\n",
      "Epoch 2/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.5555 - accuracy: 0.8120 - val_loss: 0.4517 - val_accuracy: 0.8417\n",
      "Epoch 3/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4973 - accuracy: 0.8292 - val_loss: 0.4111 - val_accuracy: 0.8533\n",
      "Epoch 4/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4641 - accuracy: 0.8411 - val_loss: 0.3953 - val_accuracy: 0.8557\n",
      "Epoch 5/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4495 - accuracy: 0.8454 - val_loss: 0.4092 - val_accuracy: 0.8503\n",
      "Epoch 6/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4330 - accuracy: 0.8511 - val_loss: 0.3734 - val_accuracy: 0.8660\n",
      "Epoch 7/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4231 - accuracy: 0.8551 - val_loss: 0.4037 - val_accuracy: 0.8557\n",
      "Epoch 8/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4172 - accuracy: 0.8553 - val_loss: 0.3595 - val_accuracy: 0.8742\n",
      "Epoch 9/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4059 - accuracy: 0.8581 - val_loss: 0.4332 - val_accuracy: 0.8493\n",
      "Epoch 10/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3993 - accuracy: 0.8615 - val_loss: 0.3616 - val_accuracy: 0.8691\n",
      "Epoch 11/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3966 - accuracy: 0.8626 - val_loss: 0.3551 - val_accuracy: 0.8733\n",
      "Epoch 12/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3941 - accuracy: 0.8637 - val_loss: 0.3529 - val_accuracy: 0.8750\n",
      "Epoch 13/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3777 - accuracy: 0.8684 - val_loss: 0.3906 - val_accuracy: 0.8580\n",
      "Epoch 14/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3784 - accuracy: 0.8698 - val_loss: 0.3395 - val_accuracy: 0.8784\n",
      "Epoch 15/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3708 - accuracy: 0.8692 - val_loss: 0.3636 - val_accuracy: 0.8692\n",
      "Epoch 16/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3663 - accuracy: 0.8713 - val_loss: 0.3406 - val_accuracy: 0.8790\n",
      "Epoch 17/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3585 - accuracy: 0.8744 - val_loss: 0.3388 - val_accuracy: 0.8787\n",
      "Epoch 18/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3567 - accuracy: 0.8759 - val_loss: 0.3440 - val_accuracy: 0.8776\n",
      "Epoch 19/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3540 - accuracy: 0.8765 - val_loss: 0.3434 - val_accuracy: 0.8809\n",
      "Epoch 20/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3506 - accuracy: 0.8769 - val_loss: 0.3529 - val_accuracy: 0.8755\n",
      "Epoch 21/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3500 - accuracy: 0.8771 - val_loss: 0.3359 - val_accuracy: 0.8801\n",
      "Epoch 22/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3442 - accuracy: 0.8816 - val_loss: 0.3285 - val_accuracy: 0.8842\n",
      "Epoch 23/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3393 - accuracy: 0.8815 - val_loss: 0.3341 - val_accuracy: 0.8821\n",
      "Epoch 24/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3373 - accuracy: 0.8825 - val_loss: 0.3284 - val_accuracy: 0.8816\n",
      "Epoch 25/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3363 - accuracy: 0.8841 - val_loss: 0.3253 - val_accuracy: 0.8856\n",
      "Epoch 26/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3287 - accuracy: 0.8849 - val_loss: 0.3415 - val_accuracy: 0.8793\n",
      "Epoch 27/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3269 - accuracy: 0.8845 - val_loss: 0.3332 - val_accuracy: 0.8846\n",
      "Epoch 28/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3213 - accuracy: 0.8860 - val_loss: 0.3326 - val_accuracy: 0.8817\n",
      "Epoch 29/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3268 - accuracy: 0.8834 - val_loss: 0.3441 - val_accuracy: 0.8811\n",
      "Epoch 30/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3224 - accuracy: 0.8867 - val_loss: 0.3247 - val_accuracy: 0.8851\n",
      "313/313 - 0s - loss: 0.3514 - accuracy: 0.8768 - 192ms/epoch - 614us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.8867499828338623\n",
      "Validation accuracy : 0.8850833177566528\n",
      "Loss : 0.35136979818344116\n",
      "Accuracy : 0.876800000667572\n",
      "\n",
      "Train Accuracy: 0.8867499828338623, Validation Accuracy: 0.8850833177566528\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with Adam, LR: 0.0005, Batch Size: 32\n",
      "Epoch 1/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9054 - accuracy: 0.6977 - val_loss: 0.4796 - val_accuracy: 0.8248\n",
      "Epoch 2/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.6111 - accuracy: 0.7914 - val_loss: 0.4173 - val_accuracy: 0.8491\n",
      "Epoch 3/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.5538 - accuracy: 0.8128 - val_loss: 0.4112 - val_accuracy: 0.8487\n",
      "Epoch 4/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.5233 - accuracy: 0.8181 - val_loss: 0.3932 - val_accuracy: 0.8567\n",
      "Epoch 5/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4989 - accuracy: 0.8290 - val_loss: 0.3815 - val_accuracy: 0.8633\n",
      "Epoch 6/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4875 - accuracy: 0.8334 - val_loss: 0.3798 - val_accuracy: 0.8641\n",
      "Epoch 7/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4717 - accuracy: 0.8380 - val_loss: 0.3706 - val_accuracy: 0.8652\n",
      "Epoch 8/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4612 - accuracy: 0.8433 - val_loss: 0.3818 - val_accuracy: 0.8617\n",
      "Epoch 9/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4543 - accuracy: 0.8443 - val_loss: 0.3683 - val_accuracy: 0.8673\n",
      "Epoch 10/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4483 - accuracy: 0.8459 - val_loss: 0.3638 - val_accuracy: 0.8692\n",
      "Epoch 11/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4447 - accuracy: 0.8490 - val_loss: 0.3643 - val_accuracy: 0.8661\n",
      "Epoch 12/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4408 - accuracy: 0.8493 - val_loss: 0.3653 - val_accuracy: 0.8702\n",
      "Epoch 13/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4351 - accuracy: 0.8506 - val_loss: 0.3540 - val_accuracy: 0.8726\n",
      "Epoch 14/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4230 - accuracy: 0.8553 - val_loss: 0.3585 - val_accuracy: 0.8726\n",
      "Epoch 15/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4207 - accuracy: 0.8569 - val_loss: 0.3509 - val_accuracy: 0.8749\n",
      "Epoch 16/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4202 - accuracy: 0.8556 - val_loss: 0.3640 - val_accuracy: 0.8729\n",
      "Epoch 17/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4195 - accuracy: 0.8547 - val_loss: 0.3567 - val_accuracy: 0.8700\n",
      "Epoch 18/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4115 - accuracy: 0.8591 - val_loss: 0.3439 - val_accuracy: 0.8758\n",
      "Epoch 19/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4030 - accuracy: 0.8622 - val_loss: 0.3404 - val_accuracy: 0.8755\n",
      "Epoch 20/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4006 - accuracy: 0.8618 - val_loss: 0.3500 - val_accuracy: 0.8766\n",
      "Epoch 21/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.3946 - accuracy: 0.8645 - val_loss: 0.3411 - val_accuracy: 0.8796\n",
      "Epoch 22/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3960 - accuracy: 0.8623 - val_loss: 0.3354 - val_accuracy: 0.8783\n",
      "Epoch 23/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.3929 - accuracy: 0.8654 - val_loss: 0.3428 - val_accuracy: 0.8778\n",
      "Epoch 24/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.3884 - accuracy: 0.8670 - val_loss: 0.3401 - val_accuracy: 0.8800\n",
      "Epoch 25/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.3823 - accuracy: 0.8672 - val_loss: 0.3489 - val_accuracy: 0.8764\n",
      "Epoch 26/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3894 - accuracy: 0.8661 - val_loss: 0.3463 - val_accuracy: 0.8738\n",
      "Epoch 27/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.3863 - accuracy: 0.8688 - val_loss: 0.3336 - val_accuracy: 0.8808\n",
      "Epoch 28/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3833 - accuracy: 0.8675 - val_loss: 0.3402 - val_accuracy: 0.8789\n",
      "Epoch 29/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.3827 - accuracy: 0.8685 - val_loss: 0.3396 - val_accuracy: 0.8793\n",
      "Epoch 30/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.3825 - accuracy: 0.8687 - val_loss: 0.3467 - val_accuracy: 0.8788\n",
      "313/313 - 0s - loss: 0.3689 - accuracy: 0.8706 - 302ms/epoch - 965us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.8686666488647461\n",
      "Validation accuracy : 0.8788333535194397\n",
      "Loss : 0.36893925070762634\n",
      "Accuracy : 0.8705999851226807\n",
      "\n",
      "Train Accuracy: 0.8686666488647461, Validation Accuracy: 0.8788333535194397\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with Adam, LR: 0.0005, Batch Size: 64\n",
      "Epoch 1/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.9193 - accuracy: 0.6972 - val_loss: 0.4981 - val_accuracy: 0.8240\n",
      "Epoch 2/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.5954 - accuracy: 0.7990 - val_loss: 0.4259 - val_accuracy: 0.8443\n",
      "Epoch 3/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.5311 - accuracy: 0.8194 - val_loss: 0.4080 - val_accuracy: 0.8503\n",
      "Epoch 4/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.4981 - accuracy: 0.8291 - val_loss: 0.3932 - val_accuracy: 0.8590\n",
      "Epoch 5/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.4814 - accuracy: 0.8363 - val_loss: 0.3877 - val_accuracy: 0.8595\n",
      "Epoch 6/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.4582 - accuracy: 0.8428 - val_loss: 0.3798 - val_accuracy: 0.8603\n",
      "Epoch 7/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.4495 - accuracy: 0.8449 - val_loss: 0.3754 - val_accuracy: 0.8637\n",
      "Epoch 8/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.4333 - accuracy: 0.8519 - val_loss: 0.3630 - val_accuracy: 0.8686\n",
      "Epoch 9/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.4200 - accuracy: 0.8536 - val_loss: 0.3696 - val_accuracy: 0.8658\n",
      "Epoch 10/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.4163 - accuracy: 0.8572 - val_loss: 0.3522 - val_accuracy: 0.8744\n",
      "Epoch 11/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.4044 - accuracy: 0.8613 - val_loss: 0.3819 - val_accuracy: 0.8587\n",
      "Epoch 12/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3959 - accuracy: 0.8633 - val_loss: 0.3465 - val_accuracy: 0.8759\n",
      "Epoch 13/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3936 - accuracy: 0.8633 - val_loss: 0.3382 - val_accuracy: 0.8773\n",
      "Epoch 14/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.3855 - accuracy: 0.8665 - val_loss: 0.3402 - val_accuracy: 0.8753\n",
      "Epoch 15/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3822 - accuracy: 0.8677 - val_loss: 0.3495 - val_accuracy: 0.8752\n",
      "Epoch 16/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3799 - accuracy: 0.8689 - val_loss: 0.3409 - val_accuracy: 0.8792\n",
      "Epoch 17/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3762 - accuracy: 0.8694 - val_loss: 0.3417 - val_accuracy: 0.8768\n",
      "Epoch 18/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3781 - accuracy: 0.8691 - val_loss: 0.3325 - val_accuracy: 0.8801\n",
      "Epoch 19/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3678 - accuracy: 0.8715 - val_loss: 0.3357 - val_accuracy: 0.8740\n",
      "Epoch 20/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.3636 - accuracy: 0.8732 - val_loss: 0.3364 - val_accuracy: 0.8775\n",
      "Epoch 21/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.3656 - accuracy: 0.8728 - val_loss: 0.3288 - val_accuracy: 0.8827\n",
      "Epoch 22/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.3592 - accuracy: 0.8749 - val_loss: 0.3399 - val_accuracy: 0.8808\n",
      "Epoch 23/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.3555 - accuracy: 0.8754 - val_loss: 0.3281 - val_accuracy: 0.8817\n",
      "Epoch 24/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3533 - accuracy: 0.8759 - val_loss: 0.3239 - val_accuracy: 0.8838\n",
      "Epoch 25/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3494 - accuracy: 0.8777 - val_loss: 0.3344 - val_accuracy: 0.8810\n",
      "Epoch 26/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3475 - accuracy: 0.8798 - val_loss: 0.3282 - val_accuracy: 0.8819\n",
      "Epoch 27/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3462 - accuracy: 0.8785 - val_loss: 0.3382 - val_accuracy: 0.8805\n",
      "Epoch 28/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.3393 - accuracy: 0.8816 - val_loss: 0.3274 - val_accuracy: 0.8852\n",
      "Epoch 29/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3342 - accuracy: 0.8839 - val_loss: 0.3291 - val_accuracy: 0.8839\n",
      "Epoch 30/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3342 - accuracy: 0.8839 - val_loss: 0.3426 - val_accuracy: 0.8783\n",
      "313/313 - 0s - loss: 0.3723 - accuracy: 0.8734 - 188ms/epoch - 599us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.8838958144187927\n",
      "Validation accuracy : 0.878250002861023\n",
      "Loss : 0.37232497334480286\n",
      "Accuracy : 0.8733999729156494\n",
      "\n",
      "Train Accuracy: 0.8838958144187927, Validation Accuracy: 0.878250002861023\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with Adam, LR: 0.0005, Batch Size: 128\n",
      "Epoch 1/30\n",
      "375/375 [==============================] - 2s 3ms/step - loss: 0.9865 - accuracy: 0.6738 - val_loss: 0.5227 - val_accuracy: 0.8200\n",
      "Epoch 2/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.6135 - accuracy: 0.7954 - val_loss: 0.4465 - val_accuracy: 0.8384\n",
      "Epoch 3/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.5376 - accuracy: 0.8191 - val_loss: 0.4163 - val_accuracy: 0.8482\n",
      "Epoch 4/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4906 - accuracy: 0.8343 - val_loss: 0.4028 - val_accuracy: 0.8541\n",
      "Epoch 5/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4638 - accuracy: 0.8425 - val_loss: 0.3802 - val_accuracy: 0.8652\n",
      "Epoch 6/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4449 - accuracy: 0.8482 - val_loss: 0.3987 - val_accuracy: 0.8543\n",
      "Epoch 7/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4347 - accuracy: 0.8515 - val_loss: 0.3619 - val_accuracy: 0.8711\n",
      "Epoch 8/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4140 - accuracy: 0.8577 - val_loss: 0.3578 - val_accuracy: 0.8718\n",
      "Epoch 9/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4101 - accuracy: 0.8590 - val_loss: 0.3729 - val_accuracy: 0.8658\n",
      "Epoch 10/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3986 - accuracy: 0.8619 - val_loss: 0.3605 - val_accuracy: 0.8707\n",
      "Epoch 11/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3940 - accuracy: 0.8652 - val_loss: 0.3629 - val_accuracy: 0.8732\n",
      "Epoch 12/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3882 - accuracy: 0.8640 - val_loss: 0.3673 - val_accuracy: 0.8719\n",
      "Epoch 13/30\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3766 - accuracy: 0.8700 - val_loss: 0.3433 - val_accuracy: 0.8777\n",
      "Epoch 14/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3738 - accuracy: 0.8703 - val_loss: 0.3581 - val_accuracy: 0.8694\n",
      "Epoch 15/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3642 - accuracy: 0.8726 - val_loss: 0.3378 - val_accuracy: 0.8794\n",
      "Epoch 16/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3599 - accuracy: 0.8741 - val_loss: 0.3580 - val_accuracy: 0.8735\n",
      "Epoch 17/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3544 - accuracy: 0.8763 - val_loss: 0.3368 - val_accuracy: 0.8808\n",
      "Epoch 18/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3499 - accuracy: 0.8779 - val_loss: 0.3373 - val_accuracy: 0.8781\n",
      "Epoch 19/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3467 - accuracy: 0.8798 - val_loss: 0.3382 - val_accuracy: 0.8828\n",
      "Epoch 20/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3435 - accuracy: 0.8802 - val_loss: 0.3326 - val_accuracy: 0.8834\n",
      "Epoch 21/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3379 - accuracy: 0.8820 - val_loss: 0.3347 - val_accuracy: 0.8819\n",
      "Epoch 22/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3316 - accuracy: 0.8841 - val_loss: 0.3272 - val_accuracy: 0.8876\n",
      "Epoch 23/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3286 - accuracy: 0.8869 - val_loss: 0.3278 - val_accuracy: 0.8873\n",
      "Epoch 24/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3263 - accuracy: 0.8855 - val_loss: 0.3311 - val_accuracy: 0.8872\n",
      "Epoch 25/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3265 - accuracy: 0.8872 - val_loss: 0.3211 - val_accuracy: 0.8871\n",
      "Epoch 26/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3188 - accuracy: 0.8886 - val_loss: 0.3370 - val_accuracy: 0.8819\n",
      "Epoch 27/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3207 - accuracy: 0.8870 - val_loss: 0.3322 - val_accuracy: 0.8836\n",
      "Epoch 28/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3161 - accuracy: 0.8877 - val_loss: 0.3204 - val_accuracy: 0.8863\n",
      "Epoch 29/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3103 - accuracy: 0.8904 - val_loss: 0.3272 - val_accuracy: 0.8850\n",
      "Epoch 30/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3133 - accuracy: 0.8915 - val_loss: 0.3247 - val_accuracy: 0.8888\n",
      "313/313 - 0s - loss: 0.3482 - accuracy: 0.8790 - 190ms/epoch - 606us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.8914583325386047\n",
      "Validation accuracy : 0.8888333439826965\n",
      "Loss : 0.34822481870651245\n",
      "Accuracy : 0.8790000081062317\n",
      "\n",
      "Train Accuracy: 0.8914583325386047, Validation Accuracy: 0.8888333439826965\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with Adam, LR: 0.0001, Batch Size: 32\n",
      "Epoch 1/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 1.2992 - accuracy: 0.5667 - val_loss: 0.6344 - val_accuracy: 0.7893\n",
      "Epoch 2/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.8168 - accuracy: 0.7309 - val_loss: 0.5088 - val_accuracy: 0.8238\n",
      "Epoch 3/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.6901 - accuracy: 0.7700 - val_loss: 0.4612 - val_accuracy: 0.8369\n",
      "Epoch 4/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.6276 - accuracy: 0.7913 - val_loss: 0.4294 - val_accuracy: 0.8495\n",
      "Epoch 5/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5867 - accuracy: 0.8063 - val_loss: 0.4104 - val_accuracy: 0.8529\n",
      "Epoch 6/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5561 - accuracy: 0.8148 - val_loss: 0.4010 - val_accuracy: 0.8548\n",
      "Epoch 7/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5300 - accuracy: 0.8242 - val_loss: 0.3887 - val_accuracy: 0.8602\n",
      "Epoch 8/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5122 - accuracy: 0.8271 - val_loss: 0.3790 - val_accuracy: 0.8642\n",
      "Epoch 9/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4974 - accuracy: 0.8329 - val_loss: 0.3746 - val_accuracy: 0.8651\n",
      "Epoch 10/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4839 - accuracy: 0.8361 - val_loss: 0.3665 - val_accuracy: 0.8685\n",
      "Epoch 11/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4729 - accuracy: 0.8412 - val_loss: 0.3644 - val_accuracy: 0.8697\n",
      "Epoch 12/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4625 - accuracy: 0.8430 - val_loss: 0.3607 - val_accuracy: 0.8725\n",
      "Epoch 13/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4585 - accuracy: 0.8455 - val_loss: 0.3582 - val_accuracy: 0.8725\n",
      "Epoch 14/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4452 - accuracy: 0.8493 - val_loss: 0.3506 - val_accuracy: 0.8748\n",
      "Epoch 15/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4352 - accuracy: 0.8516 - val_loss: 0.3486 - val_accuracy: 0.8748\n",
      "Epoch 16/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4276 - accuracy: 0.8551 - val_loss: 0.3484 - val_accuracy: 0.8747\n",
      "Epoch 17/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4232 - accuracy: 0.8570 - val_loss: 0.3455 - val_accuracy: 0.8776\n",
      "Epoch 18/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4149 - accuracy: 0.8609 - val_loss: 0.3424 - val_accuracy: 0.8799\n",
      "Epoch 19/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4186 - accuracy: 0.8586 - val_loss: 0.3432 - val_accuracy: 0.8788\n",
      "Epoch 20/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4083 - accuracy: 0.8610 - val_loss: 0.3430 - val_accuracy: 0.8782\n",
      "Epoch 21/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.4036 - accuracy: 0.8638 - val_loss: 0.3381 - val_accuracy: 0.8807\n",
      "Epoch 22/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3975 - accuracy: 0.8649 - val_loss: 0.3378 - val_accuracy: 0.8824\n",
      "Epoch 23/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3943 - accuracy: 0.8645 - val_loss: 0.3368 - val_accuracy: 0.8822\n",
      "Epoch 24/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3910 - accuracy: 0.8666 - val_loss: 0.3360 - val_accuracy: 0.8828\n",
      "Epoch 25/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3859 - accuracy: 0.8682 - val_loss: 0.3301 - val_accuracy: 0.8835\n",
      "Epoch 26/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3831 - accuracy: 0.8708 - val_loss: 0.3381 - val_accuracy: 0.8789\n",
      "Epoch 27/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3764 - accuracy: 0.8714 - val_loss: 0.3354 - val_accuracy: 0.8807\n",
      "Epoch 28/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3767 - accuracy: 0.8726 - val_loss: 0.3273 - val_accuracy: 0.8843\n",
      "Epoch 29/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3687 - accuracy: 0.8743 - val_loss: 0.3291 - val_accuracy: 0.8855\n",
      "Epoch 30/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.3669 - accuracy: 0.8744 - val_loss: 0.3251 - val_accuracy: 0.8867\n",
      "313/313 - 0s - loss: 0.3509 - accuracy: 0.8779 - 319ms/epoch - 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.8743541836738586\n",
      "Validation accuracy : 0.8867499828338623\n",
      "Loss : 0.3509325683116913\n",
      "Accuracy : 0.8779000043869019\n",
      "\n",
      "Train Accuracy: 0.8743541836738586, Validation Accuracy: 0.8867499828338623\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with Adam, LR: 0.0001, Batch Size: 64\n",
      "Epoch 1/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 1.4776 - accuracy: 0.5016 - val_loss: 0.8078 - val_accuracy: 0.7588\n",
      "Epoch 2/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.9328 - accuracy: 0.6956 - val_loss: 0.6016 - val_accuracy: 0.8087\n",
      "Epoch 3/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.7611 - accuracy: 0.7552 - val_loss: 0.5124 - val_accuracy: 0.8254\n",
      "Epoch 4/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.6683 - accuracy: 0.7825 - val_loss: 0.4635 - val_accuracy: 0.8357\n",
      "Epoch 5/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.6146 - accuracy: 0.7980 - val_loss: 0.4333 - val_accuracy: 0.8453\n",
      "Epoch 6/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.5781 - accuracy: 0.8089 - val_loss: 0.4161 - val_accuracy: 0.8508\n",
      "Epoch 7/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.5413 - accuracy: 0.8199 - val_loss: 0.4066 - val_accuracy: 0.8515\n",
      "Epoch 8/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.5163 - accuracy: 0.8282 - val_loss: 0.3932 - val_accuracy: 0.8577\n",
      "Epoch 9/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.5033 - accuracy: 0.8310 - val_loss: 0.3936 - val_accuracy: 0.8570\n",
      "Epoch 10/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.4876 - accuracy: 0.8376 - val_loss: 0.3844 - val_accuracy: 0.8599\n",
      "Epoch 11/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.4777 - accuracy: 0.8393 - val_loss: 0.3780 - val_accuracy: 0.8663\n",
      "Epoch 12/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.4636 - accuracy: 0.8448 - val_loss: 0.3697 - val_accuracy: 0.8673\n",
      "Epoch 13/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.4492 - accuracy: 0.8487 - val_loss: 0.3634 - val_accuracy: 0.8704\n",
      "Epoch 14/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.4406 - accuracy: 0.8532 - val_loss: 0.3524 - val_accuracy: 0.8758\n",
      "Epoch 15/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.4375 - accuracy: 0.8527 - val_loss: 0.3542 - val_accuracy: 0.8744\n",
      "Epoch 16/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.4280 - accuracy: 0.8560 - val_loss: 0.3556 - val_accuracy: 0.8709\n",
      "Epoch 17/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.4197 - accuracy: 0.8571 - val_loss: 0.3470 - val_accuracy: 0.8777\n",
      "Epoch 18/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.4153 - accuracy: 0.8590 - val_loss: 0.3503 - val_accuracy: 0.8743\n",
      "Epoch 19/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.4114 - accuracy: 0.8617 - val_loss: 0.3394 - val_accuracy: 0.8788\n",
      "Epoch 20/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3970 - accuracy: 0.8660 - val_loss: 0.3376 - val_accuracy: 0.8805\n",
      "Epoch 21/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3966 - accuracy: 0.8664 - val_loss: 0.3430 - val_accuracy: 0.8768\n",
      "Epoch 22/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.3943 - accuracy: 0.8668 - val_loss: 0.3408 - val_accuracy: 0.8769\n",
      "Epoch 23/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.3868 - accuracy: 0.8692 - val_loss: 0.3405 - val_accuracy: 0.8792\n",
      "Epoch 24/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3866 - accuracy: 0.8691 - val_loss: 0.3335 - val_accuracy: 0.8821\n",
      "Epoch 25/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.3774 - accuracy: 0.8718 - val_loss: 0.3482 - val_accuracy: 0.8762\n",
      "Epoch 26/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.3744 - accuracy: 0.8737 - val_loss: 0.3378 - val_accuracy: 0.8788\n",
      "Epoch 27/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.3700 - accuracy: 0.8759 - val_loss: 0.3287 - val_accuracy: 0.8843\n",
      "Epoch 28/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3675 - accuracy: 0.8752 - val_loss: 0.3378 - val_accuracy: 0.8817\n",
      "Epoch 29/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3622 - accuracy: 0.8777 - val_loss: 0.3283 - val_accuracy: 0.8833\n",
      "Epoch 30/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.3609 - accuracy: 0.8774 - val_loss: 0.3297 - val_accuracy: 0.8829\n",
      "313/313 - 0s - loss: 0.3541 - accuracy: 0.8770 - 208ms/epoch - 664us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.8773750066757202\n",
      "Validation accuracy : 0.8829166889190674\n",
      "Loss : 0.3540792763233185\n",
      "Accuracy : 0.8769999742507935\n",
      "\n",
      "Train Accuracy: 0.8773750066757202, Validation Accuracy: 0.8829166889190674\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with Adam, LR: 0.0001, Batch Size: 128\n",
      "Epoch 1/30\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 1.5416 - accuracy: 0.4877 - val_loss: 0.9450 - val_accuracy: 0.7172\n",
      "Epoch 2/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 1.0111 - accuracy: 0.6738 - val_loss: 0.6550 - val_accuracy: 0.7960\n",
      "Epoch 3/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.8446 - accuracy: 0.7278 - val_loss: 0.5623 - val_accuracy: 0.8183\n",
      "Epoch 4/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.7444 - accuracy: 0.7600 - val_loss: 0.5061 - val_accuracy: 0.8290\n",
      "Epoch 5/30\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.6806 - accuracy: 0.7808 - val_loss: 0.4728 - val_accuracy: 0.8384\n",
      "Epoch 6/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.6349 - accuracy: 0.7930 - val_loss: 0.4515 - val_accuracy: 0.8428\n",
      "Epoch 7/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.5975 - accuracy: 0.8023 - val_loss: 0.4364 - val_accuracy: 0.8462\n",
      "Epoch 8/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.5678 - accuracy: 0.8137 - val_loss: 0.4191 - val_accuracy: 0.8514\n",
      "Epoch 9/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.5447 - accuracy: 0.8208 - val_loss: 0.4039 - val_accuracy: 0.8546\n",
      "Epoch 10/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.5256 - accuracy: 0.8253 - val_loss: 0.4026 - val_accuracy: 0.8586\n",
      "Epoch 11/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.5095 - accuracy: 0.8301 - val_loss: 0.3882 - val_accuracy: 0.8628\n",
      "Epoch 12/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4958 - accuracy: 0.8366 - val_loss: 0.3848 - val_accuracy: 0.8617\n",
      "Epoch 13/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4831 - accuracy: 0.8392 - val_loss: 0.3748 - val_accuracy: 0.8651\n",
      "Epoch 14/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4698 - accuracy: 0.8434 - val_loss: 0.3674 - val_accuracy: 0.8675\n",
      "Epoch 15/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4616 - accuracy: 0.8461 - val_loss: 0.3689 - val_accuracy: 0.8662\n",
      "Epoch 16/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4558 - accuracy: 0.8479 - val_loss: 0.3661 - val_accuracy: 0.8669\n",
      "Epoch 17/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4416 - accuracy: 0.8526 - val_loss: 0.3568 - val_accuracy: 0.8710\n",
      "Epoch 18/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4328 - accuracy: 0.8553 - val_loss: 0.3552 - val_accuracy: 0.8732\n",
      "Epoch 19/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4235 - accuracy: 0.8571 - val_loss: 0.3535 - val_accuracy: 0.8717\n",
      "Epoch 20/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4246 - accuracy: 0.8578 - val_loss: 0.3526 - val_accuracy: 0.8741\n",
      "Epoch 21/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4158 - accuracy: 0.8599 - val_loss: 0.3498 - val_accuracy: 0.8742\n",
      "Epoch 22/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4108 - accuracy: 0.8607 - val_loss: 0.3472 - val_accuracy: 0.8760\n",
      "Epoch 23/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4050 - accuracy: 0.8628 - val_loss: 0.3455 - val_accuracy: 0.8781\n",
      "Epoch 24/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3988 - accuracy: 0.8653 - val_loss: 0.3408 - val_accuracy: 0.8782\n",
      "Epoch 25/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3922 - accuracy: 0.8681 - val_loss: 0.3409 - val_accuracy: 0.8801\n",
      "Epoch 26/30\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3870 - accuracy: 0.8687 - val_loss: 0.3506 - val_accuracy: 0.8766\n",
      "Epoch 27/30\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3850 - accuracy: 0.8677 - val_loss: 0.3465 - val_accuracy: 0.8776\n",
      "Epoch 28/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3822 - accuracy: 0.8711 - val_loss: 0.3381 - val_accuracy: 0.8795\n",
      "Epoch 29/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3742 - accuracy: 0.8729 - val_loss: 0.3336 - val_accuracy: 0.8819\n",
      "Epoch 30/30\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3704 - accuracy: 0.8741 - val_loss: 0.3349 - val_accuracy: 0.8806\n",
      "313/313 - 0s - loss: 0.3632 - accuracy: 0.8721 - 205ms/epoch - 655us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.8741458058357239\n",
      "Validation accuracy : 0.8805833458900452\n",
      "Loss : 0.363237589597702\n",
      "Accuracy : 0.8720999956130981\n",
      "\n",
      "Train Accuracy: 0.8741458058357239, Validation Accuracy: 0.8805833458900452\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with SGD, LR: 0.001, Batch Size: 32\n",
      "Epoch 1/30\n",
      "1500/1500 [==============================] - 3s 1ms/step - loss: 1.6831 - accuracy: 0.4513 - val_loss: 0.8716 - val_accuracy: 0.7425\n",
      "Epoch 2/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 1.1564 - accuracy: 0.6264 - val_loss: 0.7194 - val_accuracy: 0.7760\n",
      "Epoch 3/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 1.0052 - accuracy: 0.6735 - val_loss: 0.6442 - val_accuracy: 0.7904\n",
      "Epoch 4/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.9149 - accuracy: 0.7053 - val_loss: 0.6007 - val_accuracy: 0.7991\n",
      "Epoch 5/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.8533 - accuracy: 0.7192 - val_loss: 0.5609 - val_accuracy: 0.8096\n",
      "Epoch 6/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.8057 - accuracy: 0.7349 - val_loss: 0.5425 - val_accuracy: 0.8165\n",
      "Epoch 7/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.7763 - accuracy: 0.7434 - val_loss: 0.5225 - val_accuracy: 0.8218\n",
      "Epoch 8/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.7480 - accuracy: 0.7504 - val_loss: 0.5060 - val_accuracy: 0.8254\n",
      "Epoch 9/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.7254 - accuracy: 0.7601 - val_loss: 0.4939 - val_accuracy: 0.8298\n",
      "Epoch 10/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.7080 - accuracy: 0.7647 - val_loss: 0.4818 - val_accuracy: 0.8336\n",
      "Epoch 11/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.6956 - accuracy: 0.7689 - val_loss: 0.4739 - val_accuracy: 0.8342\n",
      "Epoch 12/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.6801 - accuracy: 0.7726 - val_loss: 0.4657 - val_accuracy: 0.8379\n",
      "Epoch 13/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.6607 - accuracy: 0.7787 - val_loss: 0.4597 - val_accuracy: 0.8382\n",
      "Epoch 14/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.6490 - accuracy: 0.7820 - val_loss: 0.4512 - val_accuracy: 0.8408\n",
      "Epoch 15/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.6416 - accuracy: 0.7844 - val_loss: 0.4458 - val_accuracy: 0.8413\n",
      "Epoch 16/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.6314 - accuracy: 0.7889 - val_loss: 0.4420 - val_accuracy: 0.8436\n",
      "Epoch 17/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.6229 - accuracy: 0.7943 - val_loss: 0.4370 - val_accuracy: 0.8447\n",
      "Epoch 18/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.6125 - accuracy: 0.7965 - val_loss: 0.4310 - val_accuracy: 0.8470\n",
      "Epoch 19/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.6038 - accuracy: 0.8004 - val_loss: 0.4286 - val_accuracy: 0.8487\n",
      "Epoch 20/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5969 - accuracy: 0.8011 - val_loss: 0.4249 - val_accuracy: 0.8486\n",
      "Epoch 21/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5897 - accuracy: 0.8037 - val_loss: 0.4212 - val_accuracy: 0.8497\n",
      "Epoch 22/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5828 - accuracy: 0.8047 - val_loss: 0.4155 - val_accuracy: 0.8507\n",
      "Epoch 23/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5819 - accuracy: 0.8050 - val_loss: 0.4135 - val_accuracy: 0.8510\n",
      "Epoch 24/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5716 - accuracy: 0.8079 - val_loss: 0.4092 - val_accuracy: 0.8531\n",
      "Epoch 25/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5670 - accuracy: 0.8099 - val_loss: 0.4063 - val_accuracy: 0.8533\n",
      "Epoch 26/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.5639 - accuracy: 0.8113 - val_loss: 0.4051 - val_accuracy: 0.8534\n",
      "Epoch 27/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5588 - accuracy: 0.8118 - val_loss: 0.4016 - val_accuracy: 0.8543\n",
      "Epoch 28/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5562 - accuracy: 0.8146 - val_loss: 0.4005 - val_accuracy: 0.8550\n",
      "Epoch 29/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5484 - accuracy: 0.8175 - val_loss: 0.3986 - val_accuracy: 0.8557\n",
      "Epoch 30/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5447 - accuracy: 0.8187 - val_loss: 0.3970 - val_accuracy: 0.8568\n",
      "313/313 - 0s - loss: 0.4230 - accuracy: 0.8468 - 301ms/epoch - 962us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.8187083601951599\n",
      "Validation accuracy : 0.8568333387374878\n",
      "Loss : 0.4230310916900635\n",
      "Accuracy : 0.8468000292778015\n",
      "\n",
      "Train Accuracy: 0.8187083601951599, Validation Accuracy: 0.8568333387374878\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with SGD, LR: 0.001, Batch Size: 64\n",
      "Epoch 1/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 1.9058 - accuracy: 0.3817 - val_loss: 1.0798 - val_accuracy: 0.6938\n",
      "Epoch 2/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 1.3487 - accuracy: 0.5644 - val_loss: 0.8836 - val_accuracy: 0.7432\n",
      "Epoch 3/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 1.1759 - accuracy: 0.6179 - val_loss: 0.7843 - val_accuracy: 0.7666\n",
      "Epoch 4/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 1.0711 - accuracy: 0.6550 - val_loss: 0.7224 - val_accuracy: 0.7802\n",
      "Epoch 5/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.9919 - accuracy: 0.6796 - val_loss: 0.6767 - val_accuracy: 0.7872\n",
      "Epoch 6/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.9354 - accuracy: 0.6979 - val_loss: 0.6445 - val_accuracy: 0.7929\n",
      "Epoch 7/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.8982 - accuracy: 0.7075 - val_loss: 0.6135 - val_accuracy: 0.7986\n",
      "Epoch 8/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.8622 - accuracy: 0.7212 - val_loss: 0.5936 - val_accuracy: 0.8043\n",
      "Epoch 9/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.8287 - accuracy: 0.7280 - val_loss: 0.5737 - val_accuracy: 0.8108\n",
      "Epoch 10/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.8105 - accuracy: 0.7325 - val_loss: 0.5587 - val_accuracy: 0.8133\n",
      "Epoch 11/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.7825 - accuracy: 0.7416 - val_loss: 0.5445 - val_accuracy: 0.8163\n",
      "Epoch 12/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.7690 - accuracy: 0.7450 - val_loss: 0.5333 - val_accuracy: 0.8200\n",
      "Epoch 13/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.7464 - accuracy: 0.7533 - val_loss: 0.5253 - val_accuracy: 0.8209\n",
      "Epoch 14/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.7316 - accuracy: 0.7557 - val_loss: 0.5157 - val_accuracy: 0.8219\n",
      "Epoch 15/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.7235 - accuracy: 0.7609 - val_loss: 0.5050 - val_accuracy: 0.8260\n",
      "Epoch 16/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.7102 - accuracy: 0.7676 - val_loss: 0.4980 - val_accuracy: 0.8280\n",
      "Epoch 17/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.6961 - accuracy: 0.7692 - val_loss: 0.4936 - val_accuracy: 0.8282\n",
      "Epoch 18/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.6869 - accuracy: 0.7725 - val_loss: 0.4850 - val_accuracy: 0.8308\n",
      "Epoch 19/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.6748 - accuracy: 0.7746 - val_loss: 0.4788 - val_accuracy: 0.8320\n",
      "Epoch 20/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.6575 - accuracy: 0.7813 - val_loss: 0.4741 - val_accuracy: 0.8340\n",
      "Epoch 21/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.6608 - accuracy: 0.7801 - val_loss: 0.4693 - val_accuracy: 0.8352\n",
      "Epoch 22/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.6509 - accuracy: 0.7828 - val_loss: 0.4645 - val_accuracy: 0.8377\n",
      "Epoch 23/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.6426 - accuracy: 0.7877 - val_loss: 0.4617 - val_accuracy: 0.8355\n",
      "Epoch 24/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.6383 - accuracy: 0.7884 - val_loss: 0.4560 - val_accuracy: 0.8391\n",
      "Epoch 25/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.6275 - accuracy: 0.7918 - val_loss: 0.4534 - val_accuracy: 0.8400\n",
      "Epoch 26/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.6251 - accuracy: 0.7947 - val_loss: 0.4476 - val_accuracy: 0.8415\n",
      "Epoch 27/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.6149 - accuracy: 0.7944 - val_loss: 0.4446 - val_accuracy: 0.8430\n",
      "Epoch 28/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.6086 - accuracy: 0.7985 - val_loss: 0.4429 - val_accuracy: 0.8438\n",
      "Epoch 29/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.6052 - accuracy: 0.7979 - val_loss: 0.4388 - val_accuracy: 0.8445\n",
      "Epoch 30/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.5977 - accuracy: 0.8024 - val_loss: 0.4361 - val_accuracy: 0.8445\n",
      "313/313 - 0s - loss: 0.4593 - accuracy: 0.8366 - 204ms/epoch - 653us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.8023750185966492\n",
      "Validation accuracy : 0.8445000052452087\n",
      "Loss : 0.4592823386192322\n",
      "Accuracy : 0.8366000056266785\n",
      "\n",
      "Train Accuracy: 0.8023750185966492, Validation Accuracy: 0.8445000052452087\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with SGD, LR: 0.001, Batch Size: 128\n",
      "Epoch 1/30\n",
      "375/375 [==============================] - 2s 3ms/step - loss: 2.0926 - accuracy: 0.3075 - val_loss: 1.3453 - val_accuracy: 0.5842\n",
      "Epoch 2/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 1.5457 - accuracy: 0.4780 - val_loss: 1.0427 - val_accuracy: 0.6839\n",
      "Epoch 3/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 1.3460 - accuracy: 0.5502 - val_loss: 0.9311 - val_accuracy: 0.7153\n",
      "Epoch 4/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 1.2353 - accuracy: 0.5951 - val_loss: 0.8602 - val_accuracy: 0.7372\n",
      "Epoch 5/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 1.1492 - accuracy: 0.6218 - val_loss: 0.8074 - val_accuracy: 0.7492\n",
      "Epoch 6/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 1.0868 - accuracy: 0.6423 - val_loss: 0.7659 - val_accuracy: 0.7604\n",
      "Epoch 7/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 1.0368 - accuracy: 0.6583 - val_loss: 0.7332 - val_accuracy: 0.7653\n",
      "Epoch 8/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.9918 - accuracy: 0.6737 - val_loss: 0.7069 - val_accuracy: 0.7738\n",
      "Epoch 9/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.9624 - accuracy: 0.6839 - val_loss: 0.6848 - val_accuracy: 0.7777\n",
      "Epoch 10/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.9348 - accuracy: 0.6914 - val_loss: 0.6633 - val_accuracy: 0.7838\n",
      "Epoch 11/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.9028 - accuracy: 0.7029 - val_loss: 0.6445 - val_accuracy: 0.7893\n",
      "Epoch 12/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.8864 - accuracy: 0.7072 - val_loss: 0.6318 - val_accuracy: 0.7922\n",
      "Epoch 13/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.8592 - accuracy: 0.7142 - val_loss: 0.6186 - val_accuracy: 0.7938\n",
      "Epoch 14/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.8476 - accuracy: 0.7206 - val_loss: 0.6047 - val_accuracy: 0.7974\n",
      "Epoch 15/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.8267 - accuracy: 0.7264 - val_loss: 0.5956 - val_accuracy: 0.7989\n",
      "Epoch 16/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.8154 - accuracy: 0.7280 - val_loss: 0.5835 - val_accuracy: 0.8025\n",
      "Epoch 17/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.7996 - accuracy: 0.7338 - val_loss: 0.5746 - val_accuracy: 0.8050\n",
      "Epoch 18/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.7882 - accuracy: 0.7398 - val_loss: 0.5681 - val_accuracy: 0.8083\n",
      "Epoch 19/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.7779 - accuracy: 0.7408 - val_loss: 0.5603 - val_accuracy: 0.8098\n",
      "Epoch 20/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.7683 - accuracy: 0.7438 - val_loss: 0.5524 - val_accuracy: 0.8114\n",
      "Epoch 21/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.7598 - accuracy: 0.7450 - val_loss: 0.5466 - val_accuracy: 0.8142\n",
      "Epoch 22/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.7505 - accuracy: 0.7499 - val_loss: 0.5398 - val_accuracy: 0.8154\n",
      "Epoch 23/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.7356 - accuracy: 0.7534 - val_loss: 0.5346 - val_accuracy: 0.8168\n",
      "Epoch 24/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.7281 - accuracy: 0.7567 - val_loss: 0.5287 - val_accuracy: 0.8186\n",
      "Epoch 25/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.7205 - accuracy: 0.7597 - val_loss: 0.5241 - val_accuracy: 0.8206\n",
      "Epoch 26/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.7130 - accuracy: 0.7630 - val_loss: 0.5191 - val_accuracy: 0.8213\n",
      "Epoch 27/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.7069 - accuracy: 0.7627 - val_loss: 0.5147 - val_accuracy: 0.8224\n",
      "Epoch 28/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.7003 - accuracy: 0.7652 - val_loss: 0.5090 - val_accuracy: 0.8249\n",
      "Epoch 29/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.7686 - val_loss: 0.5051 - val_accuracy: 0.8255\n",
      "Epoch 30/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.6934 - accuracy: 0.7678 - val_loss: 0.5029 - val_accuracy: 0.8257\n",
      "313/313 - 0s - loss: 0.5215 - accuracy: 0.8218 - 215ms/epoch - 688us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.7677916884422302\n",
      "Validation accuracy : 0.8257499933242798\n",
      "Loss : 0.5215120315551758\n",
      "Accuracy : 0.8217999935150146\n",
      "\n",
      "Train Accuracy: 0.7677916884422302, Validation Accuracy: 0.8257499933242798\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with SGD, LR: 0.0005, Batch Size: 32\n",
      "Epoch 1/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 1.9530 - accuracy: 0.3736 - val_loss: 1.0617 - val_accuracy: 0.6826\n",
      "Epoch 2/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 1.3672 - accuracy: 0.5500 - val_loss: 0.8912 - val_accuracy: 0.7262\n",
      "Epoch 3/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 1.2038 - accuracy: 0.6077 - val_loss: 0.7970 - val_accuracy: 0.7557\n",
      "Epoch 4/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 1.0985 - accuracy: 0.6406 - val_loss: 0.7357 - val_accuracy: 0.7681\n",
      "Epoch 5/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 1.0351 - accuracy: 0.6609 - val_loss: 0.6925 - val_accuracy: 0.7779\n",
      "Epoch 6/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.9813 - accuracy: 0.6787 - val_loss: 0.6596 - val_accuracy: 0.7849\n",
      "Epoch 7/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.9390 - accuracy: 0.6900 - val_loss: 0.6308 - val_accuracy: 0.7916\n",
      "Epoch 8/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.8979 - accuracy: 0.7015 - val_loss: 0.6062 - val_accuracy: 0.7998\n",
      "Epoch 9/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.8714 - accuracy: 0.7106 - val_loss: 0.5870 - val_accuracy: 0.8028\n",
      "Epoch 10/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.8472 - accuracy: 0.7157 - val_loss: 0.5719 - val_accuracy: 0.8048\n",
      "Epoch 11/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.8263 - accuracy: 0.7225 - val_loss: 0.5616 - val_accuracy: 0.8058\n",
      "Epoch 12/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.8067 - accuracy: 0.7300 - val_loss: 0.5455 - val_accuracy: 0.8101\n",
      "Epoch 13/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.7917 - accuracy: 0.7364 - val_loss: 0.5348 - val_accuracy: 0.8125\n",
      "Epoch 14/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.7749 - accuracy: 0.7401 - val_loss: 0.5260 - val_accuracy: 0.8159\n",
      "Epoch 15/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.7611 - accuracy: 0.7459 - val_loss: 0.5198 - val_accuracy: 0.8177\n",
      "Epoch 16/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.7481 - accuracy: 0.7479 - val_loss: 0.5093 - val_accuracy: 0.8212\n",
      "Epoch 17/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.7345 - accuracy: 0.7540 - val_loss: 0.5044 - val_accuracy: 0.8233\n",
      "Epoch 18/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.7216 - accuracy: 0.7575 - val_loss: 0.4951 - val_accuracy: 0.8256\n",
      "Epoch 19/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.7134 - accuracy: 0.7623 - val_loss: 0.4896 - val_accuracy: 0.8257\n",
      "Epoch 20/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.7043 - accuracy: 0.7656 - val_loss: 0.4839 - val_accuracy: 0.8278\n",
      "Epoch 21/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.6949 - accuracy: 0.7656 - val_loss: 0.4802 - val_accuracy: 0.8293\n",
      "Epoch 22/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.6913 - accuracy: 0.7684 - val_loss: 0.4730 - val_accuracy: 0.8320\n",
      "Epoch 23/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.6839 - accuracy: 0.7702 - val_loss: 0.4699 - val_accuracy: 0.8318\n",
      "Epoch 24/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.6784 - accuracy: 0.7729 - val_loss: 0.4635 - val_accuracy: 0.8339\n",
      "Epoch 25/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.6657 - accuracy: 0.7776 - val_loss: 0.4604 - val_accuracy: 0.8350\n",
      "Epoch 26/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.6557 - accuracy: 0.7788 - val_loss: 0.4574 - val_accuracy: 0.8357\n",
      "Epoch 27/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.6538 - accuracy: 0.7808 - val_loss: 0.4525 - val_accuracy: 0.8378\n",
      "Epoch 28/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.6442 - accuracy: 0.7865 - val_loss: 0.4497 - val_accuracy: 0.8394\n",
      "Epoch 29/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.6470 - accuracy: 0.7829 - val_loss: 0.4473 - val_accuracy: 0.8393\n",
      "Epoch 30/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.6368 - accuracy: 0.7866 - val_loss: 0.4436 - val_accuracy: 0.8400\n",
      "313/313 - 0s - loss: 0.4664 - accuracy: 0.8352 - 324ms/epoch - 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.7866041660308838\n",
      "Validation accuracy : 0.8399999737739563\n",
      "Loss : 0.4663983881473541\n",
      "Accuracy : 0.8352000117301941\n",
      "\n",
      "Train Accuracy: 0.7866041660308838, Validation Accuracy: 0.8399999737739563\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with SGD, LR: 0.0005, Batch Size: 64\n",
      "Epoch 1/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 2.2307 - accuracy: 0.2803 - val_loss: 1.3330 - val_accuracy: 0.6033\n",
      "Epoch 2/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 1.6143 - accuracy: 0.4588 - val_loss: 1.0963 - val_accuracy: 0.6729\n",
      "Epoch 3/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 1.4013 - accuracy: 0.5337 - val_loss: 0.9784 - val_accuracy: 0.7045\n",
      "Epoch 4/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 1.2776 - accuracy: 0.5762 - val_loss: 0.9071 - val_accuracy: 0.7188\n",
      "Epoch 5/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 1.1983 - accuracy: 0.6029 - val_loss: 0.8522 - val_accuracy: 0.7347\n",
      "Epoch 6/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 1.1288 - accuracy: 0.6238 - val_loss: 0.8099 - val_accuracy: 0.7454\n",
      "Epoch 7/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 1.0866 - accuracy: 0.6386 - val_loss: 0.7781 - val_accuracy: 0.7538\n",
      "Epoch 8/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 1.0457 - accuracy: 0.6513 - val_loss: 0.7446 - val_accuracy: 0.7633\n",
      "Epoch 9/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 1.0098 - accuracy: 0.6613 - val_loss: 0.7213 - val_accuracy: 0.7697\n",
      "Epoch 10/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.9770 - accuracy: 0.6754 - val_loss: 0.7003 - val_accuracy: 0.7765\n",
      "Epoch 11/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.9522 - accuracy: 0.6829 - val_loss: 0.6815 - val_accuracy: 0.7824\n",
      "Epoch 12/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.9311 - accuracy: 0.6880 - val_loss: 0.6651 - val_accuracy: 0.7870\n",
      "Epoch 13/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.9077 - accuracy: 0.6975 - val_loss: 0.6513 - val_accuracy: 0.7912\n",
      "Epoch 14/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.8896 - accuracy: 0.7026 - val_loss: 0.6360 - val_accuracy: 0.7944\n",
      "Epoch 15/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.8744 - accuracy: 0.7080 - val_loss: 0.6250 - val_accuracy: 0.7958\n",
      "Epoch 16/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.8558 - accuracy: 0.7105 - val_loss: 0.6148 - val_accuracy: 0.7995\n",
      "Epoch 17/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.8447 - accuracy: 0.7157 - val_loss: 0.6034 - val_accuracy: 0.8018\n",
      "Epoch 18/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.8287 - accuracy: 0.7215 - val_loss: 0.5936 - val_accuracy: 0.8028\n",
      "Epoch 19/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.8211 - accuracy: 0.7240 - val_loss: 0.5845 - val_accuracy: 0.8070\n",
      "Epoch 20/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.8100 - accuracy: 0.7279 - val_loss: 0.5773 - val_accuracy: 0.8077\n",
      "Epoch 21/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.7916 - accuracy: 0.7342 - val_loss: 0.5694 - val_accuracy: 0.8099\n",
      "Epoch 22/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.7919 - accuracy: 0.7352 - val_loss: 0.5626 - val_accuracy: 0.8117\n",
      "Epoch 23/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.7743 - accuracy: 0.7406 - val_loss: 0.5557 - val_accuracy: 0.8137\n",
      "Epoch 24/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.7637 - accuracy: 0.7434 - val_loss: 0.5490 - val_accuracy: 0.8152\n",
      "Epoch 25/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.7583 - accuracy: 0.7463 - val_loss: 0.5443 - val_accuracy: 0.8159\n",
      "Epoch 26/30\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.7482 - accuracy: 0.7486 - val_loss: 0.5372 - val_accuracy: 0.8182\n",
      "Epoch 27/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.7468 - accuracy: 0.7487 - val_loss: 0.5326 - val_accuracy: 0.8188\n",
      "Epoch 28/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.7354 - accuracy: 0.7509 - val_loss: 0.5277 - val_accuracy: 0.8195\n",
      "Epoch 29/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.7310 - accuracy: 0.7553 - val_loss: 0.5231 - val_accuracy: 0.8202\n",
      "Epoch 30/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.7262 - accuracy: 0.7565 - val_loss: 0.5185 - val_accuracy: 0.8225\n",
      "313/313 - 0s - loss: 0.5371 - accuracy: 0.8185 - 185ms/epoch - 592us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.7564791440963745\n",
      "Validation accuracy : 0.8224999904632568\n",
      "Loss : 0.5371198058128357\n",
      "Accuracy : 0.8184999823570251\n",
      "\n",
      "Train Accuracy: 0.7564791440963745, Validation Accuracy: 0.8224999904632568\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with SGD, LR: 0.0005, Batch Size: 128\n",
      "Epoch 1/30\n",
      "375/375 [==============================] - 2s 3ms/step - loss: 2.3535 - accuracy: 0.2472 - val_loss: 1.6186 - val_accuracy: 0.4690\n",
      "Epoch 2/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 1.8101 - accuracy: 0.3845 - val_loss: 1.3174 - val_accuracy: 0.5650\n",
      "Epoch 3/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 1.6109 - accuracy: 0.4452 - val_loss: 1.1999 - val_accuracy: 0.6138\n",
      "Epoch 4/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 1.4834 - accuracy: 0.4889 - val_loss: 1.1174 - val_accuracy: 0.6472\n",
      "Epoch 5/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 1.4014 - accuracy: 0.5179 - val_loss: 1.0563 - val_accuracy: 0.6707\n",
      "Epoch 6/30\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 1.3390 - accuracy: 0.5413 - val_loss: 1.0057 - val_accuracy: 0.6918\n",
      "Epoch 7/30\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 1.2829 - accuracy: 0.5615 - val_loss: 0.9619 - val_accuracy: 0.7109\n",
      "Epoch 8/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 1.2347 - accuracy: 0.5778 - val_loss: 0.9271 - val_accuracy: 0.7223\n",
      "Epoch 9/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 1.1992 - accuracy: 0.5929 - val_loss: 0.8954 - val_accuracy: 0.7312\n",
      "Epoch 10/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 1.1653 - accuracy: 0.6049 - val_loss: 0.8680 - val_accuracy: 0.7408\n",
      "Epoch 11/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 1.1335 - accuracy: 0.6180 - val_loss: 0.8439 - val_accuracy: 0.7462\n",
      "Epoch 12/30\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 1.1053 - accuracy: 0.6314 - val_loss: 0.8173 - val_accuracy: 0.7521\n",
      "Epoch 13/30\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 1.0881 - accuracy: 0.6329 - val_loss: 0.7979 - val_accuracy: 0.7579\n",
      "Epoch 14/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 1.0599 - accuracy: 0.6453 - val_loss: 0.7799 - val_accuracy: 0.7612\n",
      "Epoch 15/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 1.0377 - accuracy: 0.6531 - val_loss: 0.7642 - val_accuracy: 0.7645\n",
      "Epoch 16/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 1.0230 - accuracy: 0.6587 - val_loss: 0.7506 - val_accuracy: 0.7657\n",
      "Epoch 17/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 1.0030 - accuracy: 0.6639 - val_loss: 0.7347 - val_accuracy: 0.7699\n",
      "Epoch 18/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.9853 - accuracy: 0.6680 - val_loss: 0.7210 - val_accuracy: 0.7724\n",
      "Epoch 19/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.9730 - accuracy: 0.6746 - val_loss: 0.7112 - val_accuracy: 0.7751\n",
      "Epoch 20/30\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.9556 - accuracy: 0.6783 - val_loss: 0.6988 - val_accuracy: 0.7776\n",
      "Epoch 21/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.9374 - accuracy: 0.6859 - val_loss: 0.6891 - val_accuracy: 0.7799\n",
      "Epoch 22/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.9317 - accuracy: 0.6876 - val_loss: 0.6794 - val_accuracy: 0.7825\n",
      "Epoch 23/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.9207 - accuracy: 0.6902 - val_loss: 0.6703 - val_accuracy: 0.7834\n",
      "Epoch 24/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.9082 - accuracy: 0.6940 - val_loss: 0.6613 - val_accuracy: 0.7857\n",
      "Epoch 25/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.8963 - accuracy: 0.6977 - val_loss: 0.6540 - val_accuracy: 0.7883\n",
      "Epoch 26/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.8919 - accuracy: 0.7014 - val_loss: 0.6455 - val_accuracy: 0.7888\n",
      "Epoch 27/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.8795 - accuracy: 0.7042 - val_loss: 0.6375 - val_accuracy: 0.7918\n",
      "Epoch 28/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.8704 - accuracy: 0.7086 - val_loss: 0.6316 - val_accuracy: 0.7922\n",
      "Epoch 29/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.8631 - accuracy: 0.7119 - val_loss: 0.6249 - val_accuracy: 0.7933\n",
      "Epoch 30/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.8506 - accuracy: 0.7131 - val_loss: 0.6177 - val_accuracy: 0.7958\n",
      "313/313 - 0s - loss: 0.6370 - accuracy: 0.7859 - 194ms/epoch - 621us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.7130625247955322\n",
      "Validation accuracy : 0.7957500219345093\n",
      "Loss : 0.6369808316230774\n",
      "Accuracy : 0.7858999967575073\n",
      "\n",
      "Train Accuracy: 0.7130625247955322, Validation Accuracy: 0.7957500219345093\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with SGD, LR: 0.0001, Batch Size: 32\n",
      "Epoch 1/30\n",
      "1500/1500 [==============================] - 3s 1ms/step - loss: 2.6719 - accuracy: 0.1689 - val_loss: 1.7638 - val_accuracy: 0.4017\n",
      "Epoch 2/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 2.0656 - accuracy: 0.3133 - val_loss: 1.4229 - val_accuracy: 0.5599\n",
      "Epoch 3/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 1.7972 - accuracy: 0.3988 - val_loss: 1.2518 - val_accuracy: 0.6195\n",
      "Epoch 4/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 1.6387 - accuracy: 0.4523 - val_loss: 1.1534 - val_accuracy: 0.6503\n",
      "Epoch 5/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 1.5416 - accuracy: 0.4892 - val_loss: 1.0805 - val_accuracy: 0.6753\n",
      "Epoch 6/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 1.4583 - accuracy: 0.5174 - val_loss: 1.0312 - val_accuracy: 0.6893\n",
      "Epoch 7/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 1.3953 - accuracy: 0.5423 - val_loss: 0.9853 - val_accuracy: 0.7054\n",
      "Epoch 8/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 1.3404 - accuracy: 0.5606 - val_loss: 0.9498 - val_accuracy: 0.7139\n",
      "Epoch 9/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 1.2939 - accuracy: 0.5761 - val_loss: 0.9180 - val_accuracy: 0.7240\n",
      "Epoch 10/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 1.2615 - accuracy: 0.5892 - val_loss: 0.8932 - val_accuracy: 0.7299\n",
      "Epoch 11/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 1.2368 - accuracy: 0.5966 - val_loss: 0.8691 - val_accuracy: 0.7378\n",
      "Epoch 12/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 1.2077 - accuracy: 0.6074 - val_loss: 0.8466 - val_accuracy: 0.7427\n",
      "Epoch 13/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 1.1821 - accuracy: 0.6153 - val_loss: 0.8312 - val_accuracy: 0.7458\n",
      "Epoch 14/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 1.1565 - accuracy: 0.6222 - val_loss: 0.8148 - val_accuracy: 0.7517\n",
      "Epoch 15/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 1.1324 - accuracy: 0.6317 - val_loss: 0.7979 - val_accuracy: 0.7547\n",
      "Epoch 16/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 1.1194 - accuracy: 0.6361 - val_loss: 0.7871 - val_accuracy: 0.7575\n",
      "Epoch 17/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 1.0995 - accuracy: 0.6429 - val_loss: 0.7730 - val_accuracy: 0.7598\n",
      "Epoch 18/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 1.0859 - accuracy: 0.6460 - val_loss: 0.7626 - val_accuracy: 0.7637\n",
      "Epoch 19/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 1.0706 - accuracy: 0.6524 - val_loss: 0.7513 - val_accuracy: 0.7667\n",
      "Epoch 20/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 1.0536 - accuracy: 0.6599 - val_loss: 0.7389 - val_accuracy: 0.7688\n",
      "Epoch 21/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 1.0375 - accuracy: 0.6609 - val_loss: 0.7318 - val_accuracy: 0.7680\n",
      "Epoch 22/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 1.0233 - accuracy: 0.6658 - val_loss: 0.7211 - val_accuracy: 0.7715\n",
      "Epoch 23/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 1.0093 - accuracy: 0.6714 - val_loss: 0.7138 - val_accuracy: 0.7732\n",
      "Epoch 24/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.9985 - accuracy: 0.6768 - val_loss: 0.7060 - val_accuracy: 0.7733\n",
      "Epoch 25/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 1.0009 - accuracy: 0.6757 - val_loss: 0.7010 - val_accuracy: 0.7745\n",
      "Epoch 26/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.9897 - accuracy: 0.6760 - val_loss: 0.6908 - val_accuracy: 0.7763\n",
      "Epoch 27/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.9721 - accuracy: 0.6810 - val_loss: 0.6831 - val_accuracy: 0.7787\n",
      "Epoch 28/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.9598 - accuracy: 0.6839 - val_loss: 0.6781 - val_accuracy: 0.7810\n",
      "Epoch 29/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.9601 - accuracy: 0.6857 - val_loss: 0.6727 - val_accuracy: 0.7822\n",
      "Epoch 30/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.9489 - accuracy: 0.6885 - val_loss: 0.6620 - val_accuracy: 0.7841\n",
      "313/313 - 0s - loss: 0.6854 - accuracy: 0.7722 - 292ms/epoch - 934us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.6885208487510681\n",
      "Validation accuracy : 0.7840833067893982\n",
      "Loss : 0.6853601336479187\n",
      "Accuracy : 0.7721999883651733\n",
      "\n",
      "Train Accuracy: 0.6885208487510681, Validation Accuracy: 0.7840833067893982\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with SGD, LR: 0.0001, Batch Size: 64\n",
      "Epoch 1/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 2.9405 - accuracy: 0.1384 - val_loss: 2.0628 - val_accuracy: 0.2691\n",
      "Epoch 2/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 2.3856 - accuracy: 0.2362 - val_loss: 1.7189 - val_accuracy: 0.4183\n",
      "Epoch 3/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 2.1189 - accuracy: 0.3028 - val_loss: 1.5313 - val_accuracy: 0.4922\n",
      "Epoch 4/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 1.9464 - accuracy: 0.3536 - val_loss: 1.4110 - val_accuracy: 0.5428\n",
      "Epoch 5/30\n",
      "750/750 [==============================] - 288s 384ms/step - loss: 1.8271 - accuracy: 0.3897 - val_loss: 1.3202 - val_accuracy: 0.5807\n",
      "Epoch 6/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 1.7386 - accuracy: 0.4152 - val_loss: 1.2575 - val_accuracy: 0.6042\n",
      "Epoch 7/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 1.6620 - accuracy: 0.4415 - val_loss: 1.2011 - val_accuracy: 0.6246\n",
      "Epoch 8/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 1.5992 - accuracy: 0.4638 - val_loss: 1.1646 - val_accuracy: 0.6395\n",
      "Epoch 9/30\n",
      "750/750 [==============================] - 486s 649ms/step - loss: 1.5485 - accuracy: 0.4818 - val_loss: 1.1269 - val_accuracy: 0.6539\n",
      "Epoch 10/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 1.4994 - accuracy: 0.4965 - val_loss: 1.0956 - val_accuracy: 0.6658\n",
      "Epoch 11/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 1.4616 - accuracy: 0.5134 - val_loss: 1.0621 - val_accuracy: 0.6793\n",
      "Epoch 12/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 1.4269 - accuracy: 0.5246 - val_loss: 1.0397 - val_accuracy: 0.6858\n",
      "Epoch 13/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 1.3929 - accuracy: 0.5380 - val_loss: 1.0132 - val_accuracy: 0.6945\n",
      "Epoch 14/30\n",
      "750/750 [==============================] - 2192s 3s/step - loss: 1.3660 - accuracy: 0.5491 - val_loss: 0.9928 - val_accuracy: 0.7024\n",
      "Epoch 15/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 1.3472 - accuracy: 0.5559 - val_loss: 0.9704 - val_accuracy: 0.7066\n",
      "Epoch 16/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 1.3163 - accuracy: 0.5649 - val_loss: 0.9581 - val_accuracy: 0.7139\n",
      "Epoch 17/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 1.2931 - accuracy: 0.5734 - val_loss: 0.9364 - val_accuracy: 0.7215\n",
      "Epoch 18/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 1.2739 - accuracy: 0.5814 - val_loss: 0.9196 - val_accuracy: 0.7260\n",
      "Epoch 19/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 1.2570 - accuracy: 0.5885 - val_loss: 0.9063 - val_accuracy: 0.7284\n",
      "Epoch 20/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 1.2375 - accuracy: 0.5965 - val_loss: 0.8948 - val_accuracy: 0.7325\n",
      "Epoch 21/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 1.2241 - accuracy: 0.5981 - val_loss: 0.8821 - val_accuracy: 0.7370\n",
      "Epoch 22/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 1.2078 - accuracy: 0.6047 - val_loss: 0.8682 - val_accuracy: 0.7397\n",
      "Epoch 23/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 1.1903 - accuracy: 0.6119 - val_loss: 0.8567 - val_accuracy: 0.7427\n",
      "Epoch 24/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 1.1766 - accuracy: 0.6188 - val_loss: 0.8484 - val_accuracy: 0.7445\n",
      "Epoch 25/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 1.1694 - accuracy: 0.6176 - val_loss: 0.8379 - val_accuracy: 0.7468\n",
      "Epoch 26/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 1.1508 - accuracy: 0.6244 - val_loss: 0.8258 - val_accuracy: 0.7498\n",
      "Epoch 27/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 1.1415 - accuracy: 0.6294 - val_loss: 0.8178 - val_accuracy: 0.7518\n",
      "Epoch 28/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 1.1249 - accuracy: 0.6338 - val_loss: 0.8083 - val_accuracy: 0.7533\n",
      "Epoch 29/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 1.1185 - accuracy: 0.6365 - val_loss: 0.8025 - val_accuracy: 0.7550\n",
      "Epoch 30/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 1.1079 - accuracy: 0.6382 - val_loss: 0.7916 - val_accuracy: 0.7572\n",
      "313/313 - 0s - loss: 0.8254 - accuracy: 0.7470 - 198ms/epoch - 633us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.6381666660308838\n",
      "Validation accuracy : 0.7571666836738586\n",
      "Loss : 0.8254055380821228\n",
      "Accuracy : 0.746999979019165\n",
      "\n",
      "Train Accuracy: 0.6381666660308838, Validation Accuracy: 0.7571666836738586\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with SGD, LR: 0.0001, Batch Size: 128\n",
      "Epoch 1/30\n",
      "375/375 [==============================] - 2s 3ms/step - loss: 2.8420 - accuracy: 0.1348 - val_loss: 2.1344 - val_accuracy: 0.2360\n",
      "Epoch 2/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 2.4995 - accuracy: 0.1910 - val_loss: 1.8520 - val_accuracy: 0.3653\n",
      "Epoch 3/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 2.2876 - accuracy: 0.2386 - val_loss: 1.7003 - val_accuracy: 0.4316\n",
      "Epoch 4/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 2.1375 - accuracy: 0.2793 - val_loss: 1.5984 - val_accuracy: 0.4762\n",
      "Epoch 5/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 2.0379 - accuracy: 0.3067 - val_loss: 1.5203 - val_accuracy: 0.5067\n",
      "Epoch 6/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 1.9469 - accuracy: 0.3319 - val_loss: 1.4563 - val_accuracy: 0.5293\n",
      "Epoch 7/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 1.8814 - accuracy: 0.3553 - val_loss: 1.4059 - val_accuracy: 0.5506\n",
      "Epoch 8/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 1.8224 - accuracy: 0.3726 - val_loss: 1.3626 - val_accuracy: 0.5691\n",
      "Epoch 9/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 1.7705 - accuracy: 0.3892 - val_loss: 1.3251 - val_accuracy: 0.5838\n",
      "Epoch 10/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 1.7336 - accuracy: 0.4062 - val_loss: 1.2924 - val_accuracy: 0.5968\n",
      "Epoch 11/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 1.6963 - accuracy: 0.4162 - val_loss: 1.2626 - val_accuracy: 0.6062\n",
      "Epoch 12/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 1.6591 - accuracy: 0.4289 - val_loss: 1.2377 - val_accuracy: 0.6131\n",
      "Epoch 13/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 1.6289 - accuracy: 0.4404 - val_loss: 1.2182 - val_accuracy: 0.6201\n",
      "Epoch 14/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 1.5957 - accuracy: 0.4549 - val_loss: 1.1941 - val_accuracy: 0.6269\n",
      "Epoch 15/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 1.5763 - accuracy: 0.4618 - val_loss: 1.1761 - val_accuracy: 0.6332\n",
      "Epoch 16/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 1.5496 - accuracy: 0.4681 - val_loss: 1.1587 - val_accuracy: 0.6387\n",
      "Epoch 17/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 1.5290 - accuracy: 0.4759 - val_loss: 1.1426 - val_accuracy: 0.6461\n",
      "Epoch 18/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 1.4994 - accuracy: 0.4895 - val_loss: 1.1265 - val_accuracy: 0.6518\n",
      "Epoch 19/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 1.4856 - accuracy: 0.4941 - val_loss: 1.1115 - val_accuracy: 0.6569\n",
      "Epoch 20/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 1.4717 - accuracy: 0.4992 - val_loss: 1.1003 - val_accuracy: 0.6603\n",
      "Epoch 21/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 1.4493 - accuracy: 0.5074 - val_loss: 1.0874 - val_accuracy: 0.6643\n",
      "Epoch 22/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 1.4368 - accuracy: 0.5129 - val_loss: 1.0731 - val_accuracy: 0.6687\n",
      "Epoch 23/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 1.4221 - accuracy: 0.5181 - val_loss: 1.0591 - val_accuracy: 0.6752\n",
      "Epoch 24/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 1.4020 - accuracy: 0.5256 - val_loss: 1.0507 - val_accuracy: 0.6794\n",
      "Epoch 25/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 1.3953 - accuracy: 0.5278 - val_loss: 1.0408 - val_accuracy: 0.6814\n",
      "Epoch 26/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 1.3776 - accuracy: 0.5319 - val_loss: 1.0324 - val_accuracy: 0.6839\n",
      "Epoch 27/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 1.3619 - accuracy: 0.5394 - val_loss: 1.0211 - val_accuracy: 0.6866\n",
      "Epoch 28/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 1.3504 - accuracy: 0.5475 - val_loss: 1.0129 - val_accuracy: 0.6902\n",
      "Epoch 29/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 1.3429 - accuracy: 0.5457 - val_loss: 1.0011 - val_accuracy: 0.6923\n",
      "Epoch 30/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 1.3283 - accuracy: 0.5524 - val_loss: 0.9931 - val_accuracy: 0.6938\n",
      "313/313 - 0s - loss: 1.0204 - accuracy: 0.6817 - 197ms/epoch - 630us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adadelta` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adadelta`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adadelta`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.5524374842643738\n",
      "Validation accuracy : 0.6937500238418579\n",
      "Loss : 1.0203533172607422\n",
      "Accuracy : 0.6816999912261963\n",
      "\n",
      "Train Accuracy: 0.5524374842643738, Validation Accuracy: 0.6937500238418579\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with Adadelta, LR: 0.001, Batch Size: 32\n",
      "Epoch 1/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 2.9978 - accuracy: 0.1209 - val_loss: 2.3726 - val_accuracy: 0.1950\n",
      "Epoch 2/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 2.7899 - accuracy: 0.1448 - val_loss: 2.1851 - val_accuracy: 0.2407\n",
      "Epoch 3/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 2.6173 - accuracy: 0.1741 - val_loss: 2.0252 - val_accuracy: 0.2804\n",
      "Epoch 4/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 2.4677 - accuracy: 0.2007 - val_loss: 1.8769 - val_accuracy: 0.3341\n",
      "Epoch 5/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 2.3366 - accuracy: 0.2300 - val_loss: 1.7611 - val_accuracy: 0.3810\n",
      "Epoch 6/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 2.2362 - accuracy: 0.2551 - val_loss: 1.6635 - val_accuracy: 0.4315\n",
      "Epoch 7/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 2.1345 - accuracy: 0.2827 - val_loss: 1.5741 - val_accuracy: 0.4776\n",
      "Epoch 8/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 2.0540 - accuracy: 0.3038 - val_loss: 1.5086 - val_accuracy: 0.5104\n",
      "Epoch 9/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 1.9748 - accuracy: 0.3295 - val_loss: 1.4430 - val_accuracy: 0.5402\n",
      "Epoch 10/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 1.9173 - accuracy: 0.3486 - val_loss: 1.3857 - val_accuracy: 0.5652\n",
      "Epoch 11/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 1.8634 - accuracy: 0.3641 - val_loss: 1.3407 - val_accuracy: 0.5865\n",
      "Epoch 12/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 1.8051 - accuracy: 0.3832 - val_loss: 1.2950 - val_accuracy: 0.6029\n",
      "Epoch 13/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 1.7631 - accuracy: 0.3960 - val_loss: 1.2573 - val_accuracy: 0.6162\n",
      "Epoch 14/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 1.7259 - accuracy: 0.4076 - val_loss: 1.2226 - val_accuracy: 0.6295\n",
      "Epoch 15/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 1.6773 - accuracy: 0.4229 - val_loss: 1.1978 - val_accuracy: 0.6373\n",
      "Epoch 16/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 1.6487 - accuracy: 0.4365 - val_loss: 1.1694 - val_accuracy: 0.6490\n",
      "Epoch 17/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 1.6107 - accuracy: 0.4456 - val_loss: 1.1399 - val_accuracy: 0.6584\n",
      "Epoch 18/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 1.5792 - accuracy: 0.4594 - val_loss: 1.1271 - val_accuracy: 0.6624\n",
      "Epoch 19/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 1.5533 - accuracy: 0.4699 - val_loss: 1.1025 - val_accuracy: 0.6738\n",
      "Epoch 20/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 1.5310 - accuracy: 0.4781 - val_loss: 1.0900 - val_accuracy: 0.6777\n",
      "Epoch 21/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 1.5040 - accuracy: 0.4845 - val_loss: 1.0652 - val_accuracy: 0.6866\n",
      "Epoch 22/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 1.4811 - accuracy: 0.4975 - val_loss: 1.0478 - val_accuracy: 0.6920\n",
      "Epoch 23/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 1.4599 - accuracy: 0.5018 - val_loss: 1.0347 - val_accuracy: 0.6971\n",
      "Epoch 24/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 1.4459 - accuracy: 0.5094 - val_loss: 1.0178 - val_accuracy: 0.7006\n",
      "Epoch 25/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 1.4238 - accuracy: 0.5186 - val_loss: 0.9976 - val_accuracy: 0.7067\n",
      "Epoch 26/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 1.4046 - accuracy: 0.5248 - val_loss: 0.9927 - val_accuracy: 0.7118\n",
      "Epoch 27/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 1.3949 - accuracy: 0.5292 - val_loss: 0.9749 - val_accuracy: 0.7148\n",
      "Epoch 28/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 1.3728 - accuracy: 0.5339 - val_loss: 0.9684 - val_accuracy: 0.7166\n",
      "Epoch 29/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 1.3606 - accuracy: 0.5414 - val_loss: 0.9551 - val_accuracy: 0.7193\n",
      "Epoch 30/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 1.3410 - accuracy: 0.5453 - val_loss: 0.9411 - val_accuracy: 0.7253\n",
      "313/313 - 0s - loss: 0.9633 - accuracy: 0.7115 - 313ms/epoch - 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adadelta` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adadelta`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adadelta`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.5452708601951599\n",
      "Validation accuracy : 0.7253333330154419\n",
      "Loss : 0.9633207321166992\n",
      "Accuracy : 0.7114999890327454\n",
      "\n",
      "Train Accuracy: 0.5452708601951599, Validation Accuracy: 0.7253333330154419\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with Adadelta, LR: 0.001, Batch Size: 64\n",
      "Epoch 1/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 3.1321 - accuracy: 0.0940 - val_loss: 2.5018 - val_accuracy: 0.1309\n",
      "Epoch 2/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 2.8965 - accuracy: 0.1257 - val_loss: 2.2867 - val_accuracy: 0.1928\n",
      "Epoch 3/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 2.7085 - accuracy: 0.1549 - val_loss: 2.1059 - val_accuracy: 0.2653\n",
      "Epoch 4/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 2.5593 - accuracy: 0.1814 - val_loss: 1.9686 - val_accuracy: 0.3228\n",
      "Epoch 5/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 2.4421 - accuracy: 0.2059 - val_loss: 1.8619 - val_accuracy: 0.3737\n",
      "Epoch 6/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 2.3292 - accuracy: 0.2308 - val_loss: 1.7684 - val_accuracy: 0.4179\n",
      "Epoch 7/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 2.2358 - accuracy: 0.2558 - val_loss: 1.6915 - val_accuracy: 0.4584\n",
      "Epoch 8/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 2.1569 - accuracy: 0.2799 - val_loss: 1.6214 - val_accuracy: 0.4896\n",
      "Epoch 9/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 2.0831 - accuracy: 0.2990 - val_loss: 1.5591 - val_accuracy: 0.5192\n",
      "Epoch 10/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 2.0274 - accuracy: 0.3125 - val_loss: 1.5087 - val_accuracy: 0.5395\n",
      "Epoch 11/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 1.9840 - accuracy: 0.3313 - val_loss: 1.4622 - val_accuracy: 0.5595\n",
      "Epoch 12/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 1.9273 - accuracy: 0.3485 - val_loss: 1.4200 - val_accuracy: 0.5784\n",
      "Epoch 13/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 1.8769 - accuracy: 0.3626 - val_loss: 1.3808 - val_accuracy: 0.5952\n",
      "Epoch 14/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 1.8338 - accuracy: 0.3772 - val_loss: 1.3475 - val_accuracy: 0.6083\n",
      "Epoch 15/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 1.8010 - accuracy: 0.3889 - val_loss: 1.3151 - val_accuracy: 0.6214\n",
      "Epoch 16/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 1.7676 - accuracy: 0.4012 - val_loss: 1.2853 - val_accuracy: 0.6317\n",
      "Epoch 17/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 1.7286 - accuracy: 0.4139 - val_loss: 1.2579 - val_accuracy: 0.6412\n",
      "Epoch 18/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 1.6990 - accuracy: 0.4279 - val_loss: 1.2346 - val_accuracy: 0.6486\n",
      "Epoch 19/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 1.6733 - accuracy: 0.4367 - val_loss: 1.2125 - val_accuracy: 0.6541\n",
      "Epoch 20/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 1.6401 - accuracy: 0.4486 - val_loss: 1.1888 - val_accuracy: 0.6625\n",
      "Epoch 21/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 1.6223 - accuracy: 0.4574 - val_loss: 1.1682 - val_accuracy: 0.6686\n",
      "Epoch 22/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 1.5950 - accuracy: 0.4640 - val_loss: 1.1489 - val_accuracy: 0.6753\n",
      "Epoch 23/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 1.5722 - accuracy: 0.4736 - val_loss: 1.1326 - val_accuracy: 0.6812\n",
      "Epoch 24/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 1.5525 - accuracy: 0.4814 - val_loss: 1.1156 - val_accuracy: 0.6840\n",
      "Epoch 25/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 1.5287 - accuracy: 0.4871 - val_loss: 1.0964 - val_accuracy: 0.6919\n",
      "Epoch 26/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 1.5072 - accuracy: 0.4954 - val_loss: 1.0818 - val_accuracy: 0.6939\n",
      "Epoch 27/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 1.4939 - accuracy: 0.5036 - val_loss: 1.0658 - val_accuracy: 0.6983\n",
      "Epoch 28/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 1.4792 - accuracy: 0.5066 - val_loss: 1.0506 - val_accuracy: 0.7021\n",
      "Epoch 29/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 1.4614 - accuracy: 0.5142 - val_loss: 1.0383 - val_accuracy: 0.7057\n",
      "Epoch 30/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 1.4369 - accuracy: 0.5235 - val_loss: 1.0284 - val_accuracy: 0.7070\n",
      "313/313 - 0s - loss: 1.0412 - accuracy: 0.6991 - 216ms/epoch - 692us/step\n",
      "\n",
      "Training accuracy : 0.523479163646698\n",
      "Validation accuracy : 0.7070000171661377\n",
      "Loss : 1.041245937347412\n",
      "Accuracy : 0.6991000175476074\n",
      "\n",
      "Train Accuracy: 0.523479163646698, Validation Accuracy: 0.7070000171661377\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with Adadelta, LR: 0.001, Batch Size: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adadelta` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adadelta`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adadelta`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "375/375 [==============================] - 2s 3ms/step - loss: 3.1276 - accuracy: 0.0984 - val_loss: 2.4880 - val_accuracy: 0.0959\n",
      "Epoch 2/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 2.9802 - accuracy: 0.1164 - val_loss: 2.4182 - val_accuracy: 0.1452\n",
      "Epoch 3/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 2.8405 - accuracy: 0.1340 - val_loss: 2.2926 - val_accuracy: 0.1887\n",
      "Epoch 4/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 2.7265 - accuracy: 0.1547 - val_loss: 2.1774 - val_accuracy: 0.2272\n",
      "Epoch 5/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 2.6119 - accuracy: 0.1752 - val_loss: 2.0742 - val_accuracy: 0.2732\n",
      "Epoch 6/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 2.5146 - accuracy: 0.1947 - val_loss: 1.9835 - val_accuracy: 0.3128\n",
      "Epoch 7/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 2.4249 - accuracy: 0.2148 - val_loss: 1.9008 - val_accuracy: 0.3498\n",
      "Epoch 8/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 2.3346 - accuracy: 0.2345 - val_loss: 1.8315 - val_accuracy: 0.3803\n",
      "Epoch 9/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 2.2692 - accuracy: 0.2495 - val_loss: 1.7732 - val_accuracy: 0.4081\n",
      "Epoch 10/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 2.2064 - accuracy: 0.2646 - val_loss: 1.7160 - val_accuracy: 0.4307\n",
      "Epoch 11/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 2.1461 - accuracy: 0.2836 - val_loss: 1.6669 - val_accuracy: 0.4537\n",
      "Epoch 12/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 2.0950 - accuracy: 0.2958 - val_loss: 1.6233 - val_accuracy: 0.4730\n",
      "Epoch 13/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 2.0450 - accuracy: 0.3092 - val_loss: 1.5853 - val_accuracy: 0.4877\n",
      "Epoch 14/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 2.0091 - accuracy: 0.3219 - val_loss: 1.5495 - val_accuracy: 0.5005\n",
      "Epoch 15/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 1.9639 - accuracy: 0.3332 - val_loss: 1.5159 - val_accuracy: 0.5132\n",
      "Epoch 16/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 1.9355 - accuracy: 0.3428 - val_loss: 1.4865 - val_accuracy: 0.5240\n",
      "Epoch 17/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 1.8961 - accuracy: 0.3555 - val_loss: 1.4591 - val_accuracy: 0.5348\n",
      "Epoch 18/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 1.8591 - accuracy: 0.3685 - val_loss: 1.4337 - val_accuracy: 0.5432\n",
      "Epoch 19/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 1.8299 - accuracy: 0.3781 - val_loss: 1.4099 - val_accuracy: 0.5533\n",
      "Epoch 20/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 1.7959 - accuracy: 0.3850 - val_loss: 1.3895 - val_accuracy: 0.5609\n",
      "Epoch 21/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 1.7734 - accuracy: 0.3950 - val_loss: 1.3656 - val_accuracy: 0.5676\n",
      "Epoch 22/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 1.7489 - accuracy: 0.4053 - val_loss: 1.3427 - val_accuracy: 0.5755\n",
      "Epoch 23/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 1.7263 - accuracy: 0.4102 - val_loss: 1.3256 - val_accuracy: 0.5813\n",
      "Epoch 24/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 1.7047 - accuracy: 0.4221 - val_loss: 1.3083 - val_accuracy: 0.5884\n",
      "Epoch 25/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 1.6815 - accuracy: 0.4276 - val_loss: 1.2930 - val_accuracy: 0.5938\n",
      "Epoch 26/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 1.6621 - accuracy: 0.4321 - val_loss: 1.2755 - val_accuracy: 0.6013\n",
      "Epoch 27/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 1.6409 - accuracy: 0.4446 - val_loss: 1.2591 - val_accuracy: 0.6061\n",
      "Epoch 28/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 1.6244 - accuracy: 0.4468 - val_loss: 1.2438 - val_accuracy: 0.6124\n",
      "Epoch 29/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 1.6003 - accuracy: 0.4544 - val_loss: 1.2291 - val_accuracy: 0.6177\n",
      "Epoch 30/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 1.5920 - accuracy: 0.4596 - val_loss: 1.2155 - val_accuracy: 0.6232\n",
      "313/313 - 0s - loss: 1.2304 - accuracy: 0.6219 - 192ms/epoch - 612us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adadelta` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adadelta`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adadelta`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.4596041738986969\n",
      "Validation accuracy : 0.6231666803359985\n",
      "Loss : 1.230400562286377\n",
      "Accuracy : 0.6219000220298767\n",
      "\n",
      "Train Accuracy: 0.4596041738986969, Validation Accuracy: 0.6231666803359985\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with Adadelta, LR: 0.0005, Batch Size: 32\n",
      "Epoch 1/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 3.0818 - accuracy: 0.0885 - val_loss: 2.5852 - val_accuracy: 0.0864\n",
      "Epoch 2/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 2.9800 - accuracy: 0.1035 - val_loss: 2.4524 - val_accuracy: 0.1166\n",
      "Epoch 3/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 2.8566 - accuracy: 0.1185 - val_loss: 2.3499 - val_accuracy: 0.1540\n",
      "Epoch 4/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 2.7629 - accuracy: 0.1315 - val_loss: 2.2435 - val_accuracy: 0.1898\n",
      "Epoch 5/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 2.6644 - accuracy: 0.1452 - val_loss: 2.1532 - val_accuracy: 0.2187\n",
      "Epoch 6/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 2.5708 - accuracy: 0.1632 - val_loss: 2.0750 - val_accuracy: 0.2481\n",
      "Epoch 7/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 2.4925 - accuracy: 0.1787 - val_loss: 2.0063 - val_accuracy: 0.2720\n",
      "Epoch 8/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 2.4167 - accuracy: 0.1941 - val_loss: 1.9402 - val_accuracy: 0.2957\n",
      "Epoch 9/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 2.3678 - accuracy: 0.2047 - val_loss: 1.8852 - val_accuracy: 0.3114\n",
      "Epoch 10/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 2.2972 - accuracy: 0.2203 - val_loss: 1.8343 - val_accuracy: 0.3293\n",
      "Epoch 11/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 2.2510 - accuracy: 0.2340 - val_loss: 1.7932 - val_accuracy: 0.3446\n",
      "Epoch 12/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 2.2100 - accuracy: 0.2436 - val_loss: 1.7437 - val_accuracy: 0.3625\n",
      "Epoch 13/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 2.1575 - accuracy: 0.2554 - val_loss: 1.7026 - val_accuracy: 0.3793\n",
      "Epoch 14/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 2.1179 - accuracy: 0.2670 - val_loss: 1.6723 - val_accuracy: 0.3903\n",
      "Epoch 15/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 2.0851 - accuracy: 0.2728 - val_loss: 1.6354 - val_accuracy: 0.4042\n",
      "Epoch 16/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 2.0496 - accuracy: 0.2811 - val_loss: 1.6025 - val_accuracy: 0.4199\n",
      "Epoch 17/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 2.0113 - accuracy: 0.2951 - val_loss: 1.5764 - val_accuracy: 0.4319\n",
      "Epoch 18/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 1.9854 - accuracy: 0.3037 - val_loss: 1.5521 - val_accuracy: 0.4430\n",
      "Epoch 19/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 1.9523 - accuracy: 0.3139 - val_loss: 1.5219 - val_accuracy: 0.4567\n",
      "Epoch 20/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 1.9215 - accuracy: 0.3243 - val_loss: 1.4968 - val_accuracy: 0.4710\n",
      "Epoch 21/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 1.9023 - accuracy: 0.3326 - val_loss: 1.4756 - val_accuracy: 0.4784\n",
      "Epoch 22/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 1.8765 - accuracy: 0.3383 - val_loss: 1.4457 - val_accuracy: 0.4926\n",
      "Epoch 23/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 1.8530 - accuracy: 0.3475 - val_loss: 1.4269 - val_accuracy: 0.5054\n",
      "Epoch 24/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 1.8303 - accuracy: 0.3499 - val_loss: 1.4030 - val_accuracy: 0.5161\n",
      "Epoch 25/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 1.8031 - accuracy: 0.3617 - val_loss: 1.3850 - val_accuracy: 0.5271\n",
      "Epoch 26/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 1.7855 - accuracy: 0.3686 - val_loss: 1.3664 - val_accuracy: 0.5369\n",
      "Epoch 27/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 1.7674 - accuracy: 0.3730 - val_loss: 1.3484 - val_accuracy: 0.5437\n",
      "Epoch 28/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 1.7450 - accuracy: 0.3808 - val_loss: 1.3262 - val_accuracy: 0.5563\n",
      "Epoch 29/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 1.7288 - accuracy: 0.3908 - val_loss: 1.3172 - val_accuracy: 0.5625\n",
      "Epoch 30/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 1.7109 - accuracy: 0.3937 - val_loss: 1.2985 - val_accuracy: 0.5712\n",
      "313/313 - 0s - loss: 1.3083 - accuracy: 0.5616 - 297ms/epoch - 949us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adadelta` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adadelta`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adadelta`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.39368748664855957\n",
      "Validation accuracy : 0.5711666941642761\n",
      "Loss : 1.3083069324493408\n",
      "Accuracy : 0.5616000294685364\n",
      "\n",
      "Train Accuracy: 0.39368748664855957, Validation Accuracy: 0.5711666941642761\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with Adadelta, LR: 0.0005, Batch Size: 64\n",
      "Epoch 1/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 3.1794 - accuracy: 0.0928 - val_loss: 2.6153 - val_accuracy: 0.1063\n",
      "Epoch 2/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 3.0873 - accuracy: 0.1026 - val_loss: 2.5421 - val_accuracy: 0.1204\n",
      "Epoch 3/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 3.0109 - accuracy: 0.1135 - val_loss: 2.4530 - val_accuracy: 0.1381\n",
      "Epoch 4/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 2.9332 - accuracy: 0.1258 - val_loss: 2.3771 - val_accuracy: 0.1586\n",
      "Epoch 5/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 2.8473 - accuracy: 0.1394 - val_loss: 2.2982 - val_accuracy: 0.1853\n",
      "Epoch 6/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 2.7777 - accuracy: 0.1502 - val_loss: 2.2245 - val_accuracy: 0.2132\n",
      "Epoch 7/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 2.7101 - accuracy: 0.1600 - val_loss: 2.1575 - val_accuracy: 0.2409\n",
      "Epoch 8/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 2.6439 - accuracy: 0.1737 - val_loss: 2.0851 - val_accuracy: 0.2738\n",
      "Epoch 9/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 2.5766 - accuracy: 0.1871 - val_loss: 2.0221 - val_accuracy: 0.3056\n",
      "Epoch 10/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 2.5255 - accuracy: 0.1962 - val_loss: 1.9644 - val_accuracy: 0.3331\n",
      "Epoch 11/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 2.4679 - accuracy: 0.2090 - val_loss: 1.9116 - val_accuracy: 0.3649\n",
      "Epoch 12/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 2.4152 - accuracy: 0.2217 - val_loss: 1.8572 - val_accuracy: 0.3944\n",
      "Epoch 13/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 2.3559 - accuracy: 0.2358 - val_loss: 1.8158 - val_accuracy: 0.4162\n",
      "Epoch 14/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 2.3100 - accuracy: 0.2477 - val_loss: 1.7686 - val_accuracy: 0.4386\n",
      "Epoch 15/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 2.2726 - accuracy: 0.2578 - val_loss: 1.7284 - val_accuracy: 0.4596\n",
      "Epoch 16/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 2.2294 - accuracy: 0.2654 - val_loss: 1.6922 - val_accuracy: 0.4770\n",
      "Epoch 17/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 2.1919 - accuracy: 0.2761 - val_loss: 1.6580 - val_accuracy: 0.4917\n",
      "Epoch 18/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 2.1468 - accuracy: 0.2914 - val_loss: 1.6234 - val_accuracy: 0.5080\n",
      "Epoch 19/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 2.1248 - accuracy: 0.2940 - val_loss: 1.5953 - val_accuracy: 0.5219\n",
      "Epoch 20/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 2.0770 - accuracy: 0.3077 - val_loss: 1.5638 - val_accuracy: 0.5357\n",
      "Epoch 21/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 2.0605 - accuracy: 0.3164 - val_loss: 1.5357 - val_accuracy: 0.5470\n",
      "Epoch 22/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 2.0279 - accuracy: 0.3195 - val_loss: 1.5114 - val_accuracy: 0.5559\n",
      "Epoch 23/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 1.9950 - accuracy: 0.3333 - val_loss: 1.4878 - val_accuracy: 0.5662\n",
      "Epoch 24/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 1.9799 - accuracy: 0.3380 - val_loss: 1.4642 - val_accuracy: 0.5765\n",
      "Epoch 25/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 1.9549 - accuracy: 0.3437 - val_loss: 1.4417 - val_accuracy: 0.5859\n",
      "Epoch 26/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 1.9187 - accuracy: 0.3545 - val_loss: 1.4194 - val_accuracy: 0.5929\n",
      "Epoch 27/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 1.9017 - accuracy: 0.3618 - val_loss: 1.4087 - val_accuracy: 0.5963\n",
      "Epoch 28/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 1.8803 - accuracy: 0.3648 - val_loss: 1.3883 - val_accuracy: 0.6026\n",
      "Epoch 29/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 1.8630 - accuracy: 0.3741 - val_loss: 1.3658 - val_accuracy: 0.6100\n",
      "Epoch 30/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 1.8420 - accuracy: 0.3797 - val_loss: 1.3539 - val_accuracy: 0.6163\n",
      "313/313 - 0s - loss: 1.3680 - accuracy: 0.6005 - 207ms/epoch - 662us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adadelta` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adadelta`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adadelta`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.37968748807907104\n",
      "Validation accuracy : 0.6163333058357239\n",
      "Loss : 1.3679956197738647\n",
      "Accuracy : 0.6004999876022339\n",
      "\n",
      "Train Accuracy: 0.37968748807907104, Validation Accuracy: 0.6163333058357239\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with Adadelta, LR: 0.0005, Batch Size: 128\n",
      "Epoch 1/30\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 3.1411 - accuracy: 0.0943 - val_loss: 2.6055 - val_accuracy: 0.0940\n",
      "Epoch 2/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 3.0713 - accuracy: 0.1042 - val_loss: 2.5800 - val_accuracy: 0.1095\n",
      "Epoch 3/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 3.0155 - accuracy: 0.1108 - val_loss: 2.5220 - val_accuracy: 0.1238\n",
      "Epoch 4/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 2.9523 - accuracy: 0.1162 - val_loss: 2.4605 - val_accuracy: 0.1382\n",
      "Epoch 5/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 2.8941 - accuracy: 0.1268 - val_loss: 2.3975 - val_accuracy: 0.1552\n",
      "Epoch 6/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 2.8486 - accuracy: 0.1330 - val_loss: 2.3394 - val_accuracy: 0.1752\n",
      "Epoch 7/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 2.7754 - accuracy: 0.1447 - val_loss: 2.2853 - val_accuracy: 0.1942\n",
      "Epoch 8/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 2.7195 - accuracy: 0.1538 - val_loss: 2.2284 - val_accuracy: 0.2157\n",
      "Epoch 9/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 2.6707 - accuracy: 0.1613 - val_loss: 2.1786 - val_accuracy: 0.2323\n",
      "Epoch 10/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 2.6218 - accuracy: 0.1696 - val_loss: 2.1286 - val_accuracy: 0.2517\n",
      "Epoch 11/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 2.5684 - accuracy: 0.1799 - val_loss: 2.0813 - val_accuracy: 0.2690\n",
      "Epoch 12/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 2.5251 - accuracy: 0.1929 - val_loss: 2.0365 - val_accuracy: 0.2858\n",
      "Epoch 13/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 2.4850 - accuracy: 0.1974 - val_loss: 1.9949 - val_accuracy: 0.3023\n",
      "Epoch 14/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 2.4487 - accuracy: 0.2076 - val_loss: 1.9564 - val_accuracy: 0.3187\n",
      "Epoch 15/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 2.4065 - accuracy: 0.2175 - val_loss: 1.9180 - val_accuracy: 0.3344\n",
      "Epoch 16/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 2.3582 - accuracy: 0.2271 - val_loss: 1.8832 - val_accuracy: 0.3503\n",
      "Epoch 17/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 2.3277 - accuracy: 0.2344 - val_loss: 1.8476 - val_accuracy: 0.3665\n",
      "Epoch 18/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 2.2952 - accuracy: 0.2417 - val_loss: 1.8139 - val_accuracy: 0.3791\n",
      "Epoch 19/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 2.2664 - accuracy: 0.2489 - val_loss: 1.7840 - val_accuracy: 0.3940\n",
      "Epoch 20/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 2.2298 - accuracy: 0.2582 - val_loss: 1.7564 - val_accuracy: 0.4049\n",
      "Epoch 21/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 2.1981 - accuracy: 0.2686 - val_loss: 1.7285 - val_accuracy: 0.4177\n",
      "Epoch 22/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 2.1690 - accuracy: 0.2730 - val_loss: 1.7025 - val_accuracy: 0.4299\n",
      "Epoch 23/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 2.1485 - accuracy: 0.2795 - val_loss: 1.6772 - val_accuracy: 0.4410\n",
      "Epoch 24/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 2.1274 - accuracy: 0.2896 - val_loss: 1.6551 - val_accuracy: 0.4523\n",
      "Epoch 25/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 2.0934 - accuracy: 0.2976 - val_loss: 1.6320 - val_accuracy: 0.4647\n",
      "Epoch 26/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 2.0613 - accuracy: 0.3075 - val_loss: 1.6111 - val_accuracy: 0.4737\n",
      "Epoch 27/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 2.0437 - accuracy: 0.3120 - val_loss: 1.5900 - val_accuracy: 0.4837\n",
      "Epoch 28/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 2.0249 - accuracy: 0.3159 - val_loss: 1.5686 - val_accuracy: 0.4959\n",
      "Epoch 29/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 2.0121 - accuracy: 0.3211 - val_loss: 1.5510 - val_accuracy: 0.5042\n",
      "Epoch 30/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 1.9885 - accuracy: 0.3304 - val_loss: 1.5323 - val_accuracy: 0.5123\n",
      "313/313 - 0s - loss: 1.5521 - accuracy: 0.5031 - 190ms/epoch - 606us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adadelta` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adadelta`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adadelta`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.33035415410995483\n",
      "Validation accuracy : 0.5122500061988831\n",
      "Loss : 1.5520881414413452\n",
      "Accuracy : 0.5030999779701233\n",
      "\n",
      "Train Accuracy: 0.33035415410995483, Validation Accuracy: 0.5122500061988831\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with Adadelta, LR: 0.0001, Batch Size: 32\n",
      "Epoch 1/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 3.1132 - accuracy: 0.1103 - val_loss: 2.5693 - val_accuracy: 0.1152\n",
      "Epoch 2/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 3.0818 - accuracy: 0.1153 - val_loss: 2.5414 - val_accuracy: 0.1229\n",
      "Epoch 3/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 3.0701 - accuracy: 0.1190 - val_loss: 2.5159 - val_accuracy: 0.1268\n",
      "Epoch 4/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 3.0463 - accuracy: 0.1210 - val_loss: 2.4785 - val_accuracy: 0.1338\n",
      "Epoch 5/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 3.0205 - accuracy: 0.1236 - val_loss: 2.4514 - val_accuracy: 0.1447\n",
      "Epoch 6/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 2.9929 - accuracy: 0.1242 - val_loss: 2.4307 - val_accuracy: 0.1486\n",
      "Epoch 7/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 2.9648 - accuracy: 0.1284 - val_loss: 2.3985 - val_accuracy: 0.1580\n",
      "Epoch 8/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 2.9420 - accuracy: 0.1324 - val_loss: 2.3699 - val_accuracy: 0.1643\n",
      "Epoch 9/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 2.9300 - accuracy: 0.1347 - val_loss: 2.3471 - val_accuracy: 0.1724\n",
      "Epoch 10/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 2.8970 - accuracy: 0.1382 - val_loss: 2.3198 - val_accuracy: 0.1793\n",
      "Epoch 11/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 2.8816 - accuracy: 0.1398 - val_loss: 2.3004 - val_accuracy: 0.1870\n",
      "Epoch 12/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 2.8549 - accuracy: 0.1448 - val_loss: 2.2780 - val_accuracy: 0.1924\n",
      "Epoch 13/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 2.8332 - accuracy: 0.1465 - val_loss: 2.2507 - val_accuracy: 0.2027\n",
      "Epoch 14/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 2.8181 - accuracy: 0.1498 - val_loss: 2.2314 - val_accuracy: 0.2087\n",
      "Epoch 15/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 2.7889 - accuracy: 0.1565 - val_loss: 2.2003 - val_accuracy: 0.2198\n",
      "Epoch 16/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 2.7744 - accuracy: 0.1577 - val_loss: 2.1923 - val_accuracy: 0.2254\n",
      "Epoch 17/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 2.7613 - accuracy: 0.1603 - val_loss: 2.1650 - val_accuracy: 0.2350\n",
      "Epoch 18/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 2.7326 - accuracy: 0.1632 - val_loss: 2.1508 - val_accuracy: 0.2432\n",
      "Epoch 19/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 2.7097 - accuracy: 0.1667 - val_loss: 2.1267 - val_accuracy: 0.2537\n",
      "Epoch 20/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 2.7043 - accuracy: 0.1669 - val_loss: 2.1117 - val_accuracy: 0.2613\n",
      "Epoch 21/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 2.6819 - accuracy: 0.1738 - val_loss: 2.0864 - val_accuracy: 0.2697\n",
      "Epoch 22/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 2.6589 - accuracy: 0.1796 - val_loss: 2.0726 - val_accuracy: 0.2796\n",
      "Epoch 23/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 2.6389 - accuracy: 0.1788 - val_loss: 2.0456 - val_accuracy: 0.2907\n",
      "Epoch 24/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 2.6362 - accuracy: 0.1795 - val_loss: 2.0342 - val_accuracy: 0.2963\n",
      "Epoch 25/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 2.5996 - accuracy: 0.1868 - val_loss: 2.0179 - val_accuracy: 0.3031\n",
      "Epoch 26/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 2.5921 - accuracy: 0.1880 - val_loss: 2.0015 - val_accuracy: 0.3117\n",
      "Epoch 27/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 2.5819 - accuracy: 0.1920 - val_loss: 1.9807 - val_accuracy: 0.3235\n",
      "Epoch 28/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 2.5821 - accuracy: 0.1938 - val_loss: 1.9714 - val_accuracy: 0.3271\n",
      "Epoch 29/30\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 2.5454 - accuracy: 0.2001 - val_loss: 1.9550 - val_accuracy: 0.3383\n",
      "Epoch 30/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 2.5321 - accuracy: 0.1999 - val_loss: 1.9437 - val_accuracy: 0.3432\n",
      "313/313 - 0s - loss: 1.9564 - accuracy: 0.3293 - 299ms/epoch - 955us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adadelta` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adadelta`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adadelta`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.19993749260902405\n",
      "Validation accuracy : 0.34316667914390564\n",
      "Loss : 1.9564294815063477\n",
      "Accuracy : 0.3292999863624573\n",
      "\n",
      "Train Accuracy: 0.19993749260902405, Validation Accuracy: 0.34316667914390564\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with Adadelta, LR: 0.0001, Batch Size: 64\n",
      "Epoch 1/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 2.9750 - accuracy: 0.1184 - val_loss: 2.4530 - val_accuracy: 0.1306\n",
      "Epoch 2/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 2.9571 - accuracy: 0.1213 - val_loss: 2.4539 - val_accuracy: 0.1314\n",
      "Epoch 3/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 2.9458 - accuracy: 0.1241 - val_loss: 2.4306 - val_accuracy: 0.1375\n",
      "Epoch 4/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 2.9305 - accuracy: 0.1238 - val_loss: 2.4257 - val_accuracy: 0.1404\n",
      "Epoch 5/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 2.9192 - accuracy: 0.1263 - val_loss: 2.3985 - val_accuracy: 0.1478\n",
      "Epoch 6/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 2.8998 - accuracy: 0.1314 - val_loss: 2.3911 - val_accuracy: 0.1525\n",
      "Epoch 7/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 2.9003 - accuracy: 0.1319 - val_loss: 2.3777 - val_accuracy: 0.1566\n",
      "Epoch 8/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 2.8701 - accuracy: 0.1336 - val_loss: 2.3624 - val_accuracy: 0.1603\n",
      "Epoch 9/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 2.8667 - accuracy: 0.1327 - val_loss: 2.3487 - val_accuracy: 0.1654\n",
      "Epoch 10/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 2.8500 - accuracy: 0.1396 - val_loss: 2.3339 - val_accuracy: 0.1676\n",
      "Epoch 11/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 2.8280 - accuracy: 0.1425 - val_loss: 2.3213 - val_accuracy: 0.1734\n",
      "Epoch 12/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 2.8249 - accuracy: 0.1411 - val_loss: 2.3040 - val_accuracy: 0.1777\n",
      "Epoch 13/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 2.7993 - accuracy: 0.1454 - val_loss: 2.2937 - val_accuracy: 0.1815\n",
      "Epoch 14/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 2.7903 - accuracy: 0.1447 - val_loss: 2.2816 - val_accuracy: 0.1849\n",
      "Epoch 15/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 2.7897 - accuracy: 0.1492 - val_loss: 2.2716 - val_accuracy: 0.1915\n",
      "Epoch 16/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 2.7666 - accuracy: 0.1503 - val_loss: 2.2550 - val_accuracy: 0.1953\n",
      "Epoch 17/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 2.7538 - accuracy: 0.1544 - val_loss: 2.2381 - val_accuracy: 0.2008\n",
      "Epoch 18/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 2.7484 - accuracy: 0.1564 - val_loss: 2.2328 - val_accuracy: 0.2037\n",
      "Epoch 19/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 2.7270 - accuracy: 0.1589 - val_loss: 2.2183 - val_accuracy: 0.2103\n",
      "Epoch 20/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 2.7060 - accuracy: 0.1626 - val_loss: 2.2062 - val_accuracy: 0.2139\n",
      "Epoch 21/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 2.6927 - accuracy: 0.1663 - val_loss: 2.1923 - val_accuracy: 0.2188\n",
      "Epoch 22/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 2.6988 - accuracy: 0.1656 - val_loss: 2.1810 - val_accuracy: 0.2239\n",
      "Epoch 23/30\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 2.6849 - accuracy: 0.1670 - val_loss: 2.1705 - val_accuracy: 0.2292\n",
      "Epoch 24/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 2.6690 - accuracy: 0.1694 - val_loss: 2.1620 - val_accuracy: 0.2333\n",
      "Epoch 25/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 2.6557 - accuracy: 0.1765 - val_loss: 2.1454 - val_accuracy: 0.2405\n",
      "Epoch 26/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 2.6533 - accuracy: 0.1732 - val_loss: 2.1381 - val_accuracy: 0.2436\n",
      "Epoch 27/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 2.6365 - accuracy: 0.1751 - val_loss: 2.1216 - val_accuracy: 0.2512\n",
      "Epoch 28/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 2.6254 - accuracy: 0.1799 - val_loss: 2.1119 - val_accuracy: 0.2574\n",
      "Epoch 29/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 2.6138 - accuracy: 0.1809 - val_loss: 2.1008 - val_accuracy: 0.2610\n",
      "Epoch 30/30\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 2.5945 - accuracy: 0.1829 - val_loss: 2.0897 - val_accuracy: 0.2644\n",
      "313/313 - 0s - loss: 2.0941 - accuracy: 0.2645 - 204ms/epoch - 653us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adadelta` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adadelta`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adadelta`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.18285416066646576\n",
      "Validation accuracy : 0.2644166648387909\n",
      "Loss : 2.094120740890503\n",
      "Accuracy : 0.2644999921321869\n",
      "\n",
      "Train Accuracy: 0.18285416066646576, Validation Accuracy: 0.2644166648387909\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with Adadelta, LR: 0.0001, Batch Size: 128\n",
      "Epoch 1/30\n",
      "375/375 [==============================] - 2s 3ms/step - loss: 3.3979 - accuracy: 0.0802 - val_loss: 2.6819 - val_accuracy: 0.0540\n",
      "Epoch 2/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 3.3673 - accuracy: 0.0831 - val_loss: 2.7989 - val_accuracy: 0.0634\n",
      "Epoch 3/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 3.3580 - accuracy: 0.0826 - val_loss: 2.7945 - val_accuracy: 0.0662\n",
      "Epoch 4/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 3.3487 - accuracy: 0.0834 - val_loss: 2.7707 - val_accuracy: 0.0690\n",
      "Epoch 5/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 3.3111 - accuracy: 0.0872 - val_loss: 2.7513 - val_accuracy: 0.0712\n",
      "Epoch 6/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 3.3042 - accuracy: 0.0882 - val_loss: 2.7338 - val_accuracy: 0.0736\n",
      "Epoch 7/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 3.2825 - accuracy: 0.0892 - val_loss: 2.7089 - val_accuracy: 0.0768\n",
      "Epoch 8/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 3.2660 - accuracy: 0.0904 - val_loss: 2.6885 - val_accuracy: 0.0793\n",
      "Epoch 9/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 3.2470 - accuracy: 0.0948 - val_loss: 2.6679 - val_accuracy: 0.0841\n",
      "Epoch 10/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 3.2319 - accuracy: 0.0958 - val_loss: 2.6493 - val_accuracy: 0.0870\n",
      "Epoch 11/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 3.2151 - accuracy: 0.0958 - val_loss: 2.6244 - val_accuracy: 0.0904\n",
      "Epoch 12/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 3.1862 - accuracy: 0.0991 - val_loss: 2.6090 - val_accuracy: 0.0943\n",
      "Epoch 13/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 3.1836 - accuracy: 0.0998 - val_loss: 2.5878 - val_accuracy: 0.0984\n",
      "Epoch 14/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 3.1652 - accuracy: 0.1020 - val_loss: 2.5707 - val_accuracy: 0.1028\n",
      "Epoch 15/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 3.1379 - accuracy: 0.1042 - val_loss: 2.5530 - val_accuracy: 0.1062\n",
      "Epoch 16/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 3.1219 - accuracy: 0.1051 - val_loss: 2.5316 - val_accuracy: 0.1088\n",
      "Epoch 17/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 3.1026 - accuracy: 0.1075 - val_loss: 2.5128 - val_accuracy: 0.1133\n",
      "Epoch 18/30\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 3.0954 - accuracy: 0.1093 - val_loss: 2.4956 - val_accuracy: 0.1182\n",
      "Epoch 19/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 3.0725 - accuracy: 0.1130 - val_loss: 2.4768 - val_accuracy: 0.1217\n",
      "Epoch 20/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 3.0641 - accuracy: 0.1142 - val_loss: 2.4635 - val_accuracy: 0.1257\n",
      "Epoch 21/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 3.0329 - accuracy: 0.1152 - val_loss: 2.4461 - val_accuracy: 0.1302\n",
      "Epoch 22/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 3.0230 - accuracy: 0.1160 - val_loss: 2.4262 - val_accuracy: 0.1361\n",
      "Epoch 23/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 2.9963 - accuracy: 0.1203 - val_loss: 2.4092 - val_accuracy: 0.1415\n",
      "Epoch 24/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 2.9987 - accuracy: 0.1205 - val_loss: 2.3943 - val_accuracy: 0.1456\n",
      "Epoch 25/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 2.9748 - accuracy: 0.1226 - val_loss: 2.3750 - val_accuracy: 0.1515\n",
      "Epoch 26/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 2.9582 - accuracy: 0.1267 - val_loss: 2.3594 - val_accuracy: 0.1550\n",
      "Epoch 27/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 2.9592 - accuracy: 0.1232 - val_loss: 2.3459 - val_accuracy: 0.1596\n",
      "Epoch 28/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 2.9375 - accuracy: 0.1270 - val_loss: 2.3250 - val_accuracy: 0.1660\n",
      "Epoch 29/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 2.9218 - accuracy: 0.1323 - val_loss: 2.3084 - val_accuracy: 0.1714\n",
      "Epoch 30/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 2.9049 - accuracy: 0.1316 - val_loss: 2.2919 - val_accuracy: 0.1772\n",
      "313/313 - 0s - loss: 2.3002 - accuracy: 0.1729 - 193ms/epoch - 616us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Nadam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Nadam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Nadam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.1316249966621399\n",
      "Validation accuracy : 0.1771666705608368\n",
      "Loss : 2.300171375274658\n",
      "Accuracy : 0.1729000061750412\n",
      "\n",
      "Train Accuracy: 0.1316249966621399, Validation Accuracy: 0.1771666705608368\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with Nadam, LR: 0.001, Batch Size: 32\n",
      "Epoch 1/30\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.8190 - accuracy: 0.7201 - val_loss: 0.4464 - val_accuracy: 0.8394\n",
      "Epoch 2/30\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 0.5905 - accuracy: 0.7994 - val_loss: 0.4264 - val_accuracy: 0.8439\n",
      "Epoch 3/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.5628 - accuracy: 0.8077 - val_loss: 0.4340 - val_accuracy: 0.8418\n",
      "Epoch 4/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.5375 - accuracy: 0.8166 - val_loss: 0.4278 - val_accuracy: 0.8416\n",
      "Epoch 5/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.5144 - accuracy: 0.8241 - val_loss: 0.3953 - val_accuracy: 0.8570\n",
      "Epoch 6/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4948 - accuracy: 0.8283 - val_loss: 0.3980 - val_accuracy: 0.8558\n",
      "Epoch 7/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4927 - accuracy: 0.8298 - val_loss: 0.3852 - val_accuracy: 0.8598\n",
      "Epoch 8/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4845 - accuracy: 0.8342 - val_loss: 0.3911 - val_accuracy: 0.8583\n",
      "Epoch 9/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4692 - accuracy: 0.8378 - val_loss: 0.3814 - val_accuracy: 0.8609\n",
      "Epoch 10/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4664 - accuracy: 0.8387 - val_loss: 0.3973 - val_accuracy: 0.8577\n",
      "Epoch 11/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4607 - accuracy: 0.8408 - val_loss: 0.3863 - val_accuracy: 0.8614\n",
      "Epoch 12/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4559 - accuracy: 0.8429 - val_loss: 0.3849 - val_accuracy: 0.8589\n",
      "Epoch 13/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4502 - accuracy: 0.8451 - val_loss: 0.3790 - val_accuracy: 0.8648\n",
      "Epoch 14/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4431 - accuracy: 0.8491 - val_loss: 0.3636 - val_accuracy: 0.8691\n",
      "Epoch 15/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4349 - accuracy: 0.8514 - val_loss: 0.3732 - val_accuracy: 0.8652\n",
      "Epoch 16/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4349 - accuracy: 0.8512 - val_loss: 0.3624 - val_accuracy: 0.8706\n",
      "Epoch 17/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4365 - accuracy: 0.8495 - val_loss: 0.3609 - val_accuracy: 0.8692\n",
      "Epoch 18/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4332 - accuracy: 0.8503 - val_loss: 0.3762 - val_accuracy: 0.8679\n",
      "Epoch 19/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4298 - accuracy: 0.8518 - val_loss: 0.3534 - val_accuracy: 0.8758\n",
      "Epoch 20/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4202 - accuracy: 0.8561 - val_loss: 0.3555 - val_accuracy: 0.8733\n",
      "Epoch 21/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4238 - accuracy: 0.8532 - val_loss: 0.3595 - val_accuracy: 0.8717\n",
      "Epoch 22/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4153 - accuracy: 0.8570 - val_loss: 0.3451 - val_accuracy: 0.8766\n",
      "Epoch 23/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4152 - accuracy: 0.8559 - val_loss: 0.3627 - val_accuracy: 0.8686\n",
      "Epoch 24/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4106 - accuracy: 0.8580 - val_loss: 0.3607 - val_accuracy: 0.8723\n",
      "Epoch 25/30\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.4054 - accuracy: 0.8617 - val_loss: 0.3570 - val_accuracy: 0.8717\n",
      "Epoch 26/30\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 0.4102 - accuracy: 0.8583 - val_loss: 0.3544 - val_accuracy: 0.8742\n",
      "Epoch 27/30\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 0.4012 - accuracy: 0.8616 - val_loss: 0.3575 - val_accuracy: 0.8714\n",
      "Epoch 28/30\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.4010 - accuracy: 0.8614 - val_loss: 0.3686 - val_accuracy: 0.8686\n",
      "Epoch 29/30\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.4020 - accuracy: 0.8608 - val_loss: 0.3482 - val_accuracy: 0.8763\n",
      "Epoch 30/30\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 0.3976 - accuracy: 0.8636 - val_loss: 0.3581 - val_accuracy: 0.8731\n",
      "313/313 - 0s - loss: 0.3854 - accuracy: 0.8594 - 302ms/epoch - 966us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Nadam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Nadam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Nadam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.8636249899864197\n",
      "Validation accuracy : 0.8730833530426025\n",
      "Loss : 0.38537275791168213\n",
      "Accuracy : 0.8593999743461609\n",
      "\n",
      "Train Accuracy: 0.8636249899864197, Validation Accuracy: 0.8730833530426025\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with Nadam, LR: 0.001, Batch Size: 64\n",
      "Epoch 1/30\n",
      "750/750 [==============================] - 3s 3ms/step - loss: 0.7840 - accuracy: 0.7347 - val_loss: 0.5028 - val_accuracy: 0.8216\n",
      "Epoch 2/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.5486 - accuracy: 0.8131 - val_loss: 0.4116 - val_accuracy: 0.8529\n",
      "Epoch 3/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.5155 - accuracy: 0.8243 - val_loss: 0.4131 - val_accuracy: 0.8532\n",
      "Epoch 4/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.4885 - accuracy: 0.8318 - val_loss: 0.4053 - val_accuracy: 0.8547\n",
      "Epoch 5/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.4852 - accuracy: 0.8340 - val_loss: 0.3729 - val_accuracy: 0.8648\n",
      "Epoch 6/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.4664 - accuracy: 0.8403 - val_loss: 0.3982 - val_accuracy: 0.8529\n",
      "Epoch 7/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.4568 - accuracy: 0.8419 - val_loss: 0.3823 - val_accuracy: 0.8620\n",
      "Epoch 8/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.4334 - accuracy: 0.8497 - val_loss: 0.3741 - val_accuracy: 0.8649\n",
      "Epoch 9/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.4420 - accuracy: 0.8459 - val_loss: 0.3712 - val_accuracy: 0.8679\n",
      "Epoch 10/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.4337 - accuracy: 0.8508 - val_loss: 0.3786 - val_accuracy: 0.8673\n",
      "Epoch 11/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.4246 - accuracy: 0.8531 - val_loss: 0.3784 - val_accuracy: 0.8612\n",
      "Epoch 12/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.4183 - accuracy: 0.8560 - val_loss: 0.3683 - val_accuracy: 0.8683\n",
      "Epoch 13/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.4191 - accuracy: 0.8550 - val_loss: 0.3864 - val_accuracy: 0.8618\n",
      "Epoch 14/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.4175 - accuracy: 0.8556 - val_loss: 0.3798 - val_accuracy: 0.8658\n",
      "Epoch 15/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.4086 - accuracy: 0.8589 - val_loss: 0.3877 - val_accuracy: 0.8635\n",
      "Epoch 16/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.3977 - accuracy: 0.8611 - val_loss: 0.3531 - val_accuracy: 0.8735\n",
      "Epoch 17/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.3947 - accuracy: 0.8641 - val_loss: 0.3389 - val_accuracy: 0.8788\n",
      "Epoch 18/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.3881 - accuracy: 0.8654 - val_loss: 0.3474 - val_accuracy: 0.8783\n",
      "Epoch 19/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.3874 - accuracy: 0.8651 - val_loss: 0.3593 - val_accuracy: 0.8723\n",
      "Epoch 20/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.3883 - accuracy: 0.8653 - val_loss: 0.3486 - val_accuracy: 0.8770\n",
      "Epoch 21/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.3868 - accuracy: 0.8664 - val_loss: 0.3615 - val_accuracy: 0.8741\n",
      "Epoch 22/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.3816 - accuracy: 0.8691 - val_loss: 0.3470 - val_accuracy: 0.8788\n",
      "Epoch 23/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.3758 - accuracy: 0.8695 - val_loss: 0.3562 - val_accuracy: 0.8752\n",
      "Epoch 24/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.3739 - accuracy: 0.8700 - val_loss: 0.3487 - val_accuracy: 0.8744\n",
      "Epoch 25/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.3715 - accuracy: 0.8721 - val_loss: 0.3538 - val_accuracy: 0.8758\n",
      "Epoch 26/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.3663 - accuracy: 0.8724 - val_loss: 0.3473 - val_accuracy: 0.8786\n",
      "Epoch 27/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.3652 - accuracy: 0.8734 - val_loss: 0.3597 - val_accuracy: 0.8730\n",
      "Epoch 28/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.3605 - accuracy: 0.8739 - val_loss: 0.3413 - val_accuracy: 0.8783\n",
      "Epoch 29/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.3583 - accuracy: 0.8756 - val_loss: 0.3386 - val_accuracy: 0.8856\n",
      "Epoch 30/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.3587 - accuracy: 0.8745 - val_loss: 0.3523 - val_accuracy: 0.8797\n",
      "313/313 - 0s - loss: 0.3839 - accuracy: 0.8680 - 190ms/epoch - 608us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Nadam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Nadam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Nadam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.8744999766349792\n",
      "Validation accuracy : 0.8796666860580444\n",
      "Loss : 0.38391780853271484\n",
      "Accuracy : 0.8679999709129333\n",
      "\n",
      "Train Accuracy: 0.8744999766349792, Validation Accuracy: 0.8796666860580444\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with Nadam, LR: 0.001, Batch Size: 128\n",
      "Epoch 1/30\n",
      "375/375 [==============================] - 3s 5ms/step - loss: 0.8599 - accuracy: 0.7126 - val_loss: 0.4915 - val_accuracy: 0.8253\n",
      "Epoch 2/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.5630 - accuracy: 0.8095 - val_loss: 0.4300 - val_accuracy: 0.8456\n",
      "Epoch 3/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.5028 - accuracy: 0.8281 - val_loss: 0.4215 - val_accuracy: 0.8488\n",
      "Epoch 4/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4701 - accuracy: 0.8388 - val_loss: 0.3999 - val_accuracy: 0.8529\n",
      "Epoch 5/30\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4523 - accuracy: 0.8442 - val_loss: 0.3887 - val_accuracy: 0.8571\n",
      "Epoch 6/30\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4316 - accuracy: 0.8517 - val_loss: 0.3715 - val_accuracy: 0.8644\n",
      "Epoch 7/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4261 - accuracy: 0.8525 - val_loss: 0.3677 - val_accuracy: 0.8688\n",
      "Epoch 8/30\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4171 - accuracy: 0.8564 - val_loss: 0.3933 - val_accuracy: 0.8574\n",
      "Epoch 9/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4032 - accuracy: 0.8623 - val_loss: 0.3653 - val_accuracy: 0.8683\n",
      "Epoch 10/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3940 - accuracy: 0.8643 - val_loss: 0.3534 - val_accuracy: 0.8756\n",
      "Epoch 11/30\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.3902 - accuracy: 0.8630 - val_loss: 0.3488 - val_accuracy: 0.8773\n",
      "Epoch 12/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3857 - accuracy: 0.8667 - val_loss: 0.3603 - val_accuracy: 0.8712\n",
      "Epoch 13/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3860 - accuracy: 0.8671 - val_loss: 0.3553 - val_accuracy: 0.8743\n",
      "Epoch 14/30\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3808 - accuracy: 0.8684 - val_loss: 0.3467 - val_accuracy: 0.8781\n",
      "Epoch 15/30\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3765 - accuracy: 0.8702 - val_loss: 0.3632 - val_accuracy: 0.8735\n",
      "Epoch 16/30\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3680 - accuracy: 0.8719 - val_loss: 0.3523 - val_accuracy: 0.8755\n",
      "Epoch 17/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3700 - accuracy: 0.8704 - val_loss: 0.3475 - val_accuracy: 0.8753\n",
      "Epoch 18/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3588 - accuracy: 0.8740 - val_loss: 0.3674 - val_accuracy: 0.8683\n",
      "Epoch 19/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3561 - accuracy: 0.8755 - val_loss: 0.3376 - val_accuracy: 0.8818\n",
      "Epoch 20/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3458 - accuracy: 0.8793 - val_loss: 0.3424 - val_accuracy: 0.8763\n",
      "Epoch 21/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3494 - accuracy: 0.8784 - val_loss: 0.3359 - val_accuracy: 0.8810\n",
      "Epoch 22/30\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3472 - accuracy: 0.8792 - val_loss: 0.3456 - val_accuracy: 0.8765\n",
      "Epoch 23/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3446 - accuracy: 0.8797 - val_loss: 0.3472 - val_accuracy: 0.8813\n",
      "Epoch 24/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3436 - accuracy: 0.8797 - val_loss: 0.3376 - val_accuracy: 0.8773\n",
      "Epoch 25/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3447 - accuracy: 0.8785 - val_loss: 0.3482 - val_accuracy: 0.8763\n",
      "Epoch 26/30\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3339 - accuracy: 0.8818 - val_loss: 0.3394 - val_accuracy: 0.8838\n",
      "Epoch 27/30\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.3325 - accuracy: 0.8829 - val_loss: 0.3763 - val_accuracy: 0.8686\n",
      "Epoch 28/30\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3312 - accuracy: 0.8827 - val_loss: 0.3460 - val_accuracy: 0.8790\n",
      "Epoch 29/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3274 - accuracy: 0.8840 - val_loss: 0.3356 - val_accuracy: 0.8802\n",
      "Epoch 30/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3293 - accuracy: 0.8849 - val_loss: 0.3358 - val_accuracy: 0.8811\n",
      "313/313 - 0s - loss: 0.3585 - accuracy: 0.8737 - 216ms/epoch - 690us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Nadam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Nadam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Nadam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.8848749995231628\n",
      "Validation accuracy : 0.8810833096504211\n",
      "Loss : 0.3585035800933838\n",
      "Accuracy : 0.8737000226974487\n",
      "\n",
      "Train Accuracy: 0.8848749995231628, Validation Accuracy: 0.8810833096504211\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with Nadam, LR: 0.0005, Batch Size: 32\n",
      "Epoch 1/30\n",
      "1500/1500 [==============================] - 5s 2ms/step - loss: 0.9001 - accuracy: 0.6992 - val_loss: 0.4607 - val_accuracy: 0.8341\n",
      "Epoch 2/30\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 0.6065 - accuracy: 0.7931 - val_loss: 0.4365 - val_accuracy: 0.8388\n",
      "Epoch 3/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.5558 - accuracy: 0.8088 - val_loss: 0.4103 - val_accuracy: 0.8513\n",
      "Epoch 4/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.5215 - accuracy: 0.8236 - val_loss: 0.3965 - val_accuracy: 0.8547\n",
      "Epoch 5/30\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 0.5009 - accuracy: 0.8288 - val_loss: 0.3942 - val_accuracy: 0.8587\n",
      "Epoch 6/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4813 - accuracy: 0.8355 - val_loss: 0.3834 - val_accuracy: 0.8589\n",
      "Epoch 7/30\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.4729 - accuracy: 0.8377 - val_loss: 0.3747 - val_accuracy: 0.8657\n",
      "Epoch 8/30\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.4612 - accuracy: 0.8420 - val_loss: 0.3693 - val_accuracy: 0.8658\n",
      "Epoch 9/30\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.4544 - accuracy: 0.8429 - val_loss: 0.3669 - val_accuracy: 0.8671\n",
      "Epoch 10/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4474 - accuracy: 0.8454 - val_loss: 0.3814 - val_accuracy: 0.8593\n",
      "Epoch 11/30\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.4418 - accuracy: 0.8463 - val_loss: 0.3592 - val_accuracy: 0.8713\n",
      "Epoch 12/30\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 0.4306 - accuracy: 0.8534 - val_loss: 0.3609 - val_accuracy: 0.8673\n",
      "Epoch 13/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4225 - accuracy: 0.8540 - val_loss: 0.3628 - val_accuracy: 0.8696\n",
      "Epoch 14/30\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 0.4278 - accuracy: 0.8524 - val_loss: 0.3496 - val_accuracy: 0.8746\n",
      "Epoch 15/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4213 - accuracy: 0.8540 - val_loss: 0.3560 - val_accuracy: 0.8718\n",
      "Epoch 16/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4129 - accuracy: 0.8568 - val_loss: 0.3613 - val_accuracy: 0.8683\n",
      "Epoch 17/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4134 - accuracy: 0.8564 - val_loss: 0.3607 - val_accuracy: 0.8703\n",
      "Epoch 18/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4128 - accuracy: 0.8574 - val_loss: 0.3644 - val_accuracy: 0.8738\n",
      "Epoch 19/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4065 - accuracy: 0.8603 - val_loss: 0.3512 - val_accuracy: 0.8729\n",
      "Epoch 20/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4027 - accuracy: 0.8594 - val_loss: 0.3529 - val_accuracy: 0.8763\n",
      "Epoch 21/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4033 - accuracy: 0.8614 - val_loss: 0.3337 - val_accuracy: 0.8790\n",
      "Epoch 22/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4007 - accuracy: 0.8614 - val_loss: 0.3570 - val_accuracy: 0.8748\n",
      "Epoch 23/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3938 - accuracy: 0.8643 - val_loss: 0.3529 - val_accuracy: 0.8758\n",
      "Epoch 24/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3901 - accuracy: 0.8638 - val_loss: 0.3423 - val_accuracy: 0.8773\n",
      "Epoch 25/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3879 - accuracy: 0.8662 - val_loss: 0.3384 - val_accuracy: 0.8781\n",
      "Epoch 26/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3870 - accuracy: 0.8659 - val_loss: 0.3521 - val_accuracy: 0.8729\n",
      "Epoch 27/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3847 - accuracy: 0.8655 - val_loss: 0.3374 - val_accuracy: 0.8787\n",
      "Epoch 28/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3818 - accuracy: 0.8670 - val_loss: 0.3413 - val_accuracy: 0.8807\n",
      "Epoch 29/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3770 - accuracy: 0.8689 - val_loss: 0.3354 - val_accuracy: 0.8816\n",
      "Epoch 30/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3817 - accuracy: 0.8683 - val_loss: 0.3357 - val_accuracy: 0.8830\n",
      "313/313 - 0s - loss: 0.3599 - accuracy: 0.8735 - 299ms/epoch - 956us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Nadam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Nadam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Nadam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.8683124780654907\n",
      "Validation accuracy : 0.8830000162124634\n",
      "Loss : 0.3598942756652832\n",
      "Accuracy : 0.8734999895095825\n",
      "\n",
      "Train Accuracy: 0.8683124780654907, Validation Accuracy: 0.8830000162124634\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with Nadam, LR: 0.0005, Batch Size: 64\n",
      "Epoch 1/30\n",
      "750/750 [==============================] - 5s 5ms/step - loss: 0.9286 - accuracy: 0.6964 - val_loss: 0.4880 - val_accuracy: 0.8225\n",
      "Epoch 2/30\n",
      "750/750 [==============================] - 3s 4ms/step - loss: 0.6035 - accuracy: 0.7976 - val_loss: 0.4169 - val_accuracy: 0.8500\n",
      "Epoch 3/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.5328 - accuracy: 0.8205 - val_loss: 0.4127 - val_accuracy: 0.8487\n",
      "Epoch 4/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.4982 - accuracy: 0.8326 - val_loss: 0.3825 - val_accuracy: 0.8625\n",
      "Epoch 5/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.4750 - accuracy: 0.8383 - val_loss: 0.3834 - val_accuracy: 0.8607\n",
      "Epoch 6/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.4544 - accuracy: 0.8455 - val_loss: 0.3594 - val_accuracy: 0.8712\n",
      "Epoch 7/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.4412 - accuracy: 0.8503 - val_loss: 0.3688 - val_accuracy: 0.8672\n",
      "Epoch 8/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.4314 - accuracy: 0.8534 - val_loss: 0.3612 - val_accuracy: 0.8707\n",
      "Epoch 9/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.4251 - accuracy: 0.8546 - val_loss: 0.3489 - val_accuracy: 0.8729\n",
      "Epoch 10/30\n",
      "750/750 [==============================] - 4s 5ms/step - loss: 0.4148 - accuracy: 0.8587 - val_loss: 0.3432 - val_accuracy: 0.8773\n",
      "Epoch 11/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.4091 - accuracy: 0.8596 - val_loss: 0.3559 - val_accuracy: 0.8739\n",
      "Epoch 12/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.4005 - accuracy: 0.8621 - val_loss: 0.3633 - val_accuracy: 0.8690\n",
      "Epoch 13/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.3892 - accuracy: 0.8669 - val_loss: 0.3534 - val_accuracy: 0.8767\n",
      "Epoch 14/30\n",
      "750/750 [==============================] - 3s 3ms/step - loss: 0.3860 - accuracy: 0.8663 - val_loss: 0.3491 - val_accuracy: 0.8752\n",
      "Epoch 15/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.3873 - accuracy: 0.8661 - val_loss: 0.3835 - val_accuracy: 0.8637\n",
      "Epoch 16/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.3823 - accuracy: 0.8691 - val_loss: 0.3445 - val_accuracy: 0.8798\n",
      "Epoch 17/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.3765 - accuracy: 0.8697 - val_loss: 0.3481 - val_accuracy: 0.8780\n",
      "Epoch 18/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.3694 - accuracy: 0.8729 - val_loss: 0.3367 - val_accuracy: 0.8793\n",
      "Epoch 19/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.3724 - accuracy: 0.8714 - val_loss: 0.3435 - val_accuracy: 0.8763\n",
      "Epoch 20/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.3719 - accuracy: 0.8707 - val_loss: 0.3393 - val_accuracy: 0.8789\n",
      "Epoch 21/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.3644 - accuracy: 0.8749 - val_loss: 0.3333 - val_accuracy: 0.8837\n",
      "Epoch 22/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.3581 - accuracy: 0.8754 - val_loss: 0.3351 - val_accuracy: 0.8816\n",
      "Epoch 23/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.3579 - accuracy: 0.8766 - val_loss: 0.3283 - val_accuracy: 0.8842\n",
      "Epoch 24/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.3498 - accuracy: 0.8790 - val_loss: 0.3927 - val_accuracy: 0.8637\n",
      "Epoch 25/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.3553 - accuracy: 0.8777 - val_loss: 0.3338 - val_accuracy: 0.8808\n",
      "Epoch 26/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.3510 - accuracy: 0.8781 - val_loss: 0.3575 - val_accuracy: 0.8727\n",
      "Epoch 27/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.3427 - accuracy: 0.8811 - val_loss: 0.3334 - val_accuracy: 0.8810\n",
      "Epoch 28/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.3405 - accuracy: 0.8795 - val_loss: 0.3381 - val_accuracy: 0.8807\n",
      "Epoch 29/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.3402 - accuracy: 0.8815 - val_loss: 0.3623 - val_accuracy: 0.8742\n",
      "Epoch 30/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.3379 - accuracy: 0.8828 - val_loss: 0.3260 - val_accuracy: 0.8888\n",
      "313/313 - 0s - loss: 0.3457 - accuracy: 0.8813 - 196ms/epoch - 627us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Nadam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Nadam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Nadam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.8828125\n",
      "Validation accuracy : 0.8887500166893005\n",
      "Loss : 0.34573671221733093\n",
      "Accuracy : 0.8812999725341797\n",
      "\n",
      "Train Accuracy: 0.8828125, Validation Accuracy: 0.8887500166893005\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with Nadam, LR: 0.0005, Batch Size: 128\n",
      "Epoch 1/30\n",
      "375/375 [==============================] - 3s 5ms/step - loss: 1.0302 - accuracy: 0.6621 - val_loss: 0.5242 - val_accuracy: 0.8213\n",
      "Epoch 2/30\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.6274 - accuracy: 0.7923 - val_loss: 0.4419 - val_accuracy: 0.8447\n",
      "Epoch 3/30\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.5441 - accuracy: 0.8173 - val_loss: 0.3995 - val_accuracy: 0.8590\n",
      "Epoch 4/30\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4987 - accuracy: 0.8309 - val_loss: 0.3908 - val_accuracy: 0.8588\n",
      "Epoch 5/30\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4740 - accuracy: 0.8413 - val_loss: 0.4036 - val_accuracy: 0.8566\n",
      "Epoch 6/30\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4508 - accuracy: 0.8473 - val_loss: 0.3926 - val_accuracy: 0.8589\n",
      "Epoch 7/30\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4372 - accuracy: 0.8531 - val_loss: 0.3951 - val_accuracy: 0.8597\n",
      "Epoch 8/30\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4217 - accuracy: 0.8556 - val_loss: 0.3729 - val_accuracy: 0.8681\n",
      "Epoch 9/30\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4140 - accuracy: 0.8601 - val_loss: 0.3558 - val_accuracy: 0.8728\n",
      "Epoch 10/30\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4018 - accuracy: 0.8620 - val_loss: 0.3534 - val_accuracy: 0.8744\n",
      "Epoch 11/30\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3963 - accuracy: 0.8636 - val_loss: 0.3738 - val_accuracy: 0.8669\n",
      "Epoch 12/30\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.3871 - accuracy: 0.8665 - val_loss: 0.3531 - val_accuracy: 0.8752\n",
      "Epoch 13/30\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.3859 - accuracy: 0.8681 - val_loss: 0.3494 - val_accuracy: 0.8758\n",
      "Epoch 14/30\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3774 - accuracy: 0.8705 - val_loss: 0.3478 - val_accuracy: 0.8747\n",
      "Epoch 15/30\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3723 - accuracy: 0.8714 - val_loss: 0.3395 - val_accuracy: 0.8806\n",
      "Epoch 16/30\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3690 - accuracy: 0.8706 - val_loss: 0.3440 - val_accuracy: 0.8805\n",
      "Epoch 17/30\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3628 - accuracy: 0.8744 - val_loss: 0.3352 - val_accuracy: 0.8829\n",
      "Epoch 18/30\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3533 - accuracy: 0.8777 - val_loss: 0.3370 - val_accuracy: 0.8779\n",
      "Epoch 19/30\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.3506 - accuracy: 0.8779 - val_loss: 0.3425 - val_accuracy: 0.8788\n",
      "Epoch 20/30\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.3500 - accuracy: 0.8800 - val_loss: 0.3412 - val_accuracy: 0.8794\n",
      "Epoch 21/30\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3447 - accuracy: 0.8796 - val_loss: 0.3345 - val_accuracy: 0.8820\n",
      "Epoch 22/30\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3412 - accuracy: 0.8811 - val_loss: 0.3391 - val_accuracy: 0.8774\n",
      "Epoch 23/30\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3428 - accuracy: 0.8813 - val_loss: 0.3310 - val_accuracy: 0.8825\n",
      "Epoch 24/30\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3324 - accuracy: 0.8835 - val_loss: 0.3442 - val_accuracy: 0.8785\n",
      "Epoch 25/30\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3296 - accuracy: 0.8845 - val_loss: 0.3367 - val_accuracy: 0.8807\n",
      "Epoch 26/30\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.3241 - accuracy: 0.8877 - val_loss: 0.3366 - val_accuracy: 0.8836\n",
      "Epoch 27/30\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3227 - accuracy: 0.8882 - val_loss: 0.3310 - val_accuracy: 0.8831\n",
      "Epoch 28/30\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3226 - accuracy: 0.8884 - val_loss: 0.3248 - val_accuracy: 0.8871\n",
      "Epoch 29/30\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3228 - accuracy: 0.8880 - val_loss: 0.3335 - val_accuracy: 0.8846\n",
      "Epoch 30/30\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.3197 - accuracy: 0.8867 - val_loss: 0.3293 - val_accuracy: 0.8854\n",
      "313/313 - 0s - loss: 0.3525 - accuracy: 0.8765 - 205ms/epoch - 654us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Nadam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Nadam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Nadam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.8867291808128357\n",
      "Validation accuracy : 0.8854166865348816\n",
      "Loss : 0.35245999693870544\n",
      "Accuracy : 0.8765000104904175\n",
      "\n",
      "Train Accuracy: 0.8867291808128357, Validation Accuracy: 0.8854166865348816\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with Nadam, LR: 0.0001, Batch Size: 32\n",
      "Epoch 1/30\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 1.3117 - accuracy: 0.5707 - val_loss: 0.6374 - val_accuracy: 0.7925\n",
      "Epoch 2/30\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.8183 - accuracy: 0.7339 - val_loss: 0.5107 - val_accuracy: 0.8228\n",
      "Epoch 3/30\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 0.6860 - accuracy: 0.7737 - val_loss: 0.4617 - val_accuracy: 0.8352\n",
      "Epoch 4/30\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.6203 - accuracy: 0.7953 - val_loss: 0.4302 - val_accuracy: 0.8441\n",
      "Epoch 5/30\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 0.5802 - accuracy: 0.8044 - val_loss: 0.4141 - val_accuracy: 0.8478\n",
      "Epoch 6/30\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.5550 - accuracy: 0.8153 - val_loss: 0.3986 - val_accuracy: 0.8580\n",
      "Epoch 7/30\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.5311 - accuracy: 0.8225 - val_loss: 0.3900 - val_accuracy: 0.8581\n",
      "Epoch 8/30\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.5101 - accuracy: 0.8306 - val_loss: 0.3875 - val_accuracy: 0.8588\n",
      "Epoch 9/30\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.4938 - accuracy: 0.8339 - val_loss: 0.3898 - val_accuracy: 0.8576\n",
      "Epoch 10/30\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.4858 - accuracy: 0.8370 - val_loss: 0.3673 - val_accuracy: 0.8683\n",
      "Epoch 11/30\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.4766 - accuracy: 0.8398 - val_loss: 0.3602 - val_accuracy: 0.8693\n",
      "Epoch 12/30\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.4650 - accuracy: 0.8437 - val_loss: 0.3636 - val_accuracy: 0.8709\n",
      "Epoch 13/30\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.4510 - accuracy: 0.8475 - val_loss: 0.3564 - val_accuracy: 0.8721\n",
      "Epoch 14/30\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.4469 - accuracy: 0.8487 - val_loss: 0.3513 - val_accuracy: 0.8751\n",
      "Epoch 15/30\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 0.4394 - accuracy: 0.8508 - val_loss: 0.3536 - val_accuracy: 0.8751\n",
      "Epoch 16/30\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 0.4306 - accuracy: 0.8538 - val_loss: 0.3517 - val_accuracy: 0.8753\n",
      "Epoch 17/30\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 0.4292 - accuracy: 0.8545 - val_loss: 0.3524 - val_accuracy: 0.8715\n",
      "Epoch 18/30\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 0.4185 - accuracy: 0.8580 - val_loss: 0.3417 - val_accuracy: 0.8773\n",
      "Epoch 19/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4172 - accuracy: 0.8589 - val_loss: 0.3472 - val_accuracy: 0.8770\n",
      "Epoch 20/30\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 0.4095 - accuracy: 0.8604 - val_loss: 0.3421 - val_accuracy: 0.8778\n",
      "Epoch 21/30\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 0.3989 - accuracy: 0.8634 - val_loss: 0.3413 - val_accuracy: 0.8767\n",
      "Epoch 22/30\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.3981 - accuracy: 0.8635 - val_loss: 0.3376 - val_accuracy: 0.8789\n",
      "Epoch 23/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3938 - accuracy: 0.8672 - val_loss: 0.3434 - val_accuracy: 0.8779\n",
      "Epoch 24/30\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 0.3922 - accuracy: 0.8682 - val_loss: 0.3444 - val_accuracy: 0.8762\n",
      "Epoch 25/30\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 0.3868 - accuracy: 0.8671 - val_loss: 0.3344 - val_accuracy: 0.8825\n",
      "Epoch 26/30\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 0.3816 - accuracy: 0.8694 - val_loss: 0.3340 - val_accuracy: 0.8804\n",
      "Epoch 27/30\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 0.3791 - accuracy: 0.8704 - val_loss: 0.3273 - val_accuracy: 0.8845\n",
      "Epoch 28/30\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.3764 - accuracy: 0.8731 - val_loss: 0.3329 - val_accuracy: 0.8829\n",
      "Epoch 29/30\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.3724 - accuracy: 0.8713 - val_loss: 0.3325 - val_accuracy: 0.8830\n",
      "Epoch 30/30\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.3627 - accuracy: 0.8746 - val_loss: 0.3357 - val_accuracy: 0.8833\n",
      "313/313 - 0s - loss: 0.3604 - accuracy: 0.8752 - 325ms/epoch - 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Nadam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Nadam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Nadam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.8746041655540466\n",
      "Validation accuracy : 0.8833333253860474\n",
      "Loss : 0.36042094230651855\n",
      "Accuracy : 0.8751999735832214\n",
      "\n",
      "Train Accuracy: 0.8746041655540466, Validation Accuracy: 0.8833333253860474\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with Nadam, LR: 0.0001, Batch Size: 64\n",
      "Epoch 1/30\n",
      "750/750 [==============================] - 3s 3ms/step - loss: 1.4783 - accuracy: 0.5122 - val_loss: 0.7366 - val_accuracy: 0.7813\n",
      "Epoch 2/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.9120 - accuracy: 0.7088 - val_loss: 0.5751 - val_accuracy: 0.8182\n",
      "Epoch 3/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.7450 - accuracy: 0.7595 - val_loss: 0.4949 - val_accuracy: 0.8295\n",
      "Epoch 4/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.6620 - accuracy: 0.7831 - val_loss: 0.4497 - val_accuracy: 0.8422\n",
      "Epoch 5/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.6045 - accuracy: 0.7995 - val_loss: 0.4246 - val_accuracy: 0.8480\n",
      "Epoch 6/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.5689 - accuracy: 0.8110 - val_loss: 0.4078 - val_accuracy: 0.8551\n",
      "Epoch 7/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.5419 - accuracy: 0.8189 - val_loss: 0.3948 - val_accuracy: 0.8592\n",
      "Epoch 8/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.5122 - accuracy: 0.8280 - val_loss: 0.3827 - val_accuracy: 0.8625\n",
      "Epoch 9/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.4972 - accuracy: 0.8328 - val_loss: 0.3781 - val_accuracy: 0.8648\n",
      "Epoch 10/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.4871 - accuracy: 0.8383 - val_loss: 0.3727 - val_accuracy: 0.8679\n",
      "Epoch 11/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.4720 - accuracy: 0.8422 - val_loss: 0.3657 - val_accuracy: 0.8692\n",
      "Epoch 12/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.4620 - accuracy: 0.8453 - val_loss: 0.3558 - val_accuracy: 0.8727\n",
      "Epoch 13/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.4499 - accuracy: 0.8485 - val_loss: 0.3527 - val_accuracy: 0.8740\n",
      "Epoch 14/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.4430 - accuracy: 0.8509 - val_loss: 0.3551 - val_accuracy: 0.8727\n",
      "Epoch 15/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.4366 - accuracy: 0.8531 - val_loss: 0.3486 - val_accuracy: 0.8767\n",
      "Epoch 16/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.4238 - accuracy: 0.8578 - val_loss: 0.3462 - val_accuracy: 0.8781\n",
      "Epoch 17/30\n",
      "750/750 [==============================] - 3s 3ms/step - loss: 0.4166 - accuracy: 0.8593 - val_loss: 0.3480 - val_accuracy: 0.8763\n",
      "Epoch 18/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.4138 - accuracy: 0.8599 - val_loss: 0.3452 - val_accuracy: 0.8767\n",
      "Epoch 19/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.4073 - accuracy: 0.8630 - val_loss: 0.3405 - val_accuracy: 0.8804\n",
      "Epoch 20/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.3978 - accuracy: 0.8642 - val_loss: 0.3358 - val_accuracy: 0.8815\n",
      "Epoch 21/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.3967 - accuracy: 0.8657 - val_loss: 0.3332 - val_accuracy: 0.8809\n",
      "Epoch 22/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.3888 - accuracy: 0.8661 - val_loss: 0.3322 - val_accuracy: 0.8812\n",
      "Epoch 23/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.3896 - accuracy: 0.8682 - val_loss: 0.3279 - val_accuracy: 0.8823\n",
      "Epoch 24/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.3806 - accuracy: 0.8717 - val_loss: 0.3341 - val_accuracy: 0.8817\n",
      "Epoch 25/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.3772 - accuracy: 0.8714 - val_loss: 0.3396 - val_accuracy: 0.8792\n",
      "Epoch 26/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.3758 - accuracy: 0.8707 - val_loss: 0.3273 - val_accuracy: 0.8842\n",
      "Epoch 27/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.3700 - accuracy: 0.8742 - val_loss: 0.3282 - val_accuracy: 0.8827\n",
      "Epoch 28/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.3651 - accuracy: 0.8752 - val_loss: 0.3340 - val_accuracy: 0.8818\n",
      "Epoch 29/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.3602 - accuracy: 0.8765 - val_loss: 0.3225 - val_accuracy: 0.8841\n",
      "Epoch 30/30\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.3577 - accuracy: 0.8769 - val_loss: 0.3262 - val_accuracy: 0.8842\n",
      "313/313 - 0s - loss: 0.3520 - accuracy: 0.8765 - 215ms/epoch - 687us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Nadam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Nadam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Nadam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy : 0.8768749833106995\n",
      "Validation accuracy : 0.8841666579246521\n",
      "Loss : 0.35202082991600037\n",
      "Accuracy : 0.8765000104904175\n",
      "\n",
      "Train Accuracy: 0.8768749833106995, Validation Accuracy: 0.8841666579246521\n",
      "--------------------------------------------------\n",
      "\n",
      "Training with Nadam, LR: 0.0001, Batch Size: 128\n",
      "Epoch 1/30\n",
      "375/375 [==============================] - 3s 4ms/step - loss: 1.5812 - accuracy: 0.4765 - val_loss: 0.8751 - val_accuracy: 0.7538\n",
      "Epoch 2/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 1.0041 - accuracy: 0.6764 - val_loss: 0.6393 - val_accuracy: 0.8019\n",
      "Epoch 3/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.8339 - accuracy: 0.7319 - val_loss: 0.5541 - val_accuracy: 0.8182\n",
      "Epoch 4/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.7356 - accuracy: 0.7625 - val_loss: 0.4990 - val_accuracy: 0.8318\n",
      "Epoch 5/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.6697 - accuracy: 0.7831 - val_loss: 0.4635 - val_accuracy: 0.8372\n",
      "Epoch 6/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.6255 - accuracy: 0.7946 - val_loss: 0.4430 - val_accuracy: 0.8432\n",
      "Epoch 7/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.5884 - accuracy: 0.8077 - val_loss: 0.4233 - val_accuracy: 0.8495\n",
      "Epoch 8/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.5595 - accuracy: 0.8159 - val_loss: 0.4126 - val_accuracy: 0.8520\n",
      "Epoch 9/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.5436 - accuracy: 0.8195 - val_loss: 0.3995 - val_accuracy: 0.8559\n",
      "Epoch 10/30\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.5176 - accuracy: 0.8266 - val_loss: 0.3921 - val_accuracy: 0.8618\n",
      "Epoch 11/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.5031 - accuracy: 0.8325 - val_loss: 0.3866 - val_accuracy: 0.8627\n",
      "Epoch 12/30\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.4895 - accuracy: 0.8350 - val_loss: 0.3799 - val_accuracy: 0.8651\n",
      "Epoch 13/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4822 - accuracy: 0.8387 - val_loss: 0.3707 - val_accuracy: 0.8682\n",
      "Epoch 14/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4662 - accuracy: 0.8434 - val_loss: 0.3682 - val_accuracy: 0.8682\n",
      "Epoch 15/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4584 - accuracy: 0.8469 - val_loss: 0.3710 - val_accuracy: 0.8683\n",
      "Epoch 16/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4478 - accuracy: 0.8494 - val_loss: 0.3710 - val_accuracy: 0.8692\n",
      "Epoch 17/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4396 - accuracy: 0.8535 - val_loss: 0.3604 - val_accuracy: 0.8711\n",
      "Epoch 18/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4349 - accuracy: 0.8526 - val_loss: 0.3574 - val_accuracy: 0.8726\n",
      "Epoch 19/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4257 - accuracy: 0.8578 - val_loss: 0.3499 - val_accuracy: 0.8742\n",
      "Epoch 20/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4227 - accuracy: 0.8573 - val_loss: 0.3475 - val_accuracy: 0.8748\n",
      "Epoch 21/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4165 - accuracy: 0.8594 - val_loss: 0.3454 - val_accuracy: 0.8762\n",
      "Epoch 22/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4078 - accuracy: 0.8619 - val_loss: 0.3441 - val_accuracy: 0.8777\n",
      "Epoch 23/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4030 - accuracy: 0.8630 - val_loss: 0.3433 - val_accuracy: 0.8769\n",
      "Epoch 24/30\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3955 - accuracy: 0.8655 - val_loss: 0.3399 - val_accuracy: 0.8777\n",
      "Epoch 25/30\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4032 - accuracy: 0.8641 - val_loss: 0.3406 - val_accuracy: 0.8780\n",
      "Epoch 26/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3910 - accuracy: 0.8671 - val_loss: 0.3397 - val_accuracy: 0.8778\n",
      "Epoch 27/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3881 - accuracy: 0.8679 - val_loss: 0.3380 - val_accuracy: 0.8782\n",
      "Epoch 28/30\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3832 - accuracy: 0.8703 - val_loss: 0.3311 - val_accuracy: 0.8811\n",
      "Epoch 29/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3814 - accuracy: 0.8706 - val_loss: 0.3321 - val_accuracy: 0.8800\n",
      "Epoch 30/30\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3767 - accuracy: 0.8711 - val_loss: 0.3416 - val_accuracy: 0.8783\n",
      "313/313 - 0s - loss: 0.3659 - accuracy: 0.8692 - 192ms/epoch - 613us/step\n",
      "\n",
      "Training accuracy : 0.8710625171661377\n",
      "Validation accuracy : 0.878333330154419\n",
      "Loss : 0.36586207151412964\n",
      "Accuracy : 0.8691999912261963\n",
      "\n",
      "Train Accuracy: 0.8710625171661377, Validation Accuracy: 0.878333330154419\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_val_accuracy = 0\n",
    "best_params = {}\n",
    "best_history = {}  # To store the history of the best model\n",
    "\n",
    "for opt_name, opt_class in optimizers.items():\n",
    "    for lr in learning_rates:\n",
    "        for batch_size in batch_sizes:\n",
    "            print(f\"\\nTraining with {opt_name}, LR: {lr}, Batch Size: {batch_size}\")\n",
    "            model = fmm_module.FMM.create_model_v1()\n",
    "            optimizer = opt_class(learning_rate=lr)\n",
    "            history = fmm_module.FMM.compile_and_train(\n",
    "                model, X_train, y_train, optimizer, batch_size=batch_size\n",
    "            )\n",
    "\n",
    "            loss, accuracy, train_accuracy, val_accuracy = fmm_module.FMM.evaluate(model, X_test, y_test, history)\n",
    "            # Update best parameters if current model is better\n",
    "            if val_accuracy > best_val_accuracy:\n",
    "                best_val_accuracy = val_accuracy\n",
    "                best_params = {\n",
    "                    'optimizer': opt_name,\n",
    "                    'learning_rate': lr,\n",
    "                    'batch_size': batch_size,\n",
    "                    'train_accuracy': train_accuracy,\n",
    "                    'val_accuracy': val_accuracy\n",
    "                }\n",
    "                best_history = history  # Save the best history for plotting\n",
    "\n",
    "            print(f\"Train Accuracy: {train_accuracy}, Validation Accuracy: {val_accuracy}\")\n",
    "            print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d2e10e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Best Hyperparameters:\n",
      "Optimizer: Adam\n",
      "Learning Rate: 0.0005\n",
      "Batch Size: 128\n",
      "Training Accuracy: 0.8914583325386047\n",
      "Validation Accuracy: 0.8888333439826965\n"
     ]
    }
   ],
   "source": [
    "print(\"Final Best Hyperparameters:\")\n",
    "print(f\"Optimizer: {best_params['optimizer']}\")\n",
    "print(f\"Learning Rate: {best_params['learning_rate']}\")\n",
    "print(f\"Batch Size: {best_params['batch_size']}\")\n",
    "print(f\"Training Accuracy: {best_params['train_accuracy']}\")\n",
    "print(f\"Validation Accuracy: {best_params['val_accuracy']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a12815d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(best_history.history['loss'], label='Training Loss')\n",
    "plt.plot(best_history.history['val_loss'], label='Validation Loss')\n",
    "plt.title(f'Loss - Best Model\\nOptimizer: {best_params[\"optimizer\"]}, LR: {best_params[\"learning_rate\"]}, Batch: {best_params[\"batch_size\"]}')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(best_history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(best_history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title(f'Accuracy - Best Model\\nOptimizer: {best_params[\"optimizer\"]}, LR: {best_params[\"learning_rate\"]}, Batch: {best_params[\"batch_size\"]}')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ee8e9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
